{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = 'cnn-fake-news-classification-tf/data/news-data/'\n",
    "real_titles =  open(directory+\"real_titles.txt\").readlines()\n",
    "fake_titles = open(directory+\"fake_titles.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11808"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_titles = [title.lower()[:-1] for title in real_titles]\n",
    "fake_titles = [title.lower()[:-1] for title in fake_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = real_titles+fake_titles\n",
    "y=['real' for real in real_titles]+['fake' for fake in fake_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "dev_sample_percentage=0.1\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078891258  is dev score for C= 0.001\n",
      "0.771855010661  is dev score for C= 0.01\n",
      "0.828144989339  is dev score for C= 0.1\n",
      "0.827292110874  is dev score for C= 10.0\n",
      "0.809808102345  is dev score for C= 1000.0\n",
      "0.792324093817  is dev score for C= 100000.0\n",
      "0.768869936034  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "d = collections.OrderedDict()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=CountVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3918), ('to', 3232), ('of', 2279), ('in', 1984), ('and', 1797), ('for', 1578), ('trump', 1315), ('on', 1194), ('is', 878), ('as', 781), ('review', 714), ('it', 675), ('with', 622), ('at', 538), ('new', 537), ('by', 509), ('be', 452), ('from', 442), ('how', 436), ('after', 434), ('says', 395), ('over', 371), ('are', 341), ('donald', 334), ('will', 330), ('your', 329), ('us', 328), ('you', 326), ('not', 323), ('what', 318), ('can', 309), ('we', 299), ('but', 290), ('uk', 285), ('brexit', 272), ('about', 256), ('election', 255), ('an', 254), ('up', 249), ('no', 248), ('that', 247), ('world', 242), ('england', 238), ('why', 237), ('this', 231), ('my', 219), ('best', 208), ('could', 205), ('have', 202), ('may', 201), ('his', 199), ('happened', 195), ('who', 191), ('out', 190), ('more', 180), ('letters', 176), ('against', 174), ('has', 168), ('was', 168), ('clinton', 160), ('2016', 156), ('first', 156), ('do', 155), ('one', 155), ('city', 154), ('all', 153), ('life', 151), ('year', 148), ('into', 146), ('day', 144), ('make', 142), ('women', 142), ('our', 138), ('time', 138), ('its', 138), ('he', 137), ('win', 137), ('back', 136), ('off', 133), ('australia', 133), ('people', 131), ('obama', 128), ('week', 125), ('get', 125), ('president', 124), ('police', 124), ('york', 122), ('now', 122), ('like', 116), ('america', 115), ('their', 115), ('climate', 114), ('london', 113), ('tv', 112), ('deal', 111), ('home', 110), ('don', 110), ('john', 110), ('white', 108), ('manchester', 108)]\n"
     ]
    }
   ],
   "source": [
    "matrix = vectorizer.transform(real_titles)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sorted_words_true=(sorted (freqs, key = lambda x: -x[1])[:100])\n",
    "print (sorted_words_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4131), ('to', 3193), ('of', 2338), ('in', 2079), ('trump', 1600), ('on', 1445), ('and', 1442), ('for', 1439), ('is', 1194), ('hillary', 1118), ('clinton', 1008), ('by', 813), ('with', 685), ('election', 590), ('us', 533), ('are', 507), ('from', 500), ('at', 485), ('new', 477), ('as', 466), ('will', 433), ('what', 413), ('this', 412), ('fbi', 400), ('war', 390), ('that', 390), ('you', 386), ('be', 384), ('comment', 381), ('about', 376), ('it', 367), ('after', 354), ('russia', 329), ('how', 325), ('not', 321), ('world', 317), ('donald', 310), ('obama', 304), ('just', 297), ('why', 280), ('over', 274), ('out', 257), ('2016', 254), ('has', 250), ('up', 246), ('all', 236), ('america', 233), ('your', 231), ('have', 227), ('now', 226), ('who', 224), ('her', 220), ('news', 220), ('breaking', 218), ('no', 217), ('emails', 217), ('wikileaks', 217), ('its', 216), ('an', 213), ('campaign', 212), ('if', 208), ('we', 207), ('president', 201), ('against', 196), ('day', 196), ('video', 196), ('can', 195), ('more', 194), ('says', 190), ('russian', 190), ('media', 189), ('his', 186), ('was', 184), ('vote', 181), ('email', 177), ('trumps', 173), ('black', 169), ('clintons', 168), ('people', 167), ('putin', 167), ('they', 166), ('one', 164), ('syria', 163), ('he', 161), ('american', 160), ('investigation', 159), ('police', 157), ('re', 156), ('into', 154), ('white', 146), ('or', 144), ('watch', 142), ('down', 142), ('their', 142), ('state', 140), ('hillarys', 139), ('time', 137), ('could', 136), ('voting', 133), ('life', 131)]\n"
     ]
    }
   ],
   "source": [
    "matrix = vectorizer.transform(fake_titles)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sorted_words_false=(sorted (freqs, key = lambda x: -x[1])[:100])\n",
    "print (sorted_words_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_true=np.setdiff1d([word for (word, number) in sorted_words_true],[word for (word, number) in sorted_words_false])\n",
    "unique_false=np.setdiff1d([word for (word, number) in sorted_words_false],[word for (word, number) in sorted_words_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709594882729  is dev score for C= 0.001\n",
      "0.773560767591  is dev score for C= 0.01\n",
      "0.822174840085  is dev score for C= 0.1\n",
      "0.82302771855  is dev score for C= 10.0\n",
      "0.811940298507  is dev score for C= 1000.0\n",
      "0.799147121535  is dev score for C= 100000.0\n",
      "0.797014925373  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=CountVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709594882729  is dev score for C= 0.001\n",
      "0.772707889126  is dev score for C= 0.01\n",
      "0.817484008529  is dev score for C= 0.1\n",
      "0.817910447761  is dev score for C= 10.0\n",
      "0.806823027719  is dev score for C= 1000.0\n",
      "0.794456289979  is dev score for C= 100000.0\n",
      "0.791044776119  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,4))\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=CountVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711300639659  is dev score for C= 0.001\n",
      "0.768017057569  is dev score for C= 0.01\n",
      "0.817484008529  is dev score for C= 0.1\n",
      "0.81407249467  is dev score for C= 10.0\n",
      "0.797867803838  is dev score for C= 1000.0\n",
      "0.78763326226  is dev score for C= 100000.0\n",
      "0.78592750533  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,4),stop_words='english')\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=CountVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715138592751  is dev score for C= 0.001\n",
      "0.771002132196  is dev score for C= 0.01\n",
      "0.82132196162  is dev score for C= 0.1\n",
      "0.827292110874  is dev score for C= 10.0\n",
      "0.790191897655  is dev score for C= 1000.0\n",
      "0.765031982942  is dev score for C= 100000.0\n",
      "0.7539445629  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=CountVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644349680171  is dev score for C= 0.001\n",
      "0.75223880597  is dev score for C= 0.01\n",
      "0.802558635394  is dev score for C= 0.1\n",
      "0.840085287846  is dev score for C= 10.0\n",
      "0.804690831557  is dev score for C= 1000.0\n",
      "0.783795309168  is dev score for C= 100000.0\n",
      "0.768869936034  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=TfidfVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651172707889  is dev score for C= 0.001\n",
      "0.754797441365  is dev score for C= 0.01\n",
      "0.79829424307  is dev score for C= 0.1\n",
      "0.841791044776  is dev score for C= 10.0\n",
      "0.816631130064  is dev score for C= 1000.0\n",
      "0.80170575693  is dev score for C= 100000.0\n",
      "0.782515991471  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=TfidfVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612793176972  is dev score for C= 0.001\n",
      "0.762473347548  is dev score for C= 0.01\n",
      "0.790618336887  is dev score for C= 0.1\n",
      "0.834968017058  is dev score for C= 10.0\n",
      "0.832835820896  is dev score for C= 1000.0\n",
      "0.829424307036  is dev score for C= 100000.0\n",
      "0.83368869936  is dev score for C= 10000000.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "feature_list=vectorizer.get_feature_names()\n",
    "vectorizer_2=TfidfVectorizer(vocabulary = feature_list)\n",
    "X2=vectorizer_2.fit_transform(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3,1e5,1e7]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = ['australia back brexit','hey its donald trump']\n",
    "X_test = vectorizer_2.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.65041330e-13   1.00000000e+00]\n",
      " [  9.99481869e-01   5.18130742e-04]]\n",
      "['fake' 'real']\n",
      "['australia' 'back' 'best' 'brexit' 'but' 'city' 'climate' 'deal' 'do'\n",
      " 'don' 'england' 'first' 'get' 'happened' 'home' 'john' 'letters' 'like'\n",
      " 'london' 'make' 'manchester' 'may' 'my' 'off' 'our' 'review' 'tv' 'uk'\n",
      " 'week' 'win' 'women' 'year' 'york']\n",
      "['american' 'black' 'breaking' 'campaign' 'clintons' 'comment' 'down'\n",
      " 'email' 'emails' 'fbi' 'her' 'hillary' 'hillarys' 'if' 'investigation'\n",
      " 'just' 'media' 'news' 'or' 'putin' 're' 'russia' 'russian' 'state' 'syria'\n",
      " 'they' 'trumps' 'video' 'vote' 'voting' 'war' 'watch' 'wikileaks']\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "print(probs)\n",
    "print(model.classes_)\n",
    "print(unique_true)\n",
    "print(unique_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "def getIndicesVector(text_arr, word_to_indx, max_length=50):\n",
    "    nil_indx = 0\n",
    "    text_indx = [ word_to_indx[x.lower().encode('utf8')] if x.lower().encode('utf8') in word_to_indx else nil_indx for x in text_arr][:max_length]\n",
    "    if len(text_indx) < max_length:\n",
    "        text_indx.extend( [nil_indx for _ in range(max_length - len(text_indx))])\n",
    "    text_indx=np.array(text_indx)\n",
    "    return text_indx\n",
    "\n",
    "def getEmbeddingVector(filename):\n",
    "    lines = []\n",
    "    with gzip.open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    embedding_vector = []\n",
    "    word_to_indx = {}\n",
    "    for indx, l in enumerate(lines):\n",
    "        word, emb = l.split()[0], l.split()[1:]\n",
    "        vector = [float(x) for x in emb ]\n",
    "        if indx == 0:\n",
    "            embedding_vector.append( np.zeros( len(vector) ) )\n",
    "        embedding_vector.append(vector)\n",
    "        word_to_indx[word] = indx+1\n",
    "    embedding_vector = np.array(embedding_vector)\n",
    "    return embedding_vector, word_to_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings, word_to_indx = getEmbeddingVector('word_vectors.txt.gz')\n",
    "def corpus_to_embed(corpus):\n",
    "    embeddings_corpus=[]\n",
    "    for text in corpus:\n",
    "        x=getIndicesVector(text.split(),word_to_indx)\n",
    "        embeddings_corpus.append(x)\n",
    "    embeddings_corpus=np.matrix(embeddings_corpus)\n",
    "    return embeddings_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.569296375267  is embed logreg dev score for C= 0.001\n",
      "0.568443496802  is embed logreg dev score for C= 0.01\n",
      "0.567164179104  is embed logreg dev score for C= 0.1\n",
      "0.568443496802  is embed logreg dev score for C= 10.0\n",
      "0.565458422175  is embed logreg dev score for C= 1000.0\n"
     ]
    }
   ],
   "source": [
    "#utf8_corpus=[x.encode('utf8') for x in corpus]\n",
    "X=corpus_to_embed(x_train)\n",
    "#utf8_dev_corpus=[x.encode('utf8') for x in dev_corpus]\n",
    "X2=corpus_to_embed(x_dev)\n",
    "for i in [1e-3,1e-2,1e-1,1e1,1e3]:\n",
    "    logreg = linear_model.LogisticRegression(C=i)\n",
    "    model = logreg.fit(X,y_train)\n",
    "    predicted=logreg.predict(X2)\n",
    "    score=accuracy_score(y_dev,predicted)\n",
    "    print(score, \" is embed logreg dev score for C=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 60), ('to', 48), ('of', 38), ('in', 36), ('for', 24), ('trump', 21), ('and', 18), ('on', 17), ('is', 13), ('are', 12), ('by', 10), ('as', 9), ('clinton', 8), ('from', 8), ('election', 8), ('new', 8), ('what', 8), ('at', 7), ('why', 7), ('with', 7), ('will', 7), ('against', 7), ('hillary', 6), ('president', 6), ('life', 5), ('uk', 5), ('how', 5), ('us', 5), ('no', 5), ('reveals', 5), ('an', 4), ('women', 4), ('supporters', 4), ('most', 4), ('isis', 4), ('you', 4), ('that', 4), ('big', 4), ('he', 4), ('our', 4), ('donald', 3), ('america', 3), ('only', 3), ('could', 3), ('her', 3), ('have', 3), ('his', 3), ('vote', 3), ('him', 3), ('be', 3), ('there', 3), ('politics', 3), ('ex', 3), ('makes', 3), ('calls', 3), ('troops', 3), ('up', 3), ('about', 3), ('emails', 3), ('obama', 3), ('political', 3), ('referendum', 3), ('free', 3), ('their', 3), ('rubio', 2), ('seeks', 2), ('needed', 2), ('else', 2), ('people', 2), ('even', 2), ('hate', 2), ('emotional', 2), ('speech', 2), ('implores', 2), ('keep', 2), ('believing', 2), ('if', 2), ('city', 2), ('court', 2), ('man', 2), ('over', 2), ('true', 2), ('black', 2), ('friday', 2), ('report', 2), ('real', 2), ('it', 2), ('this', 2), ('where', 2), ('single', 2), ('lead', 2), ('former', 2), ('lost', 2), ('prices', 2), ('winter', 2), ('your', 2), ('steps', 2), ('get', 2), ('high', 2), ('me', 2)]\n",
      "[('the', 57), ('to', 47), ('in', 39), ('of', 29), ('and', 26), ('on', 19), ('for', 19), ('trump', 18), ('is', 15), ('at', 12), ('as', 11), ('us', 11), ('from', 10), ('new', 10), ('it', 8), ('by', 8), ('donald', 7), ('over', 7), ('election', 7), ('who', 7), ('how', 7), ('about', 7), ('story', 6), ('clinton', 5), ('could', 5), ('woman', 5), ('so', 5), ('you', 5), ('what', 5), ('into', 5), ('first', 5), ('he', 5), ('did', 5), ('people', 4), ('hillary', 4), ('america', 4), ('says', 4), ('your', 4), ('have', 4), ('can', 4), ('under', 4), ('president', 4), ('all', 4), ('obama', 4), ('time', 4), ('york', 4), ('times', 4), ('really', 4), ('life', 3), ('end', 3), ('make', 3), ('leave', 3), ('man', 3), ('2016', 3), ('are', 3), ('with', 3), ('dakota', 3), ('pipeline', 3), ('protesters', 3), ('home', 3), ('her', 3), ('re', 3), ('his', 3), ('him', 3), ('like', 3), ('that', 3), ('down', 3), ('out', 3), ('still', 3), ('has', 3), ('airstrikes', 3), ('girl', 3), ('tv', 3), ('after', 3), ('profit', 3), ('face', 3), ('aleppo', 3), ('now', 3), ('before', 3), ('drug', 3), ('say', 3), ('some', 3), ('or', 3), ('fail', 3), ('un', 3), ('sector', 3), ('warns', 3), ('an', 2), ('does', 2), ('three', 2), ('benefit', 2), ('if', 2), ('city', 2), ('military', 2), ('was', 2), ('torture', 2), ('offers', 2), ('matter', 2), ('next', 2), ('war', 2)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "d = collections.OrderedDict()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "import numpy as np\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "dev_sample_percentage=0.1\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "X=vectorizer.fit_transform(x_train)\n",
    "\n",
    "#false pos is fake, classified as real\n",
    "#false neg is real, classified as fake\n",
    "\n",
    "directory = 'cnn-text-classification-tf (titles)/'\n",
    "real_titles =  open(directory+\"false_neg.txt\").readlines()\n",
    "fake_titles = open(directory+\"false_pos.txt\").readlines()\n",
    "real_titles = [title.lower()[:-1] for title in real_titles]\n",
    "fake_titles = [title.lower()[:-1] for title in fake_titles]\n",
    "x = real_titles+fake_titles\n",
    "y=['real' for real in real_titles]+['fake' for fake in fake_titles]\n",
    "\n",
    "matrix = vectorizer.transform(real_titles)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sorted_words_true=(sorted (freqs, key = lambda x: -x[1])[:100])\n",
    "print (sorted_words_true)\n",
    "\n",
    "matrix = vectorizer.transform(fake_titles)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sorted_words_false=(sorted (freqs, key = lambda x: -x[1])[:100])\n",
    "print (sorted_words_false)\n",
    "\n",
    "\n",
    "unique_true=np.setdiff1d([word for (word, number) in sorted_words_true],[word for (word, number) in sorted_words_false])\n",
    "unique_false=np.setdiff1d([word for (word, number) in sorted_words_false],[word for (word, number) in sorted_words_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['against', 'be', 'believing', 'big', 'black', 'calls', 'court',\n",
       "       'else', 'emails', 'emotional', 'even', 'ex', 'former', 'free',\n",
       "       'friday', 'get', 'hate', 'high', 'implores', 'isis', 'keep',\n",
       "       'lead', 'lost', 'makes', 'me', 'most', 'needed', 'no', 'only',\n",
       "       'our', 'political', 'politics', 'prices', 'real', 'referendum',\n",
       "       'report', 'reveals', 'rubio', 'seeks', 'single', 'speech', 'steps',\n",
       "       'supporters', 'their', 'there', 'this', 'troops', 'true', 'uk',\n",
       "       'up', 'vote', 'where', 'why', 'will', 'winter', 'women'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016', 'after', 'airstrikes', 'aleppo', 'all', 'before',\n",
       "       'benefit', 'can', 'dakota', 'did', 'does', 'down', 'drug', 'end',\n",
       "       'face', 'fail', 'first', 'girl', 'has', 'home', 'into', 'leave',\n",
       "       'like', 'make', 'matter', 'military', 'next', 'now', 'offers',\n",
       "       'or', 'out', 'pipeline', 'profit', 'protesters', 're', 'really',\n",
       "       'say', 'says', 'sector', 'so', 'some', 'still', 'story', 'three',\n",
       "       'time', 'times', 'torture', 'tv', 'un', 'under', 'war', 'warns',\n",
       "       'was', 'who', 'woman', 'york'], dtype='<U10')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
