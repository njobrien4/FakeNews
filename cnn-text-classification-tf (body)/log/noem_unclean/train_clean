2018-04-02 19:15:06.786180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 19:15:06.788578: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 19:15:06.788634: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 19:15:08.861179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K20Xm
major: 3 minor: 5 memoryClockRate (GHz) 0.732
pciBusID 0000:03:00.0
Total memory: 5.94GiB
Free memory: 5.87GiB
2018-04-02 19:15:09.176890: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x62cae90 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-04-02 19:15:09.177857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: Tesla K20Xm
major: 3 minor: 5 memoryClockRate (GHz) 0.732
pciBusID 0000:83:00.0
Total memory: 5.94GiB
Free memory: 5.87GiB
2018-04-02 19:15:09.515659: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xa041010 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-04-02 19:15:09.516665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: Tesla K20Xm
major: 3 minor: 5 memoryClockRate (GHz) 0.732
pciBusID 0000:84:00.0
Total memory: 5.94GiB
Free memory: 5.87GiB
2018-04-02 19:15:09.534160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1
2018-04-02 19:15:09.534361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 2
2018-04-02 19:15:09.534410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0
2018-04-02 19:15:09.534780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 2 and 0
2018-04-02 19:15:09.534904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 
2018-04-02 19:15:09.534940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N N 
2018-04-02 19:15:09.534967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y Y 
2018-04-02 19:15:09.534991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   N Y Y 
2018-04-02 19:15:09.535030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:03:00.0)
2018-04-02 19:15:09.535063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K20Xm, pci bus id: 0000:83:00.0)
2018-04-02 19:15:09.535091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K20Xm, pci bus id: 0000:84:00.0)
2018-04-02 19:17:39.953618: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.05
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=100
FILTER_SIZES=3
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=/om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/news-data/non_trump_fake_bodies.txt
NUM_CHECKPOINTS=5
NUM_EPOCHS=10
NUM_FILTERS=128
POSITIVE_DATA_FILE=/om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/news-data/all_real_bodies_noem_no_trump.txt

Loading data...
1000 is mdl
Vocabulary Size: 154228
Train/Dev split: 15384/809
Tensor("conv-maxpool-3/conv:0", shape=(?, 998, 1, 128), dtype=float32) is conv in textcnn
Tensor("conv-maxpool-3/relu:0", shape=(?, 998, 1, 128), dtype=float32) is h in textcnn
Tensor("conv-maxpool-3/pool:0", shape=(?, 1, 1, 128), dtype=float32) is pooled
Tensor("dropout/dropout/mul:0", shape=(?, 128), dtype=float32) is h_drop
Writing to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510

Load word2vec file /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/word_embeddings/GoogleNews-vectors-negative300.bin
word2vec file has been loaded
2018-04-02T19:15:51.608305: step 1, loss 0.959555, acc 0.5, learning_rate 0.005
2018-04-02T19:15:52.698506: step 2, loss 1.32346, acc 0.515625, learning_rate 0.00499185
2018-04-02T19:15:53.786814: step 3, loss 1.0016, acc 0.546875, learning_rate 0.00498372
2018-04-02T19:15:54.876054: step 4, loss 0.735724, acc 0.625, learning_rate 0.0049756
2018-04-02T19:15:55.966126: step 5, loss 1.02956, acc 0.6875, learning_rate 0.00496749
2018-04-02T19:15:57.056953: step 6, loss 0.955392, acc 0.625, learning_rate 0.0049594
2018-04-02T19:15:58.145443: step 7, loss 0.531337, acc 0.78125, learning_rate 0.00495132
2018-04-02T19:15:59.245497: step 8, loss 0.799546, acc 0.703125, learning_rate 0.00494325
2018-04-02T19:16:00.341096: step 9, loss 0.740362, acc 0.6875, learning_rate 0.0049352
2018-04-02T19:16:01.433379: step 10, loss 0.550669, acc 0.796875, learning_rate 0.00492716
2018-04-02T19:16:02.534407: step 11, loss 0.611345, acc 0.75, learning_rate 0.00491914
2018-04-02T19:16:03.622729: step 12, loss 0.577396, acc 0.78125, learning_rate 0.00491112
2018-04-02T19:16:04.720608: step 13, loss 0.505102, acc 0.8125, learning_rate 0.00490312
2018-04-02T19:16:05.812662: step 14, loss 0.533147, acc 0.8125, learning_rate 0.00489514
2018-04-02T19:16:06.912402: step 15, loss 0.527345, acc 0.765625, learning_rate 0.00488716
2018-04-02T19:16:08.014198: step 16, loss 0.4483, acc 0.796875, learning_rate 0.0048792
2018-04-02T19:16:09.107763: step 17, loss 0.411029, acc 0.828125, learning_rate 0.00487126
2018-04-02T19:16:10.208968: step 18, loss 0.410456, acc 0.84375, learning_rate 0.00486333
2018-04-02T19:16:11.309369: step 19, loss 0.243919, acc 0.921875, learning_rate 0.00485541
2018-04-02T19:16:12.408998: step 20, loss 0.460905, acc 0.78125, learning_rate 0.0048475
2018-04-02T19:16:13.499199: step 21, loss 0.373141, acc 0.875, learning_rate 0.00483961
2018-04-02T19:16:14.604719: step 22, loss 0.474882, acc 0.78125, learning_rate 0.00483172
2018-04-02T19:16:15.706444: step 23, loss 0.466615, acc 0.859375, learning_rate 0.00482386
2018-04-02T19:16:16.797368: step 24, loss 0.557339, acc 0.78125, learning_rate 0.004816
2018-04-02T19:16:17.895687: step 25, loss 0.382633, acc 0.875, learning_rate 0.00480816
2018-04-02T19:16:18.981522: step 26, loss 0.227144, acc 0.90625, learning_rate 0.00480033
2018-04-02T19:16:20.069392: step 27, loss 0.26541, acc 0.921875, learning_rate 0.00479252
2018-04-02T19:16:21.156558: step 28, loss 0.278475, acc 0.859375, learning_rate 0.00478472
2018-04-02T19:16:22.245543: step 29, loss 0.303312, acc 0.90625, learning_rate 0.00477693
2018-04-02T19:16:23.328724: step 30, loss 0.40768, acc 0.828125, learning_rate 0.00476915
2018-04-02T19:16:24.426316: step 31, loss 0.34445, acc 0.921875, learning_rate 0.00476139
2018-04-02T19:16:25.517072: step 32, loss 0.180391, acc 0.90625, learning_rate 0.00475364
2018-04-02T19:16:26.608940: step 33, loss 0.38056, acc 0.84375, learning_rate 0.0047459
2018-04-02T19:16:27.697666: step 34, loss 0.336124, acc 0.875, learning_rate 0.00473818
2018-04-02T19:16:28.787663: step 35, loss 0.267465, acc 0.890625, learning_rate 0.00473046
2018-04-02T19:16:29.882250: step 36, loss 0.231914, acc 0.875, learning_rate 0.00472276
2018-04-02T19:16:30.969369: step 37, loss 0.303871, acc 0.859375, learning_rate 0.00471508
2018-04-02T19:16:32.062425: step 38, loss 0.247517, acc 0.875, learning_rate 0.0047074
2018-04-02T19:16:33.154803: step 39, loss 0.187821, acc 0.9375, learning_rate 0.00469974
2018-04-02T19:16:34.253402: step 40, loss 0.225963, acc 0.9375, learning_rate 0.0046921
2018-04-02T19:16:35.348106: step 41, loss 0.253492, acc 0.953125, learning_rate 0.00468446
2018-04-02T19:16:36.442154: step 42, loss 0.265436, acc 0.875, learning_rate 0.00467684
2018-04-02T19:16:37.526922: step 43, loss 0.21014, acc 0.890625, learning_rate 0.00466923
2018-04-02T19:16:38.621554: step 44, loss 0.185802, acc 0.9375, learning_rate 0.00466163
2018-04-02T19:16:39.708552: step 45, loss 0.235062, acc 0.890625, learning_rate 0.00465405
2018-04-02T19:16:40.801591: step 46, loss 0.239314, acc 0.90625, learning_rate 0.00464648
2018-04-02T19:16:41.896295: step 47, loss 0.225062, acc 0.9375, learning_rate 0.00463892
2018-04-02T19:16:42.983847: step 48, loss 0.321314, acc 0.859375, learning_rate 0.00463137
2018-04-02T19:16:44.077450: step 49, loss 0.1459, acc 0.9375, learning_rate 0.00462383
2018-04-02T19:16:45.173695: step 50, loss 0.179255, acc 0.90625, learning_rate 0.00461631
2018-04-02T19:16:46.259946: step 51, loss 0.233172, acc 0.890625, learning_rate 0.0046088
2018-04-02T19:16:47.355869: step 52, loss 0.28904, acc 0.921875, learning_rate 0.00460131
2018-04-02T19:16:48.442184: step 53, loss 0.180619, acc 0.921875, learning_rate 0.00459382
2018-04-02T19:16:49.539636: step 54, loss 0.231818, acc 0.90625, learning_rate 0.00458635
2018-04-02T19:16:50.636128: step 55, loss 0.13646, acc 0.9375, learning_rate 0.00457889
2018-04-02T19:16:51.730560: step 56, loss 0.217923, acc 0.9375, learning_rate 0.00457144
2018-04-02T19:16:52.820026: step 57, loss 0.0912857, acc 0.96875, learning_rate 0.00456401
2018-04-02T19:16:53.911726: step 58, loss 0.305569, acc 0.890625, learning_rate 0.00455659
2018-04-02T19:16:55.010363: step 59, loss 0.205457, acc 0.90625, learning_rate 0.00454918
2018-04-02T19:16:56.096830: step 60, loss 0.0841473, acc 0.953125, learning_rate 0.00454178
2018-04-02T19:16:57.180628: step 61, loss 0.221345, acc 0.890625, learning_rate 0.0045344
2018-04-02T19:16:58.274060: step 62, loss 0.163809, acc 0.921875, learning_rate 0.00452702
2018-04-02T19:16:59.360892: step 63, loss 0.178377, acc 0.953125, learning_rate 0.00451966
2018-04-02T19:17:00.455921: step 64, loss 0.265291, acc 0.90625, learning_rate 0.00451231
2018-04-02T19:17:01.542455: step 65, loss 0.287963, acc 0.890625, learning_rate 0.00450498
2018-04-02T19:17:02.640462: step 66, loss 0.133067, acc 0.9375, learning_rate 0.00449765
2018-04-02T19:17:03.733880: step 67, loss 0.257768, acc 0.90625, learning_rate 0.00449034
2018-04-02T19:17:04.826413: step 68, loss 0.248644, acc 0.921875, learning_rate 0.00448304
2018-04-02T19:17:05.922463: step 69, loss 0.199449, acc 0.9375, learning_rate 0.00447575
2018-04-02T19:17:07.005683: step 70, loss 0.160575, acc 0.921875, learning_rate 0.00446848
2018-04-02T19:17:08.091904: step 71, loss 0.188031, acc 0.953125, learning_rate 0.00446121
2018-04-02T19:17:09.187946: step 72, loss 0.344756, acc 0.84375, learning_rate 0.00445396
2018-04-02T19:17:10.282560: step 73, loss 0.142997, acc 0.9375, learning_rate 0.00444672
2018-04-02T19:17:11.370865: step 74, loss 0.215049, acc 0.921875, learning_rate 0.0044395
2018-04-02T19:17:12.460846: step 75, loss 0.229838, acc 0.953125, learning_rate 0.00443228
2018-04-02T19:17:13.546313: step 76, loss 0.141597, acc 0.953125, learning_rate 0.00442508
2018-04-02T19:17:14.634143: step 77, loss 0.306112, acc 0.875, learning_rate 0.00441789
2018-04-02T19:17:15.740310: step 78, loss 0.0613934, acc 0.984375, learning_rate 0.00441071
2018-04-02T19:17:16.823300: step 79, loss 0.120913, acc 0.9375, learning_rate 0.00440354
2018-04-02T19:17:17.917699: step 80, loss 0.231887, acc 0.890625, learning_rate 0.00439638
2018-04-02T19:17:19.012171: step 81, loss 0.245655, acc 0.921875, learning_rate 0.00438924
2018-04-02T19:17:20.103362: step 82, loss 0.164938, acc 0.953125, learning_rate 0.00438211
2018-04-02T19:17:21.197794: step 83, loss 0.238561, acc 0.921875, learning_rate 0.00437499
2018-04-02T19:17:22.286592: step 84, loss 0.245422, acc 0.890625, learning_rate 0.00436788
2018-04-02T19:17:23.377428: step 85, loss 0.134854, acc 0.953125, learning_rate 0.00436079
2018-04-02T19:17:24.460292: step 86, loss 0.175752, acc 0.9375, learning_rate 0.0043537
2018-04-02T19:17:25.542034: step 87, loss 0.133053, acc 0.953125, learning_rate 0.00434663
2018-04-02T19:17:26.630830: step 88, loss 0.114533, acc 0.96875, learning_rate 0.00433957
2018-04-02T19:17:27.721755: step 89, loss 0.357473, acc 0.875, learning_rate 0.00433252
2018-04-02T19:17:28.811088: step 90, loss 0.0836689, acc 0.984375, learning_rate 0.00432548
2018-04-02T19:17:29.891858: step 91, loss 0.139713, acc 0.953125, learning_rate 0.00431846
2018-04-02T19:17:30.987818: step 92, loss 0.275234, acc 0.921875, learning_rate 0.00431144
2018-04-02T19:17:32.077268: step 93, loss 0.0847904, acc 0.96875, learning_rate 0.00430444
2018-04-02T19:17:33.158066: step 94, loss 0.0734514, acc 0.984375, learning_rate 0.00429745
2018-04-02T19:17:34.247583: step 95, loss 0.107259, acc 0.96875, learning_rate 0.00429047
2018-04-02T19:17:35.328805: step 96, loss 0.12505, acc 0.96875, learning_rate 0.0042835
2018-04-02T19:17:36.406499: step 97, loss 0.300803, acc 0.90625, learning_rate 0.00427655
2018-04-02T19:17:37.486806: step 98, loss 0.100361, acc 0.953125, learning_rate 0.0042696
2018-04-02T19:17:38.576581: step 99, loss 0.0824117, acc 0.984375, learning_rate 0.00426267
2018-04-02T19:17:39.657888: step 100, loss 0.110305, acc 0.953125, learning_rate 0.00425575

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:17:40.112781: step 100, loss 0.0841131, acc 0.966625

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-100

2018-04-02T19:17:45.170744: step 101, loss 0.092472, acc 0.953125, learning_rate 0.00424884
2018-04-02T19:17:46.262238: step 102, loss 0.299148, acc 0.90625, learning_rate 0.00424194
2018-04-02T19:17:47.354813: step 103, loss 0.104429, acc 0.984375, learning_rate 0.00423505
2018-04-02T19:17:48.436807: step 104, loss 0.238391, acc 0.953125, learning_rate 0.00422818
2018-04-02T19:17:49.516559: step 105, loss 0.133603, acc 0.953125, learning_rate 0.00422131
2018-04-02T19:17:50.603848: step 106, loss 0.291585, acc 0.921875, learning_rate 0.00421446
2018-04-02T19:17:51.687795: step 107, loss 0.20707, acc 0.921875, learning_rate 0.00420762
2018-04-02T19:17:52.780452: step 108, loss 0.174614, acc 0.953125, learning_rate 0.00420079
2018-04-02T19:17:53.859503: step 109, loss 0.141663, acc 0.9375, learning_rate 0.00419397
2018-04-02T19:17:54.939666: step 110, loss 0.0598792, acc 1, learning_rate 0.00418717
2018-04-02T19:17:56.018644: step 111, loss 0.0793259, acc 0.96875, learning_rate 0.00418037
2018-04-02T19:17:57.101795: step 112, loss 0.271183, acc 0.90625, learning_rate 0.00417359
2018-04-02T19:17:58.182565: step 113, loss 0.123926, acc 0.9375, learning_rate 0.00416681
2018-04-02T19:17:59.275948: step 114, loss 0.254635, acc 0.921875, learning_rate 0.00416005
2018-04-02T19:18:00.360263: step 115, loss 0.0846786, acc 0.953125, learning_rate 0.0041533
2018-04-02T19:18:01.455777: step 116, loss 0.444737, acc 0.890625, learning_rate 0.00414656
2018-04-02T19:18:02.541081: step 117, loss 0.0758997, acc 0.953125, learning_rate 0.00413983
2018-04-02T19:18:03.634698: step 118, loss 0.271505, acc 0.921875, learning_rate 0.00413312
2018-04-02T19:18:04.723835: step 119, loss 0.119855, acc 0.984375, learning_rate 0.00412641
2018-04-02T19:18:05.812507: step 120, loss 0.134067, acc 0.9375, learning_rate 0.00411972
2018-04-02T19:18:06.901033: step 121, loss 0.0895781, acc 0.96875, learning_rate 0.00411303
2018-04-02T19:18:07.992917: step 122, loss 0.149858, acc 0.921875, learning_rate 0.00410636
2018-04-02T19:18:09.074654: step 123, loss 0.07602, acc 0.96875, learning_rate 0.0040997
2018-04-02T19:18:10.166535: step 124, loss 0.212109, acc 0.953125, learning_rate 0.00409305
2018-04-02T19:18:11.248408: step 125, loss 0.109647, acc 0.96875, learning_rate 0.00408641
2018-04-02T19:18:12.330817: step 126, loss 0.0699579, acc 0.984375, learning_rate 0.00407978
2018-04-02T19:18:13.412557: step 127, loss 0.266818, acc 0.859375, learning_rate 0.00407316
2018-04-02T19:18:14.495493: step 128, loss 0.229178, acc 0.921875, learning_rate 0.00406656
2018-04-02T19:18:15.584467: step 129, loss 0.0746938, acc 0.984375, learning_rate 0.00405996
2018-04-02T19:18:16.676006: step 130, loss 0.0983188, acc 0.984375, learning_rate 0.00405338
2018-04-02T19:18:17.764080: step 131, loss 0.116123, acc 0.953125, learning_rate 0.00404681
2018-04-02T19:18:18.854180: step 132, loss 0.276061, acc 0.953125, learning_rate 0.00404024
2018-04-02T19:18:19.941726: step 133, loss 0.20867, acc 0.96875, learning_rate 0.00403369
2018-04-02T19:18:21.031017: step 134, loss 0.141707, acc 0.96875, learning_rate 0.00402715
2018-04-02T19:18:22.115195: step 135, loss 0.288721, acc 0.921875, learning_rate 0.00402062
2018-04-02T19:18:23.202674: step 136, loss 0.0625567, acc 0.984375, learning_rate 0.0040141
2018-04-02T19:18:24.282764: step 137, loss 0.173831, acc 0.96875, learning_rate 0.0040076
2018-04-02T19:18:25.373058: step 138, loss 0.186897, acc 0.953125, learning_rate 0.0040011
2018-04-02T19:18:26.453539: step 139, loss 0.193927, acc 0.9375, learning_rate 0.00399461
2018-04-02T19:18:27.537949: step 140, loss 0.147062, acc 0.921875, learning_rate 0.00398814
2018-04-02T19:18:28.625361: step 141, loss 0.0376507, acc 1, learning_rate 0.00398167
2018-04-02T19:18:29.705030: step 142, loss 0.127639, acc 0.953125, learning_rate 0.00397522
2018-04-02T19:18:30.785003: step 143, loss 0.130042, acc 0.953125, learning_rate 0.00396877
2018-04-02T19:18:31.872184: step 144, loss 0.138916, acc 0.9375, learning_rate 0.00396234
2018-04-02T19:18:32.959927: step 145, loss 0.130162, acc 0.953125, learning_rate 0.00395592
2018-04-02T19:18:34.046172: step 146, loss 0.299124, acc 0.921875, learning_rate 0.00394951
2018-04-02T19:18:35.134976: step 147, loss 0.0838313, acc 0.953125, learning_rate 0.00394311
2018-04-02T19:18:36.221235: step 148, loss 0.164435, acc 0.953125, learning_rate 0.00393672
2018-04-02T19:18:37.304975: step 149, loss 0.201068, acc 0.96875, learning_rate 0.00393034
2018-04-02T19:18:38.390425: step 150, loss 0.10309, acc 0.96875, learning_rate 0.00392397
2018-04-02T19:18:39.470422: step 151, loss 0.225012, acc 0.90625, learning_rate 0.00391761
2018-04-02T19:18:40.561935: step 152, loss 0.20089, acc 0.9375, learning_rate 0.00391126
2018-04-02T19:18:41.643208: step 153, loss 0.0976308, acc 0.953125, learning_rate 0.00390493
2018-04-02T19:18:42.734261: step 154, loss 0.178889, acc 0.96875, learning_rate 0.0038986
2018-04-02T19:18:43.820474: step 155, loss 0.0366128, acc 1, learning_rate 0.00389229
2018-04-02T19:18:44.898533: step 156, loss 0.113421, acc 0.953125, learning_rate 0.00388598
2018-04-02T19:18:45.985051: step 157, loss 0.0713343, acc 0.96875, learning_rate 0.00387969
2018-04-02T19:18:47.064280: step 158, loss 0.078499, acc 0.96875, learning_rate 0.0038734
2018-04-02T19:18:48.156873: step 159, loss 0.277546, acc 0.90625, learning_rate 0.00386713
2018-04-02T19:18:49.236067: step 160, loss 0.129599, acc 0.96875, learning_rate 0.00386086
2018-04-02T19:18:50.326057: step 161, loss 0.143618, acc 0.953125, learning_rate 0.00385461
2018-04-02T19:18:51.407640: step 162, loss 0.127391, acc 0.953125, learning_rate 0.00384837
2018-04-02T19:18:52.488011: step 163, loss 0.234069, acc 0.921875, learning_rate 0.00384214
2018-04-02T19:18:53.570967: step 164, loss 0.106453, acc 0.953125, learning_rate 0.00383591
2018-04-02T19:18:54.661598: step 165, loss 0.113701, acc 0.96875, learning_rate 0.0038297
2018-04-02T19:18:55.751194: step 166, loss 0.100252, acc 0.953125, learning_rate 0.0038235
2018-04-02T19:18:56.830526: step 167, loss 0.0940487, acc 0.984375, learning_rate 0.00381731
2018-04-02T19:18:57.909602: step 168, loss 0.108287, acc 0.96875, learning_rate 0.00381113
2018-04-02T19:18:58.999210: step 169, loss 0.157402, acc 0.96875, learning_rate 0.00380496
2018-04-02T19:19:00.077697: step 170, loss 0.27774, acc 0.890625, learning_rate 0.0037988
2018-04-02T19:19:01.158960: step 171, loss 0.0880794, acc 0.96875, learning_rate 0.00379265
2018-04-02T19:19:02.241982: step 172, loss 0.116942, acc 0.96875, learning_rate 0.00378651
2018-04-02T19:19:03.319641: step 173, loss 0.176379, acc 0.90625, learning_rate 0.00378038
2018-04-02T19:19:04.398473: step 174, loss 0.12154, acc 0.96875, learning_rate 0.00377426
2018-04-02T19:19:05.477907: step 175, loss 0.0987127, acc 0.953125, learning_rate 0.00376815
2018-04-02T19:19:06.558443: step 176, loss 0.126674, acc 0.953125, learning_rate 0.00376205
2018-04-02T19:19:07.640211: step 177, loss 0.140579, acc 0.9375, learning_rate 0.00375596
2018-04-02T19:19:08.718105: step 178, loss 0.164443, acc 0.9375, learning_rate 0.00374988
2018-04-02T19:19:09.798347: step 179, loss 0.0925229, acc 0.953125, learning_rate 0.00374382
2018-04-02T19:19:10.876945: step 180, loss 0.125325, acc 0.953125, learning_rate 0.00373776
2018-04-02T19:19:11.956379: step 181, loss 0.160263, acc 0.96875, learning_rate 0.00373171
2018-04-02T19:19:13.041095: step 182, loss 0.0855704, acc 0.953125, learning_rate 0.00372567
2018-04-02T19:19:14.118150: step 183, loss 0.0988602, acc 0.953125, learning_rate 0.00371964
2018-04-02T19:19:15.198794: step 184, loss 0.107857, acc 0.984375, learning_rate 0.00371362
2018-04-02T19:19:16.280054: step 185, loss 0.148657, acc 0.9375, learning_rate 0.00370762
2018-04-02T19:19:17.357766: step 186, loss 0.0487168, acc 1, learning_rate 0.00370162
2018-04-02T19:19:18.439517: step 187, loss 0.0851586, acc 0.96875, learning_rate 0.00369563
2018-04-02T19:19:19.520957: step 188, loss 0.059172, acc 1, learning_rate 0.00368965
2018-04-02T19:19:20.609000: step 189, loss 0.156514, acc 0.96875, learning_rate 0.00368368
2018-04-02T19:19:21.694161: step 190, loss 0.130284, acc 0.96875, learning_rate 0.00367772
2018-04-02T19:19:22.772954: step 191, loss 0.0838629, acc 0.96875, learning_rate 0.00367177
2018-04-02T19:19:23.857946: step 192, loss 0.0508416, acc 0.984375, learning_rate 0.00366584
2018-04-02T19:19:24.936959: step 193, loss 0.249987, acc 0.9375, learning_rate 0.00365991
2018-04-02T19:19:26.022655: step 194, loss 0.0999461, acc 0.9375, learning_rate 0.00365399
2018-04-02T19:19:27.107694: step 195, loss 0.145437, acc 0.9375, learning_rate 0.00364808
2018-04-02T19:19:28.184985: step 196, loss 0.179413, acc 0.921875, learning_rate 0.00364218
2018-04-02T19:19:29.275419: step 197, loss 0.058817, acc 0.984375, learning_rate 0.00363629
2018-04-02T19:19:30.353244: step 198, loss 0.116293, acc 0.953125, learning_rate 0.00363041
2018-04-02T19:19:31.432817: step 199, loss 0.0359685, acc 1, learning_rate 0.00362454
2018-04-02T19:19:32.516846: step 200, loss 0.103401, acc 0.953125, learning_rate 0.00361868

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:19:32.704612: step 200, loss 0.0555514, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-200

2018-04-02T19:19:37.005639: step 201, loss 0.153511, acc 0.96875, learning_rate 0.00361283
2018-04-02T19:19:38.096987: step 202, loss 0.112875, acc 0.953125, learning_rate 0.00360699
2018-04-02T19:19:39.188108: step 203, loss 0.0435244, acc 1, learning_rate 0.00360116
2018-04-02T19:19:40.272731: step 204, loss 0.120477, acc 0.953125, learning_rate 0.00359534
2018-04-02T19:19:41.349442: step 205, loss 0.139688, acc 0.953125, learning_rate 0.00358953
2018-04-02T19:19:42.424757: step 206, loss 0.11948, acc 0.953125, learning_rate 0.00358372
2018-04-02T19:19:43.503959: step 207, loss 0.19573, acc 0.921875, learning_rate 0.00357793
2018-04-02T19:19:44.590948: step 208, loss 0.126616, acc 0.9375, learning_rate 0.00357215
2018-04-02T19:19:45.667072: step 209, loss 0.119721, acc 0.96875, learning_rate 0.00356637
2018-04-02T19:19:46.751560: step 210, loss 0.0940208, acc 0.984375, learning_rate 0.00356061
2018-04-02T19:19:47.835578: step 211, loss 0.02905, acc 1, learning_rate 0.00355486
2018-04-02T19:19:48.914013: step 212, loss 0.10515, acc 0.984375, learning_rate 0.00354911
2018-04-02T19:19:49.999506: step 213, loss 0.0702063, acc 0.984375, learning_rate 0.00354338
2018-04-02T19:19:51.084536: step 214, loss 0.209766, acc 0.921875, learning_rate 0.00353765
2018-04-02T19:19:52.159888: step 215, loss 0.125111, acc 0.921875, learning_rate 0.00353194
2018-04-02T19:19:53.243564: step 216, loss 0.129712, acc 0.953125, learning_rate 0.00352623
2018-04-02T19:19:54.328656: step 217, loss 0.114747, acc 0.953125, learning_rate 0.00352053
2018-04-02T19:19:55.402353: step 218, loss 0.0881053, acc 0.96875, learning_rate 0.00351485
2018-04-02T19:19:56.491463: step 219, loss 0.252419, acc 0.9375, learning_rate 0.00350917
2018-04-02T19:19:57.572934: step 220, loss 0.11774, acc 0.9375, learning_rate 0.0035035
2018-04-02T19:19:58.648021: step 221, loss 0.0607913, acc 0.96875, learning_rate 0.00349784
2018-04-02T19:19:59.725663: step 222, loss 0.141501, acc 0.953125, learning_rate 0.00349219
2018-04-02T19:20:00.802024: step 223, loss 0.0567902, acc 0.984375, learning_rate 0.00348655
2018-04-02T19:20:01.884060: step 224, loss 0.0722701, acc 0.953125, learning_rate 0.00348092
2018-04-02T19:20:02.968802: step 225, loss 0.0350899, acc 1, learning_rate 0.0034753
2018-04-02T19:20:04.051794: step 226, loss 0.118967, acc 0.953125, learning_rate 0.00346969
2018-04-02T19:20:05.140398: step 227, loss 0.0948778, acc 0.96875, learning_rate 0.00346409
2018-04-02T19:20:06.219493: step 228, loss 0.105081, acc 0.96875, learning_rate 0.00345849
2018-04-02T19:20:07.302210: step 229, loss 0.0421448, acc 0.984375, learning_rate 0.00345291
2018-04-02T19:20:08.383730: step 230, loss 0.100049, acc 0.96875, learning_rate 0.00344733
2018-04-02T19:20:09.457533: step 231, loss 0.033103, acc 1, learning_rate 0.00344177
2018-04-02T19:20:10.543878: step 232, loss 0.0233342, acc 1, learning_rate 0.00343621
2018-04-02T19:20:11.620514: step 233, loss 0.136246, acc 0.96875, learning_rate 0.00343066
2018-04-02T19:20:12.704869: step 234, loss 0.127482, acc 0.9375, learning_rate 0.00342513
2018-04-02T19:20:13.780671: step 235, loss 0.0379154, acc 1, learning_rate 0.0034196
2018-04-02T19:20:14.864603: step 236, loss 0.0703814, acc 0.984375, learning_rate 0.00341408
2018-04-02T19:20:15.950640: step 237, loss 0.0856457, acc 0.96875, learning_rate 0.00340857
2018-04-02T19:20:17.034112: step 238, loss 0.0702939, acc 0.953125, learning_rate 0.00340307
2018-04-02T19:20:18.111385: step 239, loss 0.0534496, acc 0.96875, learning_rate 0.00339758
2018-04-02T19:20:19.195329: step 240, loss 0.137906, acc 0.9375, learning_rate 0.00339209
2018-04-02T19:20:20.334549: step 241, loss 0.249837, acc 0.916667, learning_rate 0.00338662
2018-04-02T19:20:21.413229: step 242, loss 0.0457077, acc 0.984375, learning_rate 0.00338115
2018-04-02T19:20:22.499021: step 243, loss 0.0191345, acc 1, learning_rate 0.0033757
2018-04-02T19:20:23.578522: step 244, loss 0.0906669, acc 0.984375, learning_rate 0.00337025
2018-04-02T19:20:24.656820: step 245, loss 0.130645, acc 0.96875, learning_rate 0.00336481
2018-04-02T19:20:25.738973: step 246, loss 0.0831911, acc 0.984375, learning_rate 0.00335939
2018-04-02T19:20:26.811646: step 247, loss 0.0228556, acc 1, learning_rate 0.00335397
2018-04-02T19:20:27.884248: step 248, loss 0.0571895, acc 0.984375, learning_rate 0.00334856
2018-04-02T19:20:28.958804: step 249, loss 0.0811322, acc 0.953125, learning_rate 0.00334316
2018-04-02T19:20:30.104592: step 250, loss 0.0214544, acc 1, learning_rate 0.00333776
2018-04-02T19:20:31.178870: step 251, loss 0.0644338, acc 0.953125, learning_rate 0.00333238
2018-04-02T19:20:32.277785: step 252, loss 0.0160491, acc 1, learning_rate 0.00332701
2018-04-02T19:20:33.356091: step 253, loss 0.0344213, acc 0.984375, learning_rate 0.00332164
2018-04-02T19:20:34.438613: step 254, loss 0.136379, acc 0.96875, learning_rate 0.00331628
2018-04-02T19:20:35.512789: step 255, loss 0.0773036, acc 0.984375, learning_rate 0.00331094
2018-04-02T19:20:36.593943: step 256, loss 0.0172131, acc 1, learning_rate 0.0033056
2018-04-02T19:20:37.668205: step 257, loss 0.0629911, acc 0.984375, learning_rate 0.00330027
2018-04-02T19:20:38.748728: step 258, loss 0.0210664, acc 1, learning_rate 0.00329495
2018-04-02T19:20:39.825888: step 259, loss 0.0699901, acc 0.984375, learning_rate 0.00328963
2018-04-02T19:20:40.911191: step 260, loss 0.0721176, acc 0.984375, learning_rate 0.00328433
2018-04-02T19:20:41.989420: step 261, loss 0.0562771, acc 0.984375, learning_rate 0.00327904
2018-04-02T19:20:43.064382: step 262, loss 0.0109365, acc 1, learning_rate 0.00327375
2018-04-02T19:20:44.147870: step 263, loss 0.0163159, acc 1, learning_rate 0.00326847
2018-04-02T19:20:45.229175: step 264, loss 0.107105, acc 0.96875, learning_rate 0.0032632
2018-04-02T19:20:46.303098: step 265, loss 0.0651998, acc 0.96875, learning_rate 0.00325795
2018-04-02T19:20:47.384232: step 266, loss 0.00782385, acc 1, learning_rate 0.00325269
2018-04-02T19:20:48.470269: step 267, loss 0.0662457, acc 0.96875, learning_rate 0.00324745
2018-04-02T19:20:49.554406: step 268, loss 0.0329492, acc 0.984375, learning_rate 0.00324222
2018-04-02T19:20:50.629153: step 269, loss 0.0174761, acc 1, learning_rate 0.003237
2018-04-02T19:20:51.706236: step 270, loss 0.0352152, acc 0.984375, learning_rate 0.00323178
2018-04-02T19:20:52.796574: step 271, loss 0.0186185, acc 1, learning_rate 0.00322657
2018-04-02T19:20:53.883752: step 272, loss 0.0399386, acc 0.984375, learning_rate 0.00322137
2018-04-02T19:20:54.954616: step 273, loss 0.072377, acc 0.96875, learning_rate 0.00321618
2018-04-02T19:20:56.032278: step 274, loss 0.0520009, acc 0.96875, learning_rate 0.003211
2018-04-02T19:20:57.116070: step 275, loss 0.0270959, acc 1, learning_rate 0.00320583
2018-04-02T19:20:58.189944: step 276, loss 0.0424705, acc 1, learning_rate 0.00320067
2018-04-02T19:20:59.265527: step 277, loss 0.0193184, acc 1, learning_rate 0.00319551
2018-04-02T19:21:00.345367: step 278, loss 0.0698451, acc 0.96875, learning_rate 0.00319036
2018-04-02T19:21:01.434585: step 279, loss 0.0937123, acc 0.953125, learning_rate 0.00318523
2018-04-02T19:21:02.519586: step 280, loss 0.0689538, acc 0.96875, learning_rate 0.0031801
2018-04-02T19:21:03.603150: step 281, loss 0.0788529, acc 0.984375, learning_rate 0.00317497
2018-04-02T19:21:04.682313: step 282, loss 0.104478, acc 0.953125, learning_rate 0.00316986
2018-04-02T19:21:05.758361: step 283, loss 0.0921403, acc 0.984375, learning_rate 0.00316476
2018-04-02T19:21:06.833604: step 284, loss 0.107675, acc 0.96875, learning_rate 0.00315966
2018-04-02T19:21:07.905658: step 285, loss 0.0305142, acc 0.984375, learning_rate 0.00315457
2018-04-02T19:21:08.986230: step 286, loss 0.0125652, acc 1, learning_rate 0.0031495
2018-04-02T19:21:10.070425: step 287, loss 0.0194605, acc 1, learning_rate 0.00314443
2018-04-02T19:21:11.151306: step 288, loss 0.0476316, acc 0.984375, learning_rate 0.00313936
2018-04-02T19:21:12.229655: step 289, loss 0.0440634, acc 1, learning_rate 0.00313431
2018-04-02T19:21:13.308431: step 290, loss 0.0676305, acc 0.984375, learning_rate 0.00312926
2018-04-02T19:21:14.387552: step 291, loss 0.0170696, acc 1, learning_rate 0.00312423
2018-04-02T19:21:15.464984: step 292, loss 0.0198747, acc 1, learning_rate 0.0031192
2018-04-02T19:21:16.543250: step 293, loss 0.0326504, acc 0.984375, learning_rate 0.00311418
2018-04-02T19:21:17.625879: step 294, loss 0.0129432, acc 1, learning_rate 0.00310917
2018-04-02T19:21:18.706688: step 295, loss 0.0505824, acc 0.984375, learning_rate 0.00310416
2018-04-02T19:21:19.779190: step 296, loss 0.0196981, acc 1, learning_rate 0.00309917
2018-04-02T19:21:20.855290: step 297, loss 0.0272081, acc 1, learning_rate 0.00309418
2018-04-02T19:21:21.932573: step 298, loss 0.0694489, acc 0.984375, learning_rate 0.0030892
2018-04-02T19:21:23.008528: step 299, loss 0.0725684, acc 0.96875, learning_rate 0.00308423
2018-04-02T19:21:24.086844: step 300, loss 0.115412, acc 0.984375, learning_rate 0.00307927

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:21:24.275040: step 300, loss 0.0547898, acc 0.982695

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-300

2018-04-02T19:21:29.214831: step 301, loss 0.0102549, acc 1, learning_rate 0.00307432
2018-04-02T19:21:30.287857: step 302, loss 0.00688154, acc 1, learning_rate 0.00306937
2018-04-02T19:21:31.362986: step 303, loss 0.00487856, acc 1, learning_rate 0.00306444
2018-04-02T19:21:32.434518: step 304, loss 0.0517933, acc 0.984375, learning_rate 0.00305951
2018-04-02T19:21:33.506065: step 305, loss 0.061073, acc 0.984375, learning_rate 0.00305459
2018-04-02T19:21:34.579789: step 306, loss 0.0582502, acc 0.96875, learning_rate 0.00304967
2018-04-02T19:21:35.653103: step 307, loss 0.0146857, acc 1, learning_rate 0.00304477
2018-04-02T19:21:36.736529: step 308, loss 0.0434911, acc 0.984375, learning_rate 0.00303987
2018-04-02T19:21:37.814204: step 309, loss 0.0956338, acc 0.953125, learning_rate 0.00303499
2018-04-02T19:21:38.885069: step 310, loss 0.0402071, acc 0.984375, learning_rate 0.00303011
2018-04-02T19:21:39.959817: step 311, loss 0.0318373, acc 1, learning_rate 0.00302523
2018-04-02T19:21:41.036820: step 312, loss 0.072285, acc 0.984375, learning_rate 0.00302037
2018-04-02T19:21:42.119069: step 313, loss 0.0269028, acc 1, learning_rate 0.00301551
2018-04-02T19:21:43.196956: step 314, loss 0.0104361, acc 1, learning_rate 0.00301067
2018-04-02T19:21:44.279399: step 315, loss 0.134691, acc 0.9375, learning_rate 0.00300583
2018-04-02T19:21:45.358085: step 316, loss 0.0139438, acc 1, learning_rate 0.003001
2018-04-02T19:21:46.441294: step 317, loss 0.0230538, acc 1, learning_rate 0.00299617
2018-04-02T19:21:47.526657: step 318, loss 0.0304332, acc 0.984375, learning_rate 0.00299136
2018-04-02T19:21:48.609058: step 319, loss 0.0193557, acc 1, learning_rate 0.00298655
2018-04-02T19:21:49.688333: step 320, loss 0.0909941, acc 0.96875, learning_rate 0.00298175
2018-04-02T19:21:50.762062: step 321, loss 0.0385533, acc 0.984375, learning_rate 0.00297696
2018-04-02T19:21:51.847661: step 322, loss 0.0554619, acc 0.96875, learning_rate 0.00297218
2018-04-02T19:21:52.932062: step 323, loss 0.0229233, acc 1, learning_rate 0.0029674
2018-04-02T19:21:54.007826: step 324, loss 0.0342596, acc 0.984375, learning_rate 0.00296263
2018-04-02T19:21:55.091139: step 325, loss 0.0799155, acc 0.96875, learning_rate 0.00295787
2018-04-02T19:21:56.177989: step 326, loss 0.126394, acc 0.96875, learning_rate 0.00295312
2018-04-02T19:21:57.258254: step 327, loss 0.0132084, acc 1, learning_rate 0.00294838
2018-04-02T19:21:58.329962: step 328, loss 0.0447166, acc 0.984375, learning_rate 0.00294364
2018-04-02T19:21:59.403861: step 329, loss 0.0146188, acc 1, learning_rate 0.00293891
2018-04-02T19:22:00.479341: step 330, loss 0.00872522, acc 1, learning_rate 0.00293419
2018-04-02T19:22:01.561548: step 331, loss 0.0415891, acc 1, learning_rate 0.00292948
2018-04-02T19:22:02.643283: step 332, loss 0.0249849, acc 1, learning_rate 0.00292478
2018-04-02T19:22:03.721068: step 333, loss 0.00603645, acc 1, learning_rate 0.00292008
2018-04-02T19:22:04.794364: step 334, loss 0.0381897, acc 0.984375, learning_rate 0.00291539
2018-04-02T19:22:05.876331: step 335, loss 0.0605271, acc 0.984375, learning_rate 0.00291071
2018-04-02T19:22:06.956569: step 336, loss 0.0314914, acc 0.984375, learning_rate 0.00290604
2018-04-02T19:22:08.034323: step 337, loss 0.0308532, acc 1, learning_rate 0.00290137
2018-04-02T19:22:09.105494: step 338, loss 0.051289, acc 0.984375, learning_rate 0.00289671
2018-04-02T19:22:10.189934: step 339, loss 0.0670733, acc 0.96875, learning_rate 0.00289206
2018-04-02T19:22:11.263565: step 340, loss 0.0132747, acc 1, learning_rate 0.00288742
2018-04-02T19:22:12.340034: step 341, loss 0.102397, acc 0.984375, learning_rate 0.00288279
2018-04-02T19:22:13.424535: step 342, loss 0.0290615, acc 0.984375, learning_rate 0.00287816
2018-04-02T19:22:14.495013: step 343, loss 0.0219763, acc 0.984375, learning_rate 0.00287354
2018-04-02T19:22:15.609680: step 344, loss 0.0435911, acc 0.984375, learning_rate 0.00286893
2018-04-02T19:22:16.688016: step 345, loss 0.0437445, acc 0.984375, learning_rate 0.00286432
2018-04-02T19:22:17.765344: step 346, loss 0.0282834, acc 0.984375, learning_rate 0.00285973
2018-04-02T19:22:18.845766: step 347, loss 0.0323084, acc 1, learning_rate 0.00285514
2018-04-02T19:22:19.925740: step 348, loss 0.0471644, acc 0.984375, learning_rate 0.00285056
2018-04-02T19:22:21.000499: step 349, loss 0.0591126, acc 0.953125, learning_rate 0.00284599
2018-04-02T19:22:22.078798: step 350, loss 0.0439622, acc 0.984375, learning_rate 0.00284142
2018-04-02T19:22:23.155376: step 351, loss 0.0512819, acc 0.984375, learning_rate 0.00283686
2018-04-02T19:22:24.225466: step 352, loss 0.0336623, acc 0.984375, learning_rate 0.00283231
2018-04-02T19:22:25.296419: step 353, loss 0.0879092, acc 0.984375, learning_rate 0.00282777
2018-04-02T19:22:26.367272: step 354, loss 0.0320244, acc 0.984375, learning_rate 0.00282323
2018-04-02T19:22:27.454320: step 355, loss 0.0769679, acc 0.96875, learning_rate 0.0028187
2018-04-02T19:22:28.530196: step 356, loss 0.0343477, acc 0.984375, learning_rate 0.00281418
2018-04-02T19:22:29.612202: step 357, loss 0.0615381, acc 0.96875, learning_rate 0.00280967
2018-04-02T19:22:30.693995: step 358, loss 0.0598714, acc 0.984375, learning_rate 0.00280517
2018-04-02T19:22:31.767080: step 359, loss 0.00641382, acc 1, learning_rate 0.00280067
2018-04-02T19:22:32.835161: step 360, loss 0.0137715, acc 1, learning_rate 0.00279618
2018-04-02T19:22:33.909679: step 361, loss 0.0322893, acc 0.984375, learning_rate 0.0027917
2018-04-02T19:22:34.987037: step 362, loss 0.00503856, acc 1, learning_rate 0.00278722
2018-04-02T19:22:36.072152: step 363, loss 0.0372568, acc 0.984375, learning_rate 0.00278275
2018-04-02T19:22:37.151288: step 364, loss 0.0211029, acc 1, learning_rate 0.00277829
2018-04-02T19:22:38.234308: step 365, loss 0.0184742, acc 1, learning_rate 0.00277384
2018-04-02T19:22:39.317385: step 366, loss 0.027549, acc 1, learning_rate 0.00276939
2018-04-02T19:22:40.399135: step 367, loss 0.0282215, acc 1, learning_rate 0.00276495
2018-04-02T19:22:41.474142: step 368, loss 0.0228617, acc 1, learning_rate 0.00276052
2018-04-02T19:22:42.548562: step 369, loss 0.0176588, acc 1, learning_rate 0.0027561
2018-04-02T19:22:43.629571: step 370, loss 0.00924404, acc 1, learning_rate 0.00275168
2018-04-02T19:22:44.703658: step 371, loss 0.0214759, acc 1, learning_rate 0.00274727
2018-04-02T19:22:45.773523: step 372, loss 0.0291469, acc 0.984375, learning_rate 0.00274287
2018-04-02T19:22:46.852082: step 373, loss 0.0348938, acc 0.984375, learning_rate 0.00273848
2018-04-02T19:22:47.924891: step 374, loss 0.0151983, acc 1, learning_rate 0.00273409
2018-04-02T19:22:49.000788: step 375, loss 0.034624, acc 0.984375, learning_rate 0.00272971
2018-04-02T19:22:50.087152: step 376, loss 0.0189111, acc 1, learning_rate 0.00272534
2018-04-02T19:22:51.166248: step 377, loss 0.0178105, acc 1, learning_rate 0.00272097
2018-04-02T19:22:52.247682: step 378, loss 0.0351831, acc 0.984375, learning_rate 0.00271662
2018-04-02T19:22:53.327109: step 379, loss 0.0268466, acc 0.984375, learning_rate 0.00271227
2018-04-02T19:22:54.413693: step 380, loss 0.0712053, acc 0.96875, learning_rate 0.00270792
2018-04-02T19:22:55.491154: step 381, loss 0.0805168, acc 0.984375, learning_rate 0.00270359
2018-04-02T19:22:56.568900: step 382, loss 0.0502358, acc 0.96875, learning_rate 0.00269926
2018-04-02T19:22:57.644383: step 383, loss 0.0779979, acc 0.96875, learning_rate 0.00269494
2018-04-02T19:22:58.728634: step 384, loss 0.0116359, acc 1, learning_rate 0.00269062
2018-04-02T19:22:59.806321: step 385, loss 0.0211112, acc 1, learning_rate 0.00268631
2018-04-02T19:23:00.877756: step 386, loss 0.0192996, acc 1, learning_rate 0.00268201
2018-04-02T19:23:01.954126: step 387, loss 0.0545403, acc 0.984375, learning_rate 0.00267772
2018-04-02T19:23:03.032619: step 388, loss 0.0372956, acc 0.984375, learning_rate 0.00267343
2018-04-02T19:23:04.118596: step 389, loss 0.0280979, acc 1, learning_rate 0.00266916
2018-04-02T19:23:05.196888: step 390, loss 0.0243291, acc 0.984375, learning_rate 0.00266488
2018-04-02T19:23:06.276760: step 391, loss 0.0125196, acc 1, learning_rate 0.00266062
2018-04-02T19:23:07.350737: step 392, loss 0.0160289, acc 1, learning_rate 0.00265636
2018-04-02T19:23:08.435516: step 393, loss 0.00776673, acc 1, learning_rate 0.00265211
2018-04-02T19:23:09.520937: step 394, loss 0.0425227, acc 0.96875, learning_rate 0.00264787
2018-04-02T19:23:10.605437: step 395, loss 0.0299526, acc 1, learning_rate 0.00264363
2018-04-02T19:23:11.682202: step 396, loss 0.0117693, acc 1, learning_rate 0.0026394
2018-04-02T19:23:12.758165: step 397, loss 0.0597594, acc 0.984375, learning_rate 0.00263518
2018-04-02T19:23:13.843498: step 398, loss 0.0180931, acc 1, learning_rate 0.00263097
2018-04-02T19:23:14.927138: step 399, loss 0.0212584, acc 0.984375, learning_rate 0.00262676
2018-04-02T19:23:16.004617: step 400, loss 0.0374444, acc 0.984375, learning_rate 0.00262256

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:23:16.193086: step 400, loss 0.0413131, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-400

2018-04-02T19:23:20.423784: step 401, loss 0.141333, acc 0.96875, learning_rate 0.00261836
2018-04-02T19:23:21.509541: step 402, loss 0.00608929, acc 1, learning_rate 0.00261417
2018-04-02T19:23:22.584026: step 403, loss 0.0137075, acc 0.984375, learning_rate 0.00260999
2018-04-02T19:23:23.656675: step 404, loss 0.0119436, acc 1, learning_rate 0.00260582
2018-04-02T19:23:24.729774: step 405, loss 0.074478, acc 0.984375, learning_rate 0.00260165
2018-04-02T19:23:25.801707: step 406, loss 0.0177007, acc 1, learning_rate 0.0025975
2018-04-02T19:23:26.878846: step 407, loss 0.0562499, acc 0.984375, learning_rate 0.00259334
2018-04-02T19:23:27.954724: step 408, loss 0.042696, acc 0.984375, learning_rate 0.0025892
2018-04-02T19:23:29.028431: step 409, loss 0.0167891, acc 1, learning_rate 0.00258506
2018-04-02T19:23:30.107303: step 410, loss 0.198584, acc 0.953125, learning_rate 0.00258093
2018-04-02T19:23:31.180966: step 411, loss 0.0162006, acc 1, learning_rate 0.0025768
2018-04-02T19:23:32.252890: step 412, loss 0.110695, acc 0.96875, learning_rate 0.00257268
2018-04-02T19:23:33.330903: step 413, loss 0.0194516, acc 1, learning_rate 0.00256857
2018-04-02T19:23:34.404629: step 414, loss 0.11663, acc 0.96875, learning_rate 0.00256447
2018-04-02T19:23:35.480661: step 415, loss 0.0318001, acc 0.984375, learning_rate 0.00256037
2018-04-02T19:23:36.557470: step 416, loss 0.0133779, acc 1, learning_rate 0.00255628
2018-04-02T19:23:37.636129: step 417, loss 0.0160588, acc 1, learning_rate 0.00255219
2018-04-02T19:23:38.710372: step 418, loss 0.0707266, acc 0.984375, learning_rate 0.00254812
2018-04-02T19:23:39.788247: step 419, loss 0.080869, acc 0.96875, learning_rate 0.00254405
2018-04-02T19:23:40.867212: step 420, loss 0.036893, acc 0.984375, learning_rate 0.00253998
2018-04-02T19:23:41.940291: step 421, loss 0.0219341, acc 0.984375, learning_rate 0.00253593
2018-04-02T19:23:43.013950: step 422, loss 0.00745541, acc 1, learning_rate 0.00253188
2018-04-02T19:23:44.088413: step 423, loss 0.0714518, acc 0.984375, learning_rate 0.00252783
2018-04-02T19:23:45.165450: step 424, loss 0.0300277, acc 0.984375, learning_rate 0.0025238
2018-04-02T19:23:46.250481: step 425, loss 0.0357255, acc 1, learning_rate 0.00251977
2018-04-02T19:23:47.330738: step 426, loss 0.0176783, acc 1, learning_rate 0.00251574
2018-04-02T19:23:48.412802: step 427, loss 0.0238417, acc 0.984375, learning_rate 0.00251173
2018-04-02T19:23:49.489770: step 428, loss 0.00731359, acc 1, learning_rate 0.00250772
2018-04-02T19:23:50.564463: step 429, loss 0.0675696, acc 0.96875, learning_rate 0.00250371
2018-04-02T19:23:51.638608: step 430, loss 0.0124608, acc 1, learning_rate 0.00249972
2018-04-02T19:23:52.711544: step 431, loss 0.00512238, acc 1, learning_rate 0.00249573
2018-04-02T19:23:53.783949: step 432, loss 0.125583, acc 0.953125, learning_rate 0.00249174
2018-04-02T19:23:54.861777: step 433, loss 0.0319277, acc 0.984375, learning_rate 0.00248777
2018-04-02T19:23:55.938227: step 434, loss 0.0130749, acc 1, learning_rate 0.0024838
2018-04-02T19:23:57.011398: step 435, loss 0.0638532, acc 0.96875, learning_rate 0.00247983
2018-04-02T19:23:58.087098: step 436, loss 0.018068, acc 1, learning_rate 0.00247588
2018-04-02T19:23:59.160954: step 437, loss 0.0277515, acc 1, learning_rate 0.00247193
2018-04-02T19:24:00.233218: step 438, loss 0.0162352, acc 1, learning_rate 0.00246798
2018-04-02T19:24:01.313708: step 439, loss 0.0477524, acc 0.984375, learning_rate 0.00246404
2018-04-02T19:24:02.397590: step 440, loss 0.0252816, acc 1, learning_rate 0.00246011
2018-04-02T19:24:03.474321: step 441, loss 0.0289982, acc 0.984375, learning_rate 0.00245619
2018-04-02T19:24:04.557872: step 442, loss 0.0567205, acc 0.96875, learning_rate 0.00245227
2018-04-02T19:24:05.645820: step 443, loss 0.0619568, acc 0.96875, learning_rate 0.00244836
2018-04-02T19:24:06.729017: step 444, loss 0.0209332, acc 1, learning_rate 0.00244446
2018-04-02T19:24:07.805516: step 445, loss 0.0201926, acc 1, learning_rate 0.00244056
2018-04-02T19:24:08.891084: step 446, loss 0.0632406, acc 0.953125, learning_rate 0.00243667
2018-04-02T19:24:09.973801: step 447, loss 0.0650235, acc 0.984375, learning_rate 0.00243278
2018-04-02T19:24:11.058644: step 448, loss 0.0562592, acc 0.984375, learning_rate 0.0024289
2018-04-02T19:24:12.143598: step 449, loss 0.0105017, acc 1, learning_rate 0.00242503
2018-04-02T19:24:13.218551: step 450, loss 0.0214738, acc 1, learning_rate 0.00242117
2018-04-02T19:24:14.289229: step 451, loss 0.0108076, acc 1, learning_rate 0.00241731
2018-04-02T19:24:15.361540: step 452, loss 0.0423864, acc 0.984375, learning_rate 0.00241345
2018-04-02T19:24:16.429093: step 453, loss 0.0234105, acc 1, learning_rate 0.00240961
2018-04-02T19:24:17.497920: step 454, loss 0.0173447, acc 1, learning_rate 0.00240577
2018-04-02T19:24:18.567699: step 455, loss 0.0099657, acc 1, learning_rate 0.00240193
2018-04-02T19:24:19.642397: step 456, loss 0.0850824, acc 0.953125, learning_rate 0.00239811
2018-04-02T19:24:20.716121: step 457, loss 0.0378578, acc 0.96875, learning_rate 0.00239428
2018-04-02T19:24:21.793977: step 458, loss 0.0444756, acc 0.984375, learning_rate 0.00239047
2018-04-02T19:24:22.867837: step 459, loss 0.0106604, acc 1, learning_rate 0.00238666
2018-04-02T19:24:23.944481: step 460, loss 0.0550252, acc 0.96875, learning_rate 0.00238286
2018-04-02T19:24:25.017072: step 461, loss 0.0480443, acc 0.96875, learning_rate 0.00237906
2018-04-02T19:24:26.089285: step 462, loss 0.0244413, acc 1, learning_rate 0.00237527
2018-04-02T19:24:27.160146: step 463, loss 0.0699406, acc 0.984375, learning_rate 0.00237149
2018-04-02T19:24:28.231745: step 464, loss 0.0094932, acc 1, learning_rate 0.00236771
2018-04-02T19:24:29.306217: step 465, loss 0.00501453, acc 1, learning_rate 0.00236394
2018-04-02T19:24:30.377667: step 466, loss 0.0210593, acc 1, learning_rate 0.00236018
2018-04-02T19:24:31.455942: step 467, loss 0.0917258, acc 0.96875, learning_rate 0.00235642
2018-04-02T19:24:32.528482: step 468, loss 0.018407, acc 1, learning_rate 0.00235267
2018-04-02T19:24:33.605879: step 469, loss 0.074259, acc 0.96875, learning_rate 0.00234892
2018-04-02T19:24:34.682078: step 470, loss 0.0635436, acc 0.984375, learning_rate 0.00234519
2018-04-02T19:24:35.761435: step 471, loss 0.0602458, acc 0.984375, learning_rate 0.00234145
2018-04-02T19:24:36.838244: step 472, loss 0.0182251, acc 1, learning_rate 0.00233773
2018-04-02T19:24:37.922407: step 473, loss 0.00421749, acc 1, learning_rate 0.002334
2018-04-02T19:24:38.996730: step 474, loss 0.0345173, acc 0.984375, learning_rate 0.00233029
2018-04-02T19:24:40.073208: step 475, loss 0.00811462, acc 1, learning_rate 0.00232658
2018-04-02T19:24:41.157761: step 476, loss 0.00963491, acc 1, learning_rate 0.00232288
2018-04-02T19:24:42.230651: step 477, loss 0.0112434, acc 1, learning_rate 0.00231918
2018-04-02T19:24:43.316379: step 478, loss 0.0155148, acc 1, learning_rate 0.00231549
2018-04-02T19:24:44.393063: step 479, loss 0.0136974, acc 1, learning_rate 0.00231181
2018-04-02T19:24:45.476186: step 480, loss 0.0404321, acc 0.96875, learning_rate 0.00230813
2018-04-02T19:24:46.550140: step 481, loss 0.0373503, acc 0.984375, learning_rate 0.00230446
2018-04-02T19:24:47.589491: step 482, loss 0.0341829, acc 1, learning_rate 0.0023008
2018-04-02T19:24:48.674778: step 483, loss 0.017693, acc 0.984375, learning_rate 0.00229714
2018-04-02T19:24:49.758400: step 484, loss 0.00718584, acc 1, learning_rate 0.00229348
2018-04-02T19:24:50.835524: step 485, loss 0.00432036, acc 1, learning_rate 0.00228984
2018-04-02T19:24:51.917947: step 486, loss 0.0111996, acc 1, learning_rate 0.0022862
2018-04-02T19:24:53.002235: step 487, loss 0.00396399, acc 1, learning_rate 0.00228256
2018-04-02T19:24:54.076385: step 488, loss 0.0333839, acc 0.984375, learning_rate 0.00227893
2018-04-02T19:24:55.162642: step 489, loss 0.0134408, acc 1, learning_rate 0.00227531
2018-04-02T19:24:56.236429: step 490, loss 0.0198897, acc 1, learning_rate 0.00227169
2018-04-02T19:24:57.313968: step 491, loss 0.0757188, acc 0.984375, learning_rate 0.00226808
2018-04-02T19:24:58.387081: step 492, loss 0.0082044, acc 1, learning_rate 0.00226448
2018-04-02T19:24:59.460089: step 493, loss 0.00468583, acc 1, learning_rate 0.00226088
2018-04-02T19:25:00.533807: step 494, loss 0.0121552, acc 1, learning_rate 0.00225728
2018-04-02T19:25:01.617175: step 495, loss 0.0843867, acc 0.984375, learning_rate 0.0022537
2018-04-02T19:25:02.701604: step 496, loss 0.0182851, acc 1, learning_rate 0.00225012
2018-04-02T19:25:03.774623: step 497, loss 0.0181678, acc 0.984375, learning_rate 0.00224654
2018-04-02T19:25:04.848347: step 498, loss 0.00599004, acc 1, learning_rate 0.00224297
2018-04-02T19:25:05.931552: step 499, loss 0.0104636, acc 1, learning_rate 0.00223941
2018-04-02T19:25:07.002789: step 500, loss 0.00526529, acc 1, learning_rate 0.00223585

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:25:07.191718: step 500, loss 0.0405516, acc 0.982695

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-500

2018-04-02T19:25:11.046786: step 501, loss 0.00483189, acc 1, learning_rate 0.0022323
2018-04-02T19:25:12.127172: step 502, loss 0.00275926, acc 1, learning_rate 0.00222876
2018-04-02T19:25:13.206477: step 503, loss 0.0110226, acc 1, learning_rate 0.00222522
2018-04-02T19:25:14.283865: step 504, loss 0.0192695, acc 0.984375, learning_rate 0.00222168
2018-04-02T19:25:15.363536: step 505, loss 0.022615, acc 0.984375, learning_rate 0.00221816
2018-04-02T19:25:16.435685: step 506, loss 0.0427176, acc 0.984375, learning_rate 0.00221463
2018-04-02T19:25:17.509065: step 507, loss 0.00754122, acc 1, learning_rate 0.00221112
2018-04-02T19:25:18.581461: step 508, loss 0.0102456, acc 1, learning_rate 0.00220761
2018-04-02T19:25:19.657885: step 509, loss 0.0145547, acc 1, learning_rate 0.0022041
2018-04-02T19:25:20.735690: step 510, loss 0.00289963, acc 1, learning_rate 0.0022006
2018-04-02T19:25:21.809656: step 511, loss 0.00837741, acc 1, learning_rate 0.00219711
2018-04-02T19:25:22.884829: step 512, loss 0.040236, acc 0.984375, learning_rate 0.00219363
2018-04-02T19:25:23.971086: step 513, loss 0.0367761, acc 0.984375, learning_rate 0.00219014
2018-04-02T19:25:25.050168: step 514, loss 0.00533956, acc 1, learning_rate 0.00218667
2018-04-02T19:25:26.130880: step 515, loss 0.0434091, acc 0.984375, learning_rate 0.0021832
2018-04-02T19:25:27.200997: step 516, loss 0.0476457, acc 0.984375, learning_rate 0.00217974
2018-04-02T19:25:28.274638: step 517, loss 0.00166051, acc 1, learning_rate 0.00217628
2018-04-02T19:25:29.354821: step 518, loss 0.00511097, acc 1, learning_rate 0.00217283
2018-04-02T19:25:30.438483: step 519, loss 0.0115074, acc 1, learning_rate 0.00216938
2018-04-02T19:25:31.523880: step 520, loss 0.00647942, acc 1, learning_rate 0.00216594
2018-04-02T19:25:32.607609: step 521, loss 0.0220533, acc 0.984375, learning_rate 0.0021625
2018-04-02T19:25:33.691734: step 522, loss 0.0239008, acc 0.984375, learning_rate 0.00215907
2018-04-02T19:25:34.776667: step 523, loss 0.00538026, acc 1, learning_rate 0.00215565
2018-04-02T19:25:35.866155: step 524, loss 0.0028241, acc 1, learning_rate 0.00215223
2018-04-02T19:25:36.950678: step 525, loss 0.0434642, acc 0.984375, learning_rate 0.00214882
2018-04-02T19:25:38.022784: step 526, loss 0.00215577, acc 1, learning_rate 0.00214541
2018-04-02T19:25:39.104587: step 527, loss 0.00296242, acc 1, learning_rate 0.00214201
2018-04-02T19:25:40.177595: step 528, loss 0.0141892, acc 1, learning_rate 0.00213862
2018-04-02T19:25:41.253344: step 529, loss 0.00753242, acc 1, learning_rate 0.00213523
2018-04-02T19:25:42.333938: step 530, loss 0.00927989, acc 1, learning_rate 0.00213184
2018-04-02T19:25:43.417228: step 531, loss 0.00513958, acc 1, learning_rate 0.00212847
2018-04-02T19:25:44.491623: step 532, loss 0.00634273, acc 1, learning_rate 0.00212509
2018-04-02T19:25:45.566608: step 533, loss 0.0100922, acc 1, learning_rate 0.00212173
2018-04-02T19:25:46.642313: step 534, loss 0.0234897, acc 1, learning_rate 0.00211836
2018-04-02T19:25:47.716708: step 535, loss 0.00753963, acc 1, learning_rate 0.00211501
2018-04-02T19:25:48.798320: step 536, loss 0.00558077, acc 1, learning_rate 0.00211166
2018-04-02T19:25:49.884796: step 537, loss 0.00766658, acc 1, learning_rate 0.00210831
2018-04-02T19:25:50.959180: step 538, loss 0.0305961, acc 0.984375, learning_rate 0.00210497
2018-04-02T19:25:52.034901: step 539, loss 0.0216563, acc 0.984375, learning_rate 0.00210164
2018-04-02T19:25:53.121237: step 540, loss 0.00931096, acc 1, learning_rate 0.00209831
2018-04-02T19:25:54.197490: step 541, loss 0.017326, acc 1, learning_rate 0.00209499
2018-04-02T19:25:55.281175: step 542, loss 0.0467243, acc 0.984375, learning_rate 0.00209167
2018-04-02T19:25:56.354433: step 543, loss 0.0127374, acc 1, learning_rate 0.00208836
2018-04-02T19:25:57.429335: step 544, loss 0.0111029, acc 1, learning_rate 0.00208506
2018-04-02T19:25:58.503754: step 545, loss 0.00239498, acc 1, learning_rate 0.00208176
2018-04-02T19:25:59.587431: step 546, loss 0.0229386, acc 0.984375, learning_rate 0.00207846
2018-04-02T19:26:00.673140: step 547, loss 0.0318352, acc 0.984375, learning_rate 0.00207517
2018-04-02T19:26:01.756534: step 548, loss 0.0114914, acc 1, learning_rate 0.00207189
2018-04-02T19:26:02.831991: step 549, loss 0.00268194, acc 1, learning_rate 0.00206861
2018-04-02T19:26:03.915793: step 550, loss 0.0591336, acc 0.96875, learning_rate 0.00206533
2018-04-02T19:26:04.994937: step 551, loss 0.00522691, acc 1, learning_rate 0.00206207
2018-04-02T19:26:06.079944: step 552, loss 0.00449244, acc 1, learning_rate 0.0020588
2018-04-02T19:26:07.164754: step 553, loss 0.0108709, acc 1, learning_rate 0.00205555
2018-04-02T19:26:08.241260: step 554, loss 0.00270133, acc 1, learning_rate 0.0020523
2018-04-02T19:26:09.322052: step 555, loss 0.010299, acc 1, learning_rate 0.00204905
2018-04-02T19:26:10.395751: step 556, loss 0.00633065, acc 1, learning_rate 0.00204581
2018-04-02T19:26:11.470056: step 557, loss 0.010172, acc 1, learning_rate 0.00204257
2018-04-02T19:26:12.552595: step 558, loss 0.00484168, acc 1, learning_rate 0.00203934
2018-04-02T19:26:13.626321: step 559, loss 0.0164194, acc 1, learning_rate 0.00203612
2018-04-02T19:26:14.697587: step 560, loss 0.0274527, acc 0.984375, learning_rate 0.0020329
2018-04-02T19:26:15.771092: step 561, loss 0.0224106, acc 1, learning_rate 0.00202969
2018-04-02T19:26:16.844317: step 562, loss 0.0313633, acc 0.984375, learning_rate 0.00202648
2018-04-02T19:26:17.923751: step 563, loss 0.0436782, acc 0.984375, learning_rate 0.00202328
2018-04-02T19:26:19.014509: step 564, loss 0.00882076, acc 1, learning_rate 0.00202008
2018-04-02T19:26:20.095568: step 565, loss 0.114078, acc 0.984375, learning_rate 0.00201689
2018-04-02T19:26:21.177994: step 566, loss 0.0102838, acc 1, learning_rate 0.0020137
2018-04-02T19:26:22.252916: step 567, loss 0.0054991, acc 1, learning_rate 0.00201052
2018-04-02T19:26:23.333629: step 568, loss 0.00661589, acc 1, learning_rate 0.00200734
2018-04-02T19:26:24.409180: step 569, loss 0.0176228, acc 1, learning_rate 0.00200417
2018-04-02T19:26:25.491979: step 570, loss 0.0542188, acc 0.984375, learning_rate 0.002001
2018-04-02T19:26:26.572417: step 571, loss 0.00528339, acc 1, learning_rate 0.00199784
2018-04-02T19:26:27.658418: step 572, loss 0.0049906, acc 1, learning_rate 0.00199469
2018-04-02T19:26:28.744967: step 573, loss 0.00355417, acc 1, learning_rate 0.00199154
2018-04-02T19:26:29.817468: step 574, loss 0.0124099, acc 1, learning_rate 0.00198839
2018-04-02T19:26:30.899936: step 575, loss 0.0150906, acc 1, learning_rate 0.00198525
2018-04-02T19:26:31.977248: step 576, loss 0.00312266, acc 1, learning_rate 0.00198212
2018-04-02T19:26:33.057685: step 577, loss 0.0760143, acc 0.984375, learning_rate 0.00197899
2018-04-02T19:26:34.136549: step 578, loss 0.00450614, acc 1, learning_rate 0.00197586
2018-04-02T19:26:35.215067: step 579, loss 0.0588588, acc 0.984375, learning_rate 0.00197274
2018-04-02T19:26:36.299940: step 580, loss 0.0036621, acc 1, learning_rate 0.00196963
2018-04-02T19:26:37.376319: step 581, loss 0.00307335, acc 1, learning_rate 0.00196652
2018-04-02T19:26:38.451773: step 582, loss 0.0016246, acc 1, learning_rate 0.00196342
2018-04-02T19:26:39.535271: step 583, loss 0.00622074, acc 1, learning_rate 0.00196032
2018-04-02T19:26:40.617637: step 584, loss 0.00262863, acc 1, learning_rate 0.00195723
2018-04-02T19:26:41.700629: step 585, loss 0.00287578, acc 1, learning_rate 0.00195414
2018-04-02T19:26:42.777882: step 586, loss 0.00260599, acc 1, learning_rate 0.00195106
2018-04-02T19:26:43.850020: step 587, loss 0.00885201, acc 1, learning_rate 0.00194798
2018-04-02T19:26:44.922028: step 588, loss 0.0769104, acc 0.984375, learning_rate 0.00194491
2018-04-02T19:26:45.999402: step 589, loss 0.00814064, acc 1, learning_rate 0.00194184
2018-04-02T19:26:47.070001: step 590, loss 0.0198625, acc 0.984375, learning_rate 0.00193878
2018-04-02T19:26:48.156036: step 591, loss 0.0116382, acc 1, learning_rate 0.00193572
2018-04-02T19:26:49.235249: step 592, loss 0.00392033, acc 1, learning_rate 0.00193267
2018-04-02T19:26:50.303164: step 593, loss 0.0191767, acc 0.984375, learning_rate 0.00192962
2018-04-02T19:26:51.376725: step 594, loss 0.00329283, acc 1, learning_rate 0.00192658
2018-04-02T19:26:52.460334: step 595, loss 0.0123958, acc 1, learning_rate 0.00192354
2018-04-02T19:26:53.534082: step 596, loss 0.0164719, acc 0.984375, learning_rate 0.00192051
2018-04-02T19:26:54.617769: step 597, loss 0.064502, acc 0.984375, learning_rate 0.00191748
2018-04-02T19:26:55.698523: step 598, loss 0.00526226, acc 1, learning_rate 0.00191446
2018-04-02T19:26:56.776106: step 599, loss 0.0251894, acc 0.984375, learning_rate 0.00191144
2018-04-02T19:26:57.862036: step 600, loss 0.00848343, acc 1, learning_rate 0.00190843

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:26:58.051055: step 600, loss 0.0437497, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-600

2018-04-02T19:27:01.970180: step 601, loss 0.0058269, acc 1, learning_rate 0.00190542
2018-04-02T19:27:03.044768: step 602, loss 0.00170773, acc 1, learning_rate 0.00190242
2018-04-02T19:27:04.117264: step 603, loss 0.00245235, acc 1, learning_rate 0.00189942
2018-04-02T19:27:05.193264: step 604, loss 0.00274607, acc 1, learning_rate 0.00189643
2018-04-02T19:27:06.273764: step 605, loss 0.00341998, acc 1, learning_rate 0.00189345
2018-04-02T19:27:07.344268: step 606, loss 0.00468499, acc 1, learning_rate 0.00189046
2018-04-02T19:27:08.423163: step 607, loss 0.00410104, acc 1, learning_rate 0.00188749
2018-04-02T19:27:09.502068: step 608, loss 0.00811844, acc 1, learning_rate 0.00188452
2018-04-02T19:27:10.573433: step 609, loss 0.0106284, acc 1, learning_rate 0.00188155
2018-04-02T19:27:11.647643: step 610, loss 0.00590808, acc 1, learning_rate 0.00187859
2018-04-02T19:27:12.720740: step 611, loss 0.00483148, acc 1, learning_rate 0.00187563
2018-04-02T19:27:13.801899: step 612, loss 0.0186724, acc 1, learning_rate 0.00187268
2018-04-02T19:27:14.876247: step 613, loss 0.00378674, acc 1, learning_rate 0.00186973
2018-04-02T19:27:15.948534: step 614, loss 0.057161, acc 0.984375, learning_rate 0.00186679
2018-04-02T19:27:17.025696: step 615, loss 0.00504024, acc 1, learning_rate 0.00186385
2018-04-02T19:27:18.107848: step 616, loss 0.02473, acc 0.984375, learning_rate 0.00186092
2018-04-02T19:27:19.189583: step 617, loss 0.018532, acc 0.984375, learning_rate 0.00185799
2018-04-02T19:27:20.259155: step 618, loss 0.013031, acc 1, learning_rate 0.00185507
2018-04-02T19:27:21.332940: step 619, loss 0.00472216, acc 1, learning_rate 0.00185215
2018-04-02T19:27:22.405209: step 620, loss 0.00472505, acc 1, learning_rate 0.00184923
2018-04-02T19:27:23.472705: step 621, loss 0.0134801, acc 1, learning_rate 0.00184633
2018-04-02T19:27:24.555423: step 622, loss 0.0835069, acc 0.984375, learning_rate 0.00184342
2018-04-02T19:27:25.629832: step 623, loss 0.00632125, acc 1, learning_rate 0.00184052
2018-04-02T19:27:26.707818: step 624, loss 0.0688672, acc 0.984375, learning_rate 0.00183763
2018-04-02T19:27:27.791304: step 625, loss 0.00392933, acc 1, learning_rate 0.00183474
2018-04-02T19:27:28.863730: step 626, loss 0.0042644, acc 1, learning_rate 0.00183186
2018-04-02T19:27:29.939629: step 627, loss 0.00424421, acc 1, learning_rate 0.00182898
2018-04-02T19:27:31.016821: step 628, loss 0.0812916, acc 0.96875, learning_rate 0.0018261
2018-04-02T19:27:32.088016: step 629, loss 0.00523122, acc 1, learning_rate 0.00182323
2018-04-02T19:27:33.158107: step 630, loss 0.00933658, acc 1, learning_rate 0.00182037
2018-04-02T19:27:34.234567: step 631, loss 0.0102303, acc 1, learning_rate 0.00181751
2018-04-02T19:27:35.314659: step 632, loss 0.0309719, acc 0.984375, learning_rate 0.00181465
2018-04-02T19:27:36.389152: step 633, loss 0.00263921, acc 1, learning_rate 0.0018118
2018-04-02T19:27:37.463265: step 634, loss 0.00568866, acc 1, learning_rate 0.00180895
2018-04-02T19:27:38.547910: step 635, loss 0.0286458, acc 0.984375, learning_rate 0.00180611
2018-04-02T19:27:39.624803: step 636, loss 0.00738727, acc 1, learning_rate 0.00180328
2018-04-02T19:27:40.706336: step 637, loss 0.012587, acc 1, learning_rate 0.00180044
2018-04-02T19:27:41.782158: step 638, loss 0.00845974, acc 1, learning_rate 0.00179762
2018-04-02T19:27:42.854327: step 639, loss 0.0113891, acc 1, learning_rate 0.00179479
2018-04-02T19:27:43.926717: step 640, loss 0.00746349, acc 1, learning_rate 0.00179198
2018-04-02T19:27:45.008376: step 641, loss 0.0106913, acc 1, learning_rate 0.00178916
2018-04-02T19:27:46.087745: step 642, loss 0.00538578, acc 1, learning_rate 0.00178635
2018-04-02T19:27:47.164098: step 643, loss 0.0221522, acc 1, learning_rate 0.00178355
2018-04-02T19:27:48.243425: step 644, loss 0.00924005, acc 1, learning_rate 0.00178075
2018-04-02T19:27:49.321417: step 645, loss 0.0121977, acc 1, learning_rate 0.00177796
2018-04-02T19:27:50.400061: step 646, loss 0.0665604, acc 0.96875, learning_rate 0.00177517
2018-04-02T19:27:51.474222: step 647, loss 0.0103518, acc 1, learning_rate 0.00177238
2018-04-02T19:27:52.550347: step 648, loss 0.0261084, acc 0.984375, learning_rate 0.0017696
2018-04-02T19:27:53.622994: step 649, loss 0.00564631, acc 1, learning_rate 0.00176682
2018-04-02T19:27:54.696031: step 650, loss 0.0571956, acc 0.984375, learning_rate 0.00176405
2018-04-02T19:27:55.773779: step 651, loss 0.00729244, acc 1, learning_rate 0.00176129
2018-04-02T19:27:56.846514: step 652, loss 0.0823278, acc 0.984375, learning_rate 0.00175852
2018-04-02T19:27:57.919062: step 653, loss 0.00633639, acc 1, learning_rate 0.00175577
2018-04-02T19:27:59.014158: step 654, loss 0.00484653, acc 1, learning_rate 0.00175301
2018-04-02T19:28:00.093327: step 655, loss 0.0126728, acc 1, learning_rate 0.00175026
2018-04-02T19:28:01.173176: step 656, loss 0.00438863, acc 1, learning_rate 0.00174752
2018-04-02T19:28:02.255195: step 657, loss 0.0248098, acc 0.984375, learning_rate 0.00174478
2018-04-02T19:28:03.328830: step 658, loss 0.00488005, acc 1, learning_rate 0.00174205
2018-04-02T19:28:04.399345: step 659, loss 0.00561632, acc 1, learning_rate 0.00173932
2018-04-02T19:28:05.483435: step 660, loss 0.00553411, acc 1, learning_rate 0.00173659
2018-04-02T19:28:06.557794: step 661, loss 0.00658065, acc 1, learning_rate 0.00173387
2018-04-02T19:28:07.633239: step 662, loss 0.00493485, acc 1, learning_rate 0.00173115
2018-04-02T19:28:08.717931: step 663, loss 0.00676244, acc 1, learning_rate 0.00172844
2018-04-02T19:28:09.801714: step 664, loss 0.00560557, acc 1, learning_rate 0.00172573
2018-04-02T19:28:10.874213: step 665, loss 0.0402905, acc 0.984375, learning_rate 0.00172303
2018-04-02T19:28:11.958674: step 666, loss 0.0119351, acc 1, learning_rate 0.00172033
2018-04-02T19:28:13.030560: step 667, loss 0.00334522, acc 1, learning_rate 0.00171764
2018-04-02T19:28:14.110043: step 668, loss 0.00544558, acc 1, learning_rate 0.00171495
2018-04-02T19:28:15.185712: step 669, loss 0.00496105, acc 1, learning_rate 0.00171226
2018-04-02T19:28:16.261953: step 670, loss 0.00536222, acc 1, learning_rate 0.00170958
2018-04-02T19:28:17.338504: step 671, loss 0.0169353, acc 0.984375, learning_rate 0.00170691
2018-04-02T19:28:18.410077: step 672, loss 0.00453538, acc 1, learning_rate 0.00170423
2018-04-02T19:28:19.490761: step 673, loss 0.00813148, acc 1, learning_rate 0.00170157
2018-04-02T19:28:20.560783: step 674, loss 0.00546985, acc 1, learning_rate 0.0016989
2018-04-02T19:28:21.631827: step 675, loss 0.0105036, acc 1, learning_rate 0.00169625
2018-04-02T19:28:22.711889: step 676, loss 0.00254251, acc 1, learning_rate 0.00169359
2018-04-02T19:28:23.786270: step 677, loss 0.0544023, acc 0.984375, learning_rate 0.00169094
2018-04-02T19:28:24.857296: step 678, loss 0.00621121, acc 1, learning_rate 0.0016883
2018-04-02T19:28:25.936026: step 679, loss 0.00852722, acc 1, learning_rate 0.00168566
2018-04-02T19:28:27.014592: step 680, loss 0.0078136, acc 1, learning_rate 0.00168302
2018-04-02T19:28:28.090937: step 681, loss 0.00468961, acc 1, learning_rate 0.00168039
2018-04-02T19:28:29.163826: step 682, loss 0.0373114, acc 0.96875, learning_rate 0.00167776
2018-04-02T19:28:30.237697: step 683, loss 0.00453697, acc 1, learning_rate 0.00167514
2018-04-02T19:28:31.312006: step 684, loss 0.039051, acc 0.984375, learning_rate 0.00167252
2018-04-02T19:28:32.392537: step 685, loss 0.0927577, acc 0.984375, learning_rate 0.0016699
2018-04-02T19:28:33.468312: step 686, loss 0.00401433, acc 1, learning_rate 0.00166729
2018-04-02T19:28:34.538396: step 687, loss 0.00408109, acc 1, learning_rate 0.00166469
2018-04-02T19:28:35.619247: step 688, loss 0.0266895, acc 0.984375, learning_rate 0.00166209
2018-04-02T19:28:36.695111: step 689, loss 0.00654508, acc 1, learning_rate 0.00165949
2018-04-02T19:28:37.769685: step 690, loss 0.0110776, acc 1, learning_rate 0.00165689
2018-04-02T19:28:38.842170: step 691, loss 0.0279309, acc 0.984375, learning_rate 0.00165431
2018-04-02T19:28:39.923413: step 692, loss 0.00401083, acc 1, learning_rate 0.00165172
2018-04-02T19:28:41.001778: step 693, loss 0.00139825, acc 1, learning_rate 0.00164914
2018-04-02T19:28:42.073532: step 694, loss 0.00222865, acc 1, learning_rate 0.00164657
2018-04-02T19:28:43.146409: step 695, loss 0.139282, acc 0.953125, learning_rate 0.00164399
2018-04-02T19:28:44.216744: step 696, loss 0.0485892, acc 0.984375, learning_rate 0.00164143
2018-04-02T19:28:45.289453: step 697, loss 0.00366955, acc 1, learning_rate 0.00163886
2018-04-02T19:28:46.363753: step 698, loss 0.00318173, acc 1, learning_rate 0.00163631
2018-04-02T19:28:47.441489: step 699, loss 0.00530498, acc 1, learning_rate 0.00163375
2018-04-02T19:28:48.510273: step 700, loss 0.00422474, acc 1, learning_rate 0.0016312

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:28:48.699038: step 700, loss 0.0457747, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-700

2018-04-02T19:28:53.107426: step 701, loss 0.00764703, acc 1, learning_rate 0.00162866
2018-04-02T19:28:54.182924: step 702, loss 0.0323922, acc 0.984375, learning_rate 0.00162611
2018-04-02T19:28:55.258776: step 703, loss 0.00232068, acc 1, learning_rate 0.00162358
2018-04-02T19:28:56.334486: step 704, loss 0.0136909, acc 1, learning_rate 0.00162104
2018-04-02T19:28:57.413350: step 705, loss 0.00623492, acc 1, learning_rate 0.00161851
2018-04-02T19:28:58.496169: step 706, loss 0.0102273, acc 1, learning_rate 0.00161599
2018-04-02T19:28:59.565595: step 707, loss 0.0465895, acc 0.984375, learning_rate 0.00161347
2018-04-02T19:29:00.647748: step 708, loss 0.0559966, acc 0.984375, learning_rate 0.00161095
2018-04-02T19:29:01.731387: step 709, loss 0.0030468, acc 1, learning_rate 0.00160844
2018-04-02T19:29:02.812899: step 710, loss 0.00310447, acc 1, learning_rate 0.00160593
2018-04-02T19:29:03.892816: step 711, loss 0.0738385, acc 0.984375, learning_rate 0.00160343
2018-04-02T19:29:04.967877: step 712, loss 0.00309023, acc 1, learning_rate 0.00160093
2018-04-02T19:29:06.038577: step 713, loss 0.0576378, acc 0.984375, learning_rate 0.00159843
2018-04-02T19:29:07.119778: step 714, loss 0.0034072, acc 1, learning_rate 0.00159594
2018-04-02T19:29:08.195863: step 715, loss 0.00801376, acc 1, learning_rate 0.00159345
2018-04-02T19:29:09.264968: step 716, loss 0.00410606, acc 1, learning_rate 0.00159097
2018-04-02T19:29:10.342067: step 717, loss 0.0178917, acc 1, learning_rate 0.00158849
2018-04-02T19:29:11.422450: step 718, loss 0.00588068, acc 1, learning_rate 0.00158602
2018-04-02T19:29:12.504489: step 719, loss 0.00358364, acc 1, learning_rate 0.00158355
2018-04-02T19:29:13.582723: step 720, loss 0.0141992, acc 1, learning_rate 0.00158108
2018-04-02T19:29:14.663921: step 721, loss 0.00530642, acc 1, learning_rate 0.00157862
2018-04-02T19:29:15.736055: step 722, loss 0.00750391, acc 1, learning_rate 0.00157616
2018-04-02T19:29:16.775100: step 723, loss 0.00723121, acc 1, learning_rate 0.0015737
2018-04-02T19:29:17.849227: step 724, loss 0.00471138, acc 1, learning_rate 0.00157125
2018-04-02T19:29:18.922984: step 725, loss 0.00611518, acc 1, learning_rate 0.00156881
2018-04-02T19:29:20.001757: step 726, loss 0.00581888, acc 1, learning_rate 0.00156637
2018-04-02T19:29:21.076175: step 727, loss 0.0122375, acc 1, learning_rate 0.00156393
2018-04-02T19:29:22.156029: step 728, loss 0.0113438, acc 1, learning_rate 0.00156149
2018-04-02T19:29:23.235267: step 729, loss 0.0032305, acc 1, learning_rate 0.00155906
2018-04-02T19:29:24.305527: step 730, loss 0.0027252, acc 1, learning_rate 0.00155664
2018-04-02T19:29:25.383634: step 731, loss 0.00295419, acc 1, learning_rate 0.00155422
2018-04-02T19:29:26.455045: step 732, loss 0.00486377, acc 1, learning_rate 0.0015518
2018-04-02T19:29:27.528083: step 733, loss 0.0196109, acc 1, learning_rate 0.00154938
2018-04-02T19:29:28.608486: step 734, loss 0.00785276, acc 1, learning_rate 0.00154697
2018-04-02T19:29:29.679687: step 735, loss 0.00730965, acc 1, learning_rate 0.00154457
2018-04-02T19:29:30.764638: step 736, loss 0.00689226, acc 1, learning_rate 0.00154217
2018-04-02T19:29:31.837421: step 737, loss 0.00428159, acc 1, learning_rate 0.00153977
2018-04-02T19:29:32.912023: step 738, loss 0.016205, acc 0.984375, learning_rate 0.00153737
2018-04-02T19:29:33.993925: step 739, loss 0.00830341, acc 1, learning_rate 0.00153498
2018-04-02T19:29:35.065591: step 740, loss 0.00532932, acc 1, learning_rate 0.0015326
2018-04-02T19:29:36.140768: step 741, loss 0.0116329, acc 1, learning_rate 0.00153022
2018-04-02T19:29:37.213820: step 742, loss 0.00788051, acc 1, learning_rate 0.00152784
2018-04-02T19:29:38.294779: step 743, loss 0.00787948, acc 1, learning_rate 0.00152546
2018-04-02T19:29:39.375958: step 744, loss 0.00456243, acc 1, learning_rate 0.00152309
2018-04-02T19:29:40.447546: step 745, loss 0.055641, acc 0.984375, learning_rate 0.00152073
2018-04-02T19:29:41.526262: step 746, loss 0.00486072, acc 1, learning_rate 0.00151837
2018-04-02T19:29:42.602307: step 747, loss 0.000787946, acc 1, learning_rate 0.00151601
2018-04-02T19:29:43.675339: step 748, loss 0.0105698, acc 1, learning_rate 0.00151365
2018-04-02T19:29:44.754654: step 749, loss 0.00248224, acc 1, learning_rate 0.0015113
2018-04-02T19:29:45.824305: step 750, loss 0.00405345, acc 1, learning_rate 0.00150896
2018-04-02T19:29:46.903903: step 751, loss 0.0144514, acc 0.984375, learning_rate 0.00150661
2018-04-02T19:29:47.981649: step 752, loss 0.00252687, acc 1, learning_rate 0.00150428
2018-04-02T19:29:49.062375: step 753, loss 0.0033177, acc 1, learning_rate 0.00150194
2018-04-02T19:29:50.139015: step 754, loss 0.00587614, acc 1, learning_rate 0.00149961
2018-04-02T19:29:51.218029: step 755, loss 0.00389847, acc 1, learning_rate 0.00149728
2018-04-02T19:29:52.292515: step 756, loss 0.00735215, acc 1, learning_rate 0.00149496
2018-04-02T19:29:53.366262: step 757, loss 0.00684765, acc 1, learning_rate 0.00149264
2018-04-02T19:29:54.449337: step 758, loss 0.00212729, acc 1, learning_rate 0.00149032
2018-04-02T19:29:55.532994: step 759, loss 0.00301979, acc 1, learning_rate 0.00148801
2018-04-02T19:29:56.615608: step 760, loss 0.1303, acc 0.953125, learning_rate 0.0014857
2018-04-02T19:29:57.696428: step 761, loss 0.003464, acc 1, learning_rate 0.0014834
2018-04-02T19:29:58.766987: step 762, loss 0.00198065, acc 1, learning_rate 0.0014811
2018-04-02T19:29:59.846541: step 763, loss 0.00985489, acc 1, learning_rate 0.0014788
2018-04-02T19:30:00.923404: step 764, loss 0.00478569, acc 1, learning_rate 0.00147651
2018-04-02T19:30:01.996094: step 765, loss 0.0188693, acc 0.984375, learning_rate 0.00147422
2018-04-02T19:30:03.069502: step 766, loss 0.00543801, acc 1, learning_rate 0.00147194
2018-04-02T19:30:04.143612: step 767, loss 0.00143584, acc 1, learning_rate 0.00146966
2018-04-02T19:30:05.218288: step 768, loss 0.00739184, acc 1, learning_rate 0.00146738
2018-04-02T19:30:06.302023: step 769, loss 0.00381789, acc 1, learning_rate 0.00146511
2018-04-02T19:30:07.378306: step 770, loss 0.00206945, acc 1, learning_rate 0.00146284
2018-04-02T19:30:08.453695: step 771, loss 0.00307585, acc 1, learning_rate 0.00146057
2018-04-02T19:30:09.532860: step 772, loss 0.00425679, acc 1, learning_rate 0.00145831
2018-04-02T19:30:10.612531: step 773, loss 0.00695657, acc 1, learning_rate 0.00145605
2018-04-02T19:30:11.686938: step 774, loss 0.0018439, acc 1, learning_rate 0.00145379
2018-04-02T19:30:12.756444: step 775, loss 0.00216261, acc 1, learning_rate 0.00145154
2018-04-02T19:30:13.837017: step 776, loss 0.00424872, acc 1, learning_rate 0.0014493
2018-04-02T19:30:14.920086: step 777, loss 0.00737538, acc 1, learning_rate 0.00144705
2018-04-02T19:30:15.992184: step 778, loss 0.00540571, acc 1, learning_rate 0.00144481
2018-04-02T19:30:17.066400: step 779, loss 0.00416389, acc 1, learning_rate 0.00144258
2018-04-02T19:30:18.148496: step 780, loss 0.00503989, acc 1, learning_rate 0.00144035
2018-04-02T19:30:19.219727: step 781, loss 0.00572326, acc 1, learning_rate 0.00143812
2018-04-02T19:30:20.293521: step 782, loss 0.00169297, acc 1, learning_rate 0.00143589
2018-04-02T19:30:21.374283: step 783, loss 0.00535778, acc 1, learning_rate 0.00143367
2018-04-02T19:30:22.450758: step 784, loss 0.00148796, acc 1, learning_rate 0.00143145
2018-04-02T19:30:23.529935: step 785, loss 0.00171903, acc 1, learning_rate 0.00142924
2018-04-02T19:30:24.599583: step 786, loss 0.00434502, acc 1, learning_rate 0.00142703
2018-04-02T19:30:25.672215: step 787, loss 0.00080641, acc 1, learning_rate 0.00142482
2018-04-02T19:30:26.743489: step 788, loss 0.00107965, acc 1, learning_rate 0.00142262
2018-04-02T19:30:27.817575: step 789, loss 0.0231974, acc 0.984375, learning_rate 0.00142042
2018-04-02T19:30:28.891248: step 790, loss 0.0120796, acc 1, learning_rate 0.00141823
2018-04-02T19:30:29.962052: step 791, loss 0.00533105, acc 1, learning_rate 0.00141603
2018-04-02T19:30:31.045435: step 792, loss 0.00480828, acc 1, learning_rate 0.00141385
2018-04-02T19:30:32.116407: step 793, loss 0.101025, acc 0.984375, learning_rate 0.00141166
2018-04-02T19:30:33.199883: step 794, loss 0.00394368, acc 1, learning_rate 0.00140948
2018-04-02T19:30:34.278949: step 795, loss 0.00447828, acc 1, learning_rate 0.0014073
2018-04-02T19:30:35.350702: step 796, loss 0.0146386, acc 1, learning_rate 0.00140513
2018-04-02T19:30:36.419156: step 797, loss 0.08136, acc 0.984375, learning_rate 0.00140296
2018-04-02T19:30:37.500475: step 798, loss 0.00217154, acc 1, learning_rate 0.00140079
2018-04-02T19:30:38.581295: step 799, loss 0.0022488, acc 1, learning_rate 0.00139863
2018-04-02T19:30:39.660923: step 800, loss 0.00367281, acc 1, learning_rate 0.00139647

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:30:39.850221: step 800, loss 0.0434218, acc 0.978987

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-800

2018-04-02T19:30:43.685765: step 801, loss 0.00379936, acc 1, learning_rate 0.00139432
2018-04-02T19:30:44.763300: step 802, loss 0.00356804, acc 1, learning_rate 0.00139216
2018-04-02T19:30:45.846192: step 803, loss 0.00271768, acc 1, learning_rate 0.00139001
2018-04-02T19:30:46.926789: step 804, loss 0.0124418, acc 1, learning_rate 0.00138787
2018-04-02T19:30:47.999258: step 805, loss 0.00278333, acc 1, learning_rate 0.00138573
2018-04-02T19:30:49.075632: step 806, loss 0.0412521, acc 0.984375, learning_rate 0.00138359
2018-04-02T19:30:50.149245: step 807, loss 0.00239019, acc 1, learning_rate 0.00138146
2018-04-02T19:30:51.230332: step 808, loss 0.00289508, acc 1, learning_rate 0.00137933
2018-04-02T19:30:52.307209: step 809, loss 0.00738833, acc 1, learning_rate 0.0013772
2018-04-02T19:30:53.378458: step 810, loss 0.00328917, acc 1, learning_rate 0.00137508
2018-04-02T19:30:54.462710: step 811, loss 0.00369977, acc 1, learning_rate 0.00137296
2018-04-02T19:30:55.542609: step 812, loss 0.00254362, acc 1, learning_rate 0.00137084
2018-04-02T19:30:56.625093: step 813, loss 0.00300958, acc 1, learning_rate 0.00136873
2018-04-02T19:30:57.707446: step 814, loss 0.0109631, acc 1, learning_rate 0.00136662
2018-04-02T19:30:58.780271: step 815, loss 0.0061079, acc 1, learning_rate 0.00136451
2018-04-02T19:30:59.853940: step 816, loss 0.00220547, acc 1, learning_rate 0.00136241
2018-04-02T19:31:00.935280: step 817, loss 0.0176059, acc 0.984375, learning_rate 0.00136031
2018-04-02T19:31:02.018818: step 818, loss 0.00342423, acc 1, learning_rate 0.00135821
2018-04-02T19:31:03.101272: step 819, loss 0.00311961, acc 1, learning_rate 0.00135612
2018-04-02T19:31:04.183653: step 820, loss 0.108049, acc 0.96875, learning_rate 0.00135403
2018-04-02T19:31:05.263382: step 821, loss 0.00163221, acc 1, learning_rate 0.00135195
2018-04-02T19:31:06.342504: step 822, loss 0.00202975, acc 1, learning_rate 0.00134987
2018-04-02T19:31:07.417280: step 823, loss 0.074687, acc 0.984375, learning_rate 0.00134779
2018-04-02T19:31:08.501161: step 824, loss 0.00353724, acc 1, learning_rate 0.00134571
2018-04-02T19:31:09.576846: step 825, loss 0.0706512, acc 0.984375, learning_rate 0.00134364
2018-04-02T19:31:10.658135: step 826, loss 0.00352569, acc 1, learning_rate 0.00134157
2018-04-02T19:31:11.727827: step 827, loss 0.00672643, acc 1, learning_rate 0.00133951
2018-04-02T19:31:12.801952: step 828, loss 0.00183187, acc 1, learning_rate 0.00133745
2018-04-02T19:31:13.883740: step 829, loss 0.00113222, acc 1, learning_rate 0.00133539
2018-04-02T19:31:14.952328: step 830, loss 0.00318979, acc 1, learning_rate 0.00133334
2018-04-02T19:31:16.025982: step 831, loss 0.0040099, acc 1, learning_rate 0.00133129
2018-04-02T19:31:17.102012: step 832, loss 0.00481178, acc 1, learning_rate 0.00132924
2018-04-02T19:31:18.182198: step 833, loss 0.00846563, acc 1, learning_rate 0.0013272
2018-04-02T19:31:19.253298: step 834, loss 0.00591336, acc 1, learning_rate 0.00132516
2018-04-02T19:31:20.331316: step 835, loss 0.0142508, acc 0.984375, learning_rate 0.00132312
2018-04-02T19:31:21.403129: step 836, loss 0.00304724, acc 1, learning_rate 0.00132108
2018-04-02T19:31:22.475085: step 837, loss 0.00671913, acc 1, learning_rate 0.00131905
2018-04-02T19:31:23.553868: step 838, loss 0.00692252, acc 1, learning_rate 0.00131703
2018-04-02T19:31:24.635367: step 839, loss 0.00605024, acc 1, learning_rate 0.001315
2018-04-02T19:31:25.719269: step 840, loss 0.00424993, acc 1, learning_rate 0.00131298
2018-04-02T19:31:26.797016: step 841, loss 0.0045479, acc 1, learning_rate 0.00131097
2018-04-02T19:31:27.878200: step 842, loss 0.00203938, acc 1, learning_rate 0.00130895
2018-04-02T19:31:28.949969: step 843, loss 0.0149083, acc 0.984375, learning_rate 0.00130694
2018-04-02T19:31:30.030950: step 844, loss 0.0110876, acc 1, learning_rate 0.00130494
2018-04-02T19:31:31.110054: step 845, loss 0.0010281, acc 1, learning_rate 0.00130293
2018-04-02T19:31:32.178760: step 846, loss 0.0114277, acc 1, learning_rate 0.00130093
2018-04-02T19:31:33.246202: step 847, loss 0.00315486, acc 1, learning_rate 0.00129894
2018-04-02T19:31:34.317924: step 848, loss 0.113893, acc 0.96875, learning_rate 0.00129694
2018-04-02T19:31:35.398739: step 849, loss 0.0113893, acc 1, learning_rate 0.00129495
2018-04-02T19:31:36.468368: step 850, loss 0.0187848, acc 1, learning_rate 0.00129297
2018-04-02T19:31:37.550942: step 851, loss 0.0192012, acc 1, learning_rate 0.00129098
2018-04-02T19:31:38.620708: step 852, loss 0.00461791, acc 1, learning_rate 0.001289
2018-04-02T19:31:39.695753: step 853, loss 0.0046137, acc 1, learning_rate 0.00128703
2018-04-02T19:31:40.769914: step 854, loss 0.00707142, acc 1, learning_rate 0.00128505
2018-04-02T19:31:41.842538: step 855, loss 0.00723464, acc 1, learning_rate 0.00128308
2018-04-02T19:31:42.925983: step 856, loss 0.0139981, acc 1, learning_rate 0.00128111
2018-04-02T19:31:43.997921: step 857, loss 0.00338196, acc 1, learning_rate 0.00127915
2018-04-02T19:31:45.076095: step 858, loss 0.00306924, acc 1, learning_rate 0.00127719
2018-04-02T19:31:46.158904: step 859, loss 0.0332654, acc 0.96875, learning_rate 0.00127523
2018-04-02T19:31:47.231193: step 860, loss 0.0148661, acc 1, learning_rate 0.00127328
2018-04-02T19:31:48.304320: step 861, loss 0.00505112, acc 1, learning_rate 0.00127133
2018-04-02T19:31:49.386209: step 862, loss 0.00194571, acc 1, learning_rate 0.00126938
2018-04-02T19:31:50.461565: step 863, loss 0.00885639, acc 1, learning_rate 0.00126744
2018-04-02T19:31:51.534596: step 864, loss 0.00520437, acc 1, learning_rate 0.00126549
2018-04-02T19:31:52.607427: step 865, loss 0.00425438, acc 1, learning_rate 0.00126356
2018-04-02T19:31:53.681105: step 866, loss 0.00303832, acc 1, learning_rate 0.00126162
2018-04-02T19:31:54.763120: step 867, loss 0.00739621, acc 1, learning_rate 0.00125969
2018-04-02T19:31:55.843899: step 868, loss 0.00424401, acc 1, learning_rate 0.00125776
2018-04-02T19:31:56.913574: step 869, loss 0.00156196, acc 1, learning_rate 0.00125584
2018-04-02T19:31:57.994281: step 870, loss 0.0159574, acc 1, learning_rate 0.00125392
2018-04-02T19:31:59.074508: step 871, loss 0.00373237, acc 1, learning_rate 0.001252
2018-04-02T19:32:00.153572: step 872, loss 0.00373013, acc 1, learning_rate 0.00125008
2018-04-02T19:32:01.226395: step 873, loss 0.00126462, acc 1, learning_rate 0.00124817
2018-04-02T19:32:02.301401: step 874, loss 0.00296869, acc 1, learning_rate 0.00124626
2018-04-02T19:32:03.384436: step 875, loss 0.00229264, acc 1, learning_rate 0.00124435
2018-04-02T19:32:04.454340: step 876, loss 0.00728917, acc 1, learning_rate 0.00124245
2018-04-02T19:32:05.529322: step 877, loss 0.00258668, acc 1, learning_rate 0.00124055
2018-04-02T19:32:06.608812: step 878, loss 0.00264214, acc 1, learning_rate 0.00123866
2018-04-02T19:32:07.679310: step 879, loss 0.00185882, acc 1, learning_rate 0.00123676
2018-04-02T19:32:08.750271: step 880, loss 0.00691116, acc 1, learning_rate 0.00123487
2018-04-02T19:32:09.820764: step 881, loss 0.00382005, acc 1, learning_rate 0.00123299
2018-04-02T19:32:10.899037: step 882, loss 0.00309866, acc 1, learning_rate 0.0012311
2018-04-02T19:32:11.971771: step 883, loss 0.00784717, acc 1, learning_rate 0.00122922
2018-04-02T19:32:13.041732: step 884, loss 0.00269829, acc 1, learning_rate 0.00122734
2018-04-02T19:32:14.116039: step 885, loss 0.00150421, acc 1, learning_rate 0.00122547
2018-04-02T19:32:15.186689: step 886, loss 0.00197124, acc 1, learning_rate 0.0012236
2018-04-02T19:32:16.256685: step 887, loss 0.00556508, acc 1, learning_rate 0.00122173
2018-04-02T19:32:17.327400: step 888, loss 0.0174577, acc 0.984375, learning_rate 0.00121987
2018-04-02T19:32:18.399982: step 889, loss 0.00611866, acc 1, learning_rate 0.001218
2018-04-02T19:32:19.480001: step 890, loss 0.00865447, acc 1, learning_rate 0.00121614
2018-04-02T19:32:20.552622: step 891, loss 0.00228236, acc 1, learning_rate 0.00121429
2018-04-02T19:32:21.634590: step 892, loss 0.00577591, acc 1, learning_rate 0.00121244
2018-04-02T19:32:22.705185: step 893, loss 0.00231057, acc 1, learning_rate 0.00121059
2018-04-02T19:32:23.776512: step 894, loss 0.00212671, acc 1, learning_rate 0.00120874
2018-04-02T19:32:24.854065: step 895, loss 0.00738442, acc 1, learning_rate 0.0012069
2018-04-02T19:32:25.924042: step 896, loss 0.00409453, acc 1, learning_rate 0.00120506
2018-04-02T19:32:27.005121: step 897, loss 0.00234472, acc 1, learning_rate 0.00120322
2018-04-02T19:32:28.078559: step 898, loss 0.014334, acc 1, learning_rate 0.00120138
2018-04-02T19:32:29.160687: step 899, loss 0.00115337, acc 1, learning_rate 0.00119955
2018-04-02T19:32:30.228230: step 900, loss 0.00777158, acc 1, learning_rate 0.00119772

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:32:30.418247: step 900, loss 0.0439614, acc 0.978986

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-900

2018-04-02T19:32:34.489964: step 901, loss 0.0190209, acc 0.984375, learning_rate 0.0011959
2018-04-02T19:32:35.565034: step 902, loss 0.00380337, acc 1, learning_rate 0.00119408
2018-04-02T19:32:36.648178: step 903, loss 0.00266757, acc 1, learning_rate 0.00119226
2018-04-02T19:32:37.719439: step 904, loss 0.00139068, acc 1, learning_rate 0.00119044
2018-04-02T19:32:38.792284: step 905, loss 0.00326356, acc 1, learning_rate 0.00118863
2018-04-02T19:32:39.870967: step 906, loss 0.000920219, acc 1, learning_rate 0.00118682
2018-04-02T19:32:40.955727: step 907, loss 0.00397404, acc 1, learning_rate 0.00118501
2018-04-02T19:32:42.035426: step 908, loss 0.00286352, acc 1, learning_rate 0.00118321
2018-04-02T19:32:43.106427: step 909, loss 0.00182691, acc 1, learning_rate 0.00118141
2018-04-02T19:32:44.177311: step 910, loss 0.00538275, acc 1, learning_rate 0.00117961
2018-04-02T19:32:45.245743: step 911, loss 0.010864, acc 1, learning_rate 0.00117781
2018-04-02T19:32:46.314129: step 912, loss 0.00224353, acc 1, learning_rate 0.00117602
2018-04-02T19:32:47.386249: step 913, loss 0.00219724, acc 1, learning_rate 0.00117423
2018-04-02T19:32:48.466522: step 914, loss 0.00367681, acc 1, learning_rate 0.00117245
2018-04-02T19:32:49.539529: step 915, loss 0.00173024, acc 1, learning_rate 0.00117066
2018-04-02T19:32:50.613304: step 916, loss 0.00187329, acc 1, learning_rate 0.00116888
2018-04-02T19:32:51.692623: step 917, loss 0.00321891, acc 1, learning_rate 0.00116711
2018-04-02T19:32:52.762262: step 918, loss 0.00664991, acc 1, learning_rate 0.00116533
2018-04-02T19:32:53.846110: step 919, loss 0.068143, acc 0.984375, learning_rate 0.00116356
2018-04-02T19:32:54.914819: step 920, loss 0.0255809, acc 0.984375, learning_rate 0.00116179
2018-04-02T19:32:55.986711: step 921, loss 0.00914905, acc 1, learning_rate 0.00116003
2018-04-02T19:32:57.069540: step 922, loss 0.00374661, acc 1, learning_rate 0.00115826
2018-04-02T19:32:58.141708: step 923, loss 0.0117449, acc 1, learning_rate 0.0011565
2018-04-02T19:32:59.220039: step 924, loss 0.00932031, acc 1, learning_rate 0.00115475
2018-04-02T19:33:00.297943: step 925, loss 0.0677844, acc 0.984375, learning_rate 0.00115299
2018-04-02T19:33:01.379122: step 926, loss 0.0135489, acc 0.984375, learning_rate 0.00115124
2018-04-02T19:33:02.455573: step 927, loss 0.00375669, acc 1, learning_rate 0.0011495
2018-04-02T19:33:03.527639: step 928, loss 0.0047868, acc 1, learning_rate 0.00114775
2018-04-02T19:33:04.605138: step 929, loss 0.00417431, acc 1, learning_rate 0.00114601
2018-04-02T19:33:05.684447: step 930, loss 0.0590942, acc 0.984375, learning_rate 0.00114427
2018-04-02T19:33:06.755149: step 931, loss 0.00235313, acc 1, learning_rate 0.00114253
2018-04-02T19:33:07.838313: step 932, loss 0.0368346, acc 0.96875, learning_rate 0.0011408
2018-04-02T19:33:08.918464: step 933, loss 0.00191873, acc 1, learning_rate 0.00113907
2018-04-02T19:33:09.991144: step 934, loss 0.0253515, acc 0.984375, learning_rate 0.00113734
2018-04-02T19:33:11.067795: step 935, loss 0.00263918, acc 1, learning_rate 0.00113562
2018-04-02T19:33:12.138050: step 936, loss 0.00235067, acc 1, learning_rate 0.00113389
2018-04-02T19:33:13.208701: step 937, loss 0.00202964, acc 1, learning_rate 0.00113218
2018-04-02T19:33:14.284592: step 938, loss 0.00466379, acc 1, learning_rate 0.00113046
2018-04-02T19:33:15.356955: step 939, loss 0.00140201, acc 1, learning_rate 0.00112875
2018-04-02T19:33:16.434618: step 940, loss 0.00610261, acc 1, learning_rate 0.00112704
2018-04-02T19:33:17.516976: step 941, loss 0.00351493, acc 1, learning_rate 0.00112533
2018-04-02T19:33:18.591325: step 942, loss 0.0129346, acc 1, learning_rate 0.00112362
2018-04-02T19:33:19.667207: step 943, loss 0.0749409, acc 0.984375, learning_rate 0.00112192
2018-04-02T19:33:20.738215: step 944, loss 0.00482332, acc 1, learning_rate 0.00112022
2018-04-02T19:33:21.812938: step 945, loss 0.00349513, acc 1, learning_rate 0.00111853
2018-04-02T19:33:22.881418: step 946, loss 0.00655688, acc 1, learning_rate 0.00111683
2018-04-02T19:33:23.958993: step 947, loss 0.0814499, acc 0.984375, learning_rate 0.00111514
2018-04-02T19:33:25.028740: step 948, loss 0.0737191, acc 0.984375, learning_rate 0.00111345
2018-04-02T19:33:26.108666: step 949, loss 0.000744591, acc 1, learning_rate 0.00111177
2018-04-02T19:33:27.183228: step 950, loss 0.00447035, acc 1, learning_rate 0.00111009
2018-04-02T19:33:28.252718: step 951, loss 0.0540719, acc 0.984375, learning_rate 0.00110841
2018-04-02T19:33:29.323338: step 952, loss 0.0209309, acc 0.984375, learning_rate 0.00110673
2018-04-02T19:33:30.400521: step 953, loss 0.00356976, acc 1, learning_rate 0.00110506
2018-04-02T19:33:31.485208: step 954, loss 0.00516898, acc 1, learning_rate 0.00110339
2018-04-02T19:33:32.558795: step 955, loss 0.00377171, acc 1, learning_rate 0.00110172
2018-04-02T19:33:33.633210: step 956, loss 0.00258631, acc 1, learning_rate 0.00110005
2018-04-02T19:33:34.706771: step 957, loss 0.0686784, acc 0.984375, learning_rate 0.00109839
2018-04-02T19:33:35.791098: step 958, loss 0.00406426, acc 1, learning_rate 0.00109673
2018-04-02T19:33:36.862651: step 959, loss 0.000842662, acc 1, learning_rate 0.00109507
2018-04-02T19:33:37.942590: step 960, loss 0.0914292, acc 0.96875, learning_rate 0.00109342
2018-04-02T19:33:39.023373: step 961, loss 0.00531469, acc 1, learning_rate 0.00109177
2018-04-02T19:33:40.094931: step 962, loss 0.0111969, acc 1, learning_rate 0.00109012
2018-04-02T19:33:41.167988: step 963, loss 0.00831343, acc 1, learning_rate 0.00108847
2018-04-02T19:33:42.192949: step 964, loss 0.00173324, acc 1, learning_rate 0.00108683
2018-04-02T19:33:43.273019: step 965, loss 0.0407775, acc 0.984375, learning_rate 0.00108519
2018-04-02T19:33:44.354412: step 966, loss 0.000698839, acc 1, learning_rate 0.00108355
2018-04-02T19:33:45.436888: step 967, loss 0.00693562, acc 1, learning_rate 0.00108191
2018-04-02T19:33:46.516898: step 968, loss 0.0148423, acc 1, learning_rate 0.00108028
2018-04-02T19:33:47.590358: step 969, loss 0.00661061, acc 1, learning_rate 0.00107865
2018-04-02T19:33:48.665598: step 970, loss 0.0330603, acc 0.984375, learning_rate 0.00107702
2018-04-02T19:33:49.734549: step 971, loss 0.0104216, acc 1, learning_rate 0.0010754
2018-04-02T19:33:50.813378: step 972, loss 0.00211902, acc 1, learning_rate 0.00107378
2018-04-02T19:33:51.887463: step 973, loss 0.00495683, acc 1, learning_rate 0.00107216
2018-04-02T19:33:52.964815: step 974, loss 0.0110663, acc 1, learning_rate 0.00107054
2018-04-02T19:33:54.043233: step 975, loss 0.00417772, acc 1, learning_rate 0.00106893
2018-04-02T19:33:55.111923: step 976, loss 0.00887645, acc 1, learning_rate 0.00106732
2018-04-02T19:33:56.181373: step 977, loss 0.00330588, acc 1, learning_rate 0.00106571
2018-04-02T19:33:57.257234: step 978, loss 0.0323052, acc 0.984375, learning_rate 0.0010641
2018-04-02T19:33:58.331948: step 979, loss 0.0569093, acc 0.984375, learning_rate 0.0010625
2018-04-02T19:33:59.416339: step 980, loss 0.00808722, acc 1, learning_rate 0.0010609
2018-04-02T19:34:00.497270: step 981, loss 0.00539586, acc 1, learning_rate 0.0010593
2018-04-02T19:34:01.569551: step 982, loss 0.00790987, acc 1, learning_rate 0.00105771
2018-04-02T19:34:02.651146: step 983, loss 0.00589984, acc 1, learning_rate 0.00105611
2018-04-02T19:34:03.732259: step 984, loss 0.00338095, acc 1, learning_rate 0.00105452
2018-04-02T19:34:04.801792: step 985, loss 0.00179276, acc 1, learning_rate 0.00105294
2018-04-02T19:34:05.882313: step 986, loss 0.00596182, acc 1, learning_rate 0.00105135
2018-04-02T19:34:06.958372: step 987, loss 0.00255588, acc 1, learning_rate 0.00104977
2018-04-02T19:34:08.029387: step 988, loss 0.00529259, acc 1, learning_rate 0.00104819
2018-04-02T19:34:09.100893: step 989, loss 0.00557198, acc 1, learning_rate 0.00104662
2018-04-02T19:34:10.168914: step 990, loss 0.00419514, acc 1, learning_rate 0.00104504
2018-04-02T19:34:11.239258: step 991, loss 0.00443808, acc 1, learning_rate 0.00104347
2018-04-02T19:34:12.311313: step 992, loss 0.00366014, acc 1, learning_rate 0.0010419
2018-04-02T19:34:13.385071: step 993, loss 0.00527024, acc 1, learning_rate 0.00104034
2018-04-02T19:34:14.454581: step 994, loss 0.00285423, acc 1, learning_rate 0.00103877
2018-04-02T19:34:15.524585: step 995, loss 0.00572475, acc 1, learning_rate 0.00103721
2018-04-02T19:34:16.592034: step 996, loss 0.00318651, acc 1, learning_rate 0.00103565
2018-04-02T19:34:17.661135: step 997, loss 0.00790644, acc 1, learning_rate 0.0010341
2018-04-02T19:34:18.763547: step 998, loss 0.00413237, acc 1, learning_rate 0.00103254
2018-04-02T19:34:19.833342: step 999, loss 0.0035405, acc 1, learning_rate 0.00103099
2018-04-02T19:34:20.902666: step 1000, loss 0.00251582, acc 1, learning_rate 0.00102945

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:34:21.090990: step 1000, loss 0.0445254, acc 0.978986

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1000

2018-04-02T19:34:24.795895: step 1001, loss 0.00201092, acc 1, learning_rate 0.0010279
2018-04-02T19:34:25.866499: step 1002, loss 0.00401763, acc 1, learning_rate 0.00102636
2018-04-02T19:34:26.941134: step 1003, loss 0.00114741, acc 1, learning_rate 0.00102482
2018-04-02T19:34:28.013962: step 1004, loss 0.00624596, acc 1, learning_rate 0.00102328
2018-04-02T19:34:29.087183: step 1005, loss 0.00151411, acc 1, learning_rate 0.00102174
2018-04-02T19:34:30.157549: step 1006, loss 0.00624876, acc 1, learning_rate 0.00102021
2018-04-02T19:34:31.227257: step 1007, loss 0.00414148, acc 1, learning_rate 0.00101868
2018-04-02T19:34:32.300000: step 1008, loss 0.00598316, acc 1, learning_rate 0.00101715
2018-04-02T19:34:33.368711: step 1009, loss 0.0047943, acc 1, learning_rate 0.00101563
2018-04-02T19:34:34.440811: step 1010, loss 0.00194764, acc 1, learning_rate 0.00101411
2018-04-02T19:34:35.510621: step 1011, loss 0.00566377, acc 1, learning_rate 0.00101259
2018-04-02T19:34:36.584776: step 1012, loss 0.00471171, acc 1, learning_rate 0.00101107
2018-04-02T19:34:37.658008: step 1013, loss 0.00242013, acc 1, learning_rate 0.00100955
2018-04-02T19:34:38.730169: step 1014, loss 0.0143426, acc 1, learning_rate 0.00100804
2018-04-02T19:34:39.800825: step 1015, loss 0.00327464, acc 1, learning_rate 0.00100653
2018-04-02T19:34:40.871130: step 1016, loss 0.00306463, acc 1, learning_rate 0.00100503
2018-04-02T19:34:41.945156: step 1017, loss 0.00436072, acc 1, learning_rate 0.00100352
2018-04-02T19:34:43.016329: step 1018, loss 0.0030488, acc 1, learning_rate 0.00100202
2018-04-02T19:34:44.087953: step 1019, loss 0.00366589, acc 1, learning_rate 0.00100052
2018-04-02T19:34:45.161037: step 1020, loss 0.00268718, acc 1, learning_rate 0.000999021
2018-04-02T19:34:46.233371: step 1021, loss 0.00623885, acc 1, learning_rate 0.000997526
2018-04-02T19:34:47.306812: step 1022, loss 0.00410954, acc 1, learning_rate 0.000996034
2018-04-02T19:34:48.380027: step 1023, loss 0.00345727, acc 1, learning_rate 0.000994544
2018-04-02T19:34:49.455002: step 1024, loss 0.00254288, acc 1, learning_rate 0.000993057
2018-04-02T19:34:50.526265: step 1025, loss 0.00164673, acc 1, learning_rate 0.000991572
2018-04-02T19:34:51.595510: step 1026, loss 0.00350482, acc 1, learning_rate 0.00099009
2018-04-02T19:34:52.673631: step 1027, loss 0.00126702, acc 1, learning_rate 0.00098861
2018-04-02T19:34:53.752193: step 1028, loss 0.00429129, acc 1, learning_rate 0.000987132
2018-04-02T19:34:54.826152: step 1029, loss 0.0344175, acc 0.984375, learning_rate 0.000985657
2018-04-02T19:34:55.899781: step 1030, loss 0.00656945, acc 1, learning_rate 0.000984185
2018-04-02T19:34:56.970774: step 1031, loss 0.00422516, acc 1, learning_rate 0.000982715
2018-04-02T19:34:58.044848: step 1032, loss 0.0243475, acc 0.984375, learning_rate 0.000981247
2018-04-02T19:34:59.118071: step 1033, loss 0.00419449, acc 1, learning_rate 0.000979782
2018-04-02T19:35:00.194168: step 1034, loss 0.00227832, acc 1, learning_rate 0.000978319
2018-04-02T19:35:01.264987: step 1035, loss 0.00115322, acc 1, learning_rate 0.000976859
2018-04-02T19:35:02.338064: step 1036, loss 0.00183576, acc 1, learning_rate 0.000975401
2018-04-02T19:35:03.407753: step 1037, loss 0.0458839, acc 0.984375, learning_rate 0.000973945
2018-04-02T19:35:04.483565: step 1038, loss 0.00247959, acc 1, learning_rate 0.000972492
2018-04-02T19:35:05.548900: step 1039, loss 0.00401647, acc 1, learning_rate 0.000971041
2018-04-02T19:35:06.617551: step 1040, loss 0.00345509, acc 1, learning_rate 0.000969593
2018-04-02T19:35:07.689605: step 1041, loss 0.000963731, acc 1, learning_rate 0.000968147
2018-04-02T19:35:08.760804: step 1042, loss 0.00260241, acc 1, learning_rate 0.000966704
2018-04-02T19:35:09.829883: step 1043, loss 0.0242086, acc 0.984375, learning_rate 0.000965263
2018-04-02T19:35:10.901221: step 1044, loss 0.00200141, acc 1, learning_rate 0.000963824
2018-04-02T19:35:11.970605: step 1045, loss 0.0038199, acc 1, learning_rate 0.000962388
2018-04-02T19:35:13.038491: step 1046, loss 0.00387525, acc 1, learning_rate 0.000960954
2018-04-02T19:35:14.109402: step 1047, loss 0.00376755, acc 1, learning_rate 0.000959522
2018-04-02T19:35:15.182132: step 1048, loss 0.000789495, acc 1, learning_rate 0.000958093
2018-04-02T19:35:16.247500: step 1049, loss 0.00422343, acc 1, learning_rate 0.000956667
2018-04-02T19:35:17.320224: step 1050, loss 0.00539747, acc 1, learning_rate 0.000955242
2018-04-02T19:35:18.390834: step 1051, loss 0.0028359, acc 1, learning_rate 0.00095382
2018-04-02T19:35:19.462297: step 1052, loss 0.0041173, acc 1, learning_rate 0.000952401
2018-04-02T19:35:20.533925: step 1053, loss 0.00192044, acc 1, learning_rate 0.000950983
2018-04-02T19:35:21.602403: step 1054, loss 0.00274604, acc 1, learning_rate 0.000949568
2018-04-02T19:35:22.676138: step 1055, loss 0.0388224, acc 0.984375, learning_rate 0.000948156
2018-04-02T19:35:23.747199: step 1056, loss 0.00441791, acc 1, learning_rate 0.000946746
2018-04-02T19:35:24.815167: step 1057, loss 0.00362275, acc 1, learning_rate 0.000945338
2018-04-02T19:35:25.886927: step 1058, loss 0.00646801, acc 1, learning_rate 0.000943932
2018-04-02T19:35:26.957267: step 1059, loss 0.00214243, acc 1, learning_rate 0.000942529
2018-04-02T19:35:28.026734: step 1060, loss 0.000914154, acc 1, learning_rate 0.000941128
2018-04-02T19:35:29.097368: step 1061, loss 0.00294108, acc 1, learning_rate 0.00093973
2018-04-02T19:35:30.168910: step 1062, loss 0.0025274, acc 1, learning_rate 0.000938333
2018-04-02T19:35:31.240246: step 1063, loss 0.00123399, acc 1, learning_rate 0.00093694
2018-04-02T19:35:32.307067: step 1064, loss 0.00784893, acc 1, learning_rate 0.000935548
2018-04-02T19:35:33.378719: step 1065, loss 0.00180849, acc 1, learning_rate 0.000934159
2018-04-02T19:35:34.450791: step 1066, loss 0.0020555, acc 1, learning_rate 0.000932772
2018-04-02T19:35:35.521452: step 1067, loss 0.00530709, acc 1, learning_rate 0.000931387
2018-04-02T19:35:36.594325: step 1068, loss 0.00117288, acc 1, learning_rate 0.000930005
2018-04-02T19:35:37.665306: step 1069, loss 0.00154851, acc 1, learning_rate 0.000928625
2018-04-02T19:35:38.739130: step 1070, loss 0.00357635, acc 1, learning_rate 0.000927247
2018-04-02T19:35:39.809956: step 1071, loss 0.00234729, acc 1, learning_rate 0.000925872
2018-04-02T19:35:40.882427: step 1072, loss 0.00298774, acc 1, learning_rate 0.000924498
2018-04-02T19:35:41.959008: step 1073, loss 0.00192326, acc 1, learning_rate 0.000923128
2018-04-02T19:35:43.026090: step 1074, loss 0.00987174, acc 1, learning_rate 0.000921759
2018-04-02T19:35:44.096592: step 1075, loss 0.00344966, acc 1, learning_rate 0.000920393
2018-04-02T19:35:45.167295: step 1076, loss 0.00315212, acc 1, learning_rate 0.000919029
2018-04-02T19:35:46.237745: step 1077, loss 0.102767, acc 0.984375, learning_rate 0.000917667
2018-04-02T19:35:47.316595: step 1078, loss 0.00238058, acc 1, learning_rate 0.000916307
2018-04-02T19:35:48.387947: step 1079, loss 0.00358629, acc 1, learning_rate 0.00091495
2018-04-02T19:35:49.459695: step 1080, loss 0.0361055, acc 0.984375, learning_rate 0.000913595
2018-04-02T19:35:50.526146: step 1081, loss 0.00327679, acc 1, learning_rate 0.000912242
2018-04-02T19:35:51.598035: step 1082, loss 0.00338524, acc 1, learning_rate 0.000910892
2018-04-02T19:35:52.672226: step 1083, loss 0.00636144, acc 1, learning_rate 0.000909543
2018-04-02T19:35:53.743040: step 1084, loss 0.00213681, acc 1, learning_rate 0.000908197
2018-04-02T19:35:54.812038: step 1085, loss 0.00255766, acc 1, learning_rate 0.000906854
2018-04-02T19:35:55.885680: step 1086, loss 0.00361576, acc 1, learning_rate 0.000905512
2018-04-02T19:35:56.957886: step 1087, loss 0.00122492, acc 1, learning_rate 0.000904173
2018-04-02T19:35:58.029599: step 1088, loss 0.00212857, acc 1, learning_rate 0.000902836
2018-04-02T19:35:59.103101: step 1089, loss 0.00197312, acc 1, learning_rate 0.000901501
2018-04-02T19:36:00.168951: step 1090, loss 0.00176808, acc 1, learning_rate 0.000900168
2018-04-02T19:36:01.240932: step 1091, loss 0.00224143, acc 1, learning_rate 0.000898838
2018-04-02T19:36:02.312023: step 1092, loss 0.00417449, acc 1, learning_rate 0.00089751
2018-04-02T19:36:03.385058: step 1093, loss 0.0682524, acc 0.984375, learning_rate 0.000896184
2018-04-02T19:36:04.455425: step 1094, loss 0.00208178, acc 1, learning_rate 0.00089486
2018-04-02T19:36:05.530334: step 1095, loss 0.000969809, acc 1, learning_rate 0.000893538
2018-04-02T19:36:06.600967: step 1096, loss 0.00895217, acc 1, learning_rate 0.000892219
2018-04-02T19:36:07.673445: step 1097, loss 0.00264763, acc 1, learning_rate 0.000890902
2018-04-02T19:36:08.747996: step 1098, loss 0.0246057, acc 0.984375, learning_rate 0.000889587
2018-04-02T19:36:09.820108: step 1099, loss 0.0568848, acc 0.96875, learning_rate 0.000888274
2018-04-02T19:36:10.889175: step 1100, loss 0.00585603, acc 1, learning_rate 0.000886963

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:36:11.078600: step 1100, loss 0.046436, acc 0.978986

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1100

2018-04-02T19:36:15.598891: step 1101, loss 0.00595059, acc 1, learning_rate 0.000885655
2018-04-02T19:36:16.668113: step 1102, loss 0.0556489, acc 0.984375, learning_rate 0.000884348
2018-04-02T19:36:17.740477: step 1103, loss 0.00380878, acc 1, learning_rate 0.000883044
2018-04-02T19:36:18.810927: step 1104, loss 0.0891009, acc 0.984375, learning_rate 0.000881742
2018-04-02T19:36:19.876169: step 1105, loss 0.0305355, acc 0.984375, learning_rate 0.000880442
2018-04-02T19:36:20.947731: step 1106, loss 0.00310079, acc 1, learning_rate 0.000879145
2018-04-02T19:36:22.018742: step 1107, loss 0.0139054, acc 1, learning_rate 0.000877849
2018-04-02T19:36:23.089585: step 1108, loss 0.0417793, acc 0.984375, learning_rate 0.000876556
2018-04-02T19:36:24.161171: step 1109, loss 0.00616085, acc 1, learning_rate 0.000875265
2018-04-02T19:36:25.230833: step 1110, loss 0.00790931, acc 1, learning_rate 0.000873976
2018-04-02T19:36:26.302599: step 1111, loss 0.00194767, acc 1, learning_rate 0.000872689
2018-04-02T19:36:27.375957: step 1112, loss 0.00393062, acc 1, learning_rate 0.000871404
2018-04-02T19:36:28.449604: step 1113, loss 0.00304786, acc 1, learning_rate 0.000870122
2018-04-02T19:36:29.522801: step 1114, loss 0.00585289, acc 1, learning_rate 0.000868841
2018-04-02T19:36:30.600812: step 1115, loss 0.00397704, acc 1, learning_rate 0.000867563
2018-04-02T19:36:31.674880: step 1116, loss 0.00543127, acc 1, learning_rate 0.000866287
2018-04-02T19:36:32.745990: step 1117, loss 0.00552976, acc 1, learning_rate 0.000865013
2018-04-02T19:36:33.817733: step 1118, loss 0.0027627, acc 1, learning_rate 0.000863741
2018-04-02T19:36:34.890129: step 1119, loss 0.01123, acc 1, learning_rate 0.000862471
2018-04-02T19:36:35.962038: step 1120, loss 0.00267854, acc 1, learning_rate 0.000861203
2018-04-02T19:36:37.037209: step 1121, loss 0.0047941, acc 1, learning_rate 0.000859937
2018-04-02T19:36:38.106228: step 1122, loss 0.0062945, acc 1, learning_rate 0.000858674
2018-04-02T19:36:39.179692: step 1123, loss 0.00309169, acc 1, learning_rate 0.000857412
2018-04-02T19:36:40.249499: step 1124, loss 0.00230188, acc 1, learning_rate 0.000856153
2018-04-02T19:36:41.320320: step 1125, loss 0.0612122, acc 0.984375, learning_rate 0.000854896
2018-04-02T19:36:42.394434: step 1126, loss 0.00164976, acc 1, learning_rate 0.000853641
2018-04-02T19:36:43.465124: step 1127, loss 0.0011356, acc 1, learning_rate 0.000852388
2018-04-02T19:36:44.537466: step 1128, loss 0.00344789, acc 1, learning_rate 0.000851137
2018-04-02T19:36:45.611327: step 1129, loss 0.00248889, acc 1, learning_rate 0.000849888
2018-04-02T19:36:46.686239: step 1130, loss 0.0016783, acc 1, learning_rate 0.000848641
2018-04-02T19:36:47.759867: step 1131, loss 0.00108775, acc 1, learning_rate 0.000847396
2018-04-02T19:36:48.831347: step 1132, loss 0.00646721, acc 1, learning_rate 0.000846153
2018-04-02T19:36:49.903203: step 1133, loss 0.00101589, acc 1, learning_rate 0.000844913
2018-04-02T19:36:50.973980: step 1134, loss 0.00516112, acc 1, learning_rate 0.000843674
2018-04-02T19:36:52.047018: step 1135, loss 0.00255862, acc 1, learning_rate 0.000842438
2018-04-02T19:36:53.117183: step 1136, loss 0.0583301, acc 0.984375, learning_rate 0.000841203
2018-04-02T19:36:54.190882: step 1137, loss 0.00187426, acc 1, learning_rate 0.000839971
2018-04-02T19:36:55.259660: step 1138, loss 0.00592645, acc 1, learning_rate 0.000838741
2018-04-02T19:36:56.332686: step 1139, loss 0.00274642, acc 1, learning_rate 0.000837512
2018-04-02T19:36:57.406126: step 1140, loss 0.00642864, acc 1, learning_rate 0.000836286
2018-04-02T19:36:58.477976: step 1141, loss 0.00238779, acc 1, learning_rate 0.000835062
2018-04-02T19:36:59.548193: step 1142, loss 0.00406443, acc 1, learning_rate 0.00083384
2018-04-02T19:37:00.620665: step 1143, loss 0.0157847, acc 0.984375, learning_rate 0.00083262
2018-04-02T19:37:01.695367: step 1144, loss 0.00503492, acc 1, learning_rate 0.000831401
2018-04-02T19:37:02.770208: step 1145, loss 0.00387697, acc 1, learning_rate 0.000830185
2018-04-02T19:37:03.839906: step 1146, loss 0.00233759, acc 1, learning_rate 0.000828971
2018-04-02T19:37:04.913593: step 1147, loss 0.00678182, acc 1, learning_rate 0.000827759
2018-04-02T19:37:05.986731: step 1148, loss 0.00175303, acc 1, learning_rate 0.000826549
2018-04-02T19:37:07.056281: step 1149, loss 0.00299762, acc 1, learning_rate 0.000825341
2018-04-02T19:37:08.129007: step 1150, loss 0.00266521, acc 1, learning_rate 0.000824135
2018-04-02T19:37:09.200191: step 1151, loss 0.00171842, acc 1, learning_rate 0.000822931
2018-04-02T19:37:10.271356: step 1152, loss 0.0097216, acc 1, learning_rate 0.000821729
2018-04-02T19:37:11.340514: step 1153, loss 0.00155519, acc 1, learning_rate 0.000820529
2018-04-02T19:37:12.409035: step 1154, loss 0.00248538, acc 1, learning_rate 0.000819331
2018-04-02T19:37:13.476452: step 1155, loss 0.00092471, acc 1, learning_rate 0.000818135
2018-04-02T19:37:14.549175: step 1156, loss 0.00245508, acc 1, learning_rate 0.000816941
2018-04-02T19:37:15.619996: step 1157, loss 0.00388447, acc 1, learning_rate 0.000815749
2018-04-02T19:37:16.689084: step 1158, loss 0.00249224, acc 1, learning_rate 0.000814559
2018-04-02T19:37:17.764961: step 1159, loss 0.00594768, acc 1, learning_rate 0.000813371
2018-04-02T19:37:18.837726: step 1160, loss 0.0638991, acc 0.984375, learning_rate 0.000812185
2018-04-02T19:37:19.910533: step 1161, loss 0.00151127, acc 1, learning_rate 0.000811001
2018-04-02T19:37:20.979386: step 1162, loss 0.00515449, acc 1, learning_rate 0.000809818
2018-04-02T19:37:22.051907: step 1163, loss 0.000879519, acc 1, learning_rate 0.000808638
2018-04-02T19:37:23.122915: step 1164, loss 0.0025143, acc 1, learning_rate 0.00080746
2018-04-02T19:37:24.193623: step 1165, loss 0.000933123, acc 1, learning_rate 0.000806284
2018-04-02T19:37:25.264089: step 1166, loss 0.00396621, acc 1, learning_rate 0.000805109
2018-04-02T19:37:26.336652: step 1167, loss 0.00164543, acc 1, learning_rate 0.000803937
2018-04-02T19:37:27.408987: step 1168, loss 0.00270587, acc 1, learning_rate 0.000802767
2018-04-02T19:37:28.478809: step 1169, loss 0.00646322, acc 1, learning_rate 0.000801598
2018-04-02T19:37:29.549338: step 1170, loss 0.00375976, acc 1, learning_rate 0.000800432
2018-04-02T19:37:30.622096: step 1171, loss 0.00316378, acc 1, learning_rate 0.000799267
2018-04-02T19:37:31.692814: step 1172, loss 0.00190961, acc 1, learning_rate 0.000798104
2018-04-02T19:37:32.766253: step 1173, loss 0.00120987, acc 1, learning_rate 0.000796944
2018-04-02T19:37:33.835865: step 1174, loss 0.00172764, acc 1, learning_rate 0.000795785
2018-04-02T19:37:34.905425: step 1175, loss 0.00298624, acc 1, learning_rate 0.000794628
2018-04-02T19:37:35.978860: step 1176, loss 0.00380527, acc 1, learning_rate 0.000793473
2018-04-02T19:37:37.050626: step 1177, loss 0.0053485, acc 1, learning_rate 0.00079232
2018-04-02T19:37:38.127393: step 1178, loss 0.00762977, acc 1, learning_rate 0.000791169
2018-04-02T19:37:39.199382: step 1179, loss 0.00295608, acc 1, learning_rate 0.00079002
2018-04-02T19:37:40.270372: step 1180, loss 0.0041369, acc 1, learning_rate 0.000788872
2018-04-02T19:37:41.341241: step 1181, loss 0.0947403, acc 0.96875, learning_rate 0.000787727
2018-04-02T19:37:42.414329: step 1182, loss 0.00471997, acc 1, learning_rate 0.000786584
2018-04-02T19:37:43.482823: step 1183, loss 0.00297842, acc 1, learning_rate 0.000785442
2018-04-02T19:37:44.551185: step 1184, loss 0.00290012, acc 1, learning_rate 0.000784302
2018-04-02T19:37:45.623522: step 1185, loss 0.00445623, acc 1, learning_rate 0.000783164
2018-04-02T19:37:46.694980: step 1186, loss 0.00363327, acc 1, learning_rate 0.000782029
2018-04-02T19:37:47.768669: step 1187, loss 0.012716, acc 1, learning_rate 0.000780895
2018-04-02T19:37:48.839529: step 1188, loss 0.00317284, acc 1, learning_rate 0.000779763
2018-04-02T19:37:49.911442: step 1189, loss 0.00199514, acc 1, learning_rate 0.000778632
2018-04-02T19:37:50.983985: step 1190, loss 0.0189658, acc 0.984375, learning_rate 0.000777504
2018-04-02T19:37:52.055818: step 1191, loss 0.0012021, acc 1, learning_rate 0.000776377
2018-04-02T19:37:53.123806: step 1192, loss 0.00663388, acc 1, learning_rate 0.000775253
2018-04-02T19:37:54.193830: step 1193, loss 0.0746723, acc 0.984375, learning_rate 0.00077413
2018-04-02T19:37:55.263497: step 1194, loss 0.00266647, acc 1, learning_rate 0.000773009
2018-04-02T19:37:56.336677: step 1195, loss 0.00265537, acc 1, learning_rate 0.00077189
2018-04-02T19:37:57.409892: step 1196, loss 0.0018847, acc 1, learning_rate 0.000770773
2018-04-02T19:37:58.481763: step 1197, loss 0.00286901, acc 1, learning_rate 0.000769658
2018-04-02T19:37:59.553110: step 1198, loss 0.000305569, acc 1, learning_rate 0.000768544
2018-04-02T19:38:00.625411: step 1199, loss 0.0017402, acc 1, learning_rate 0.000767433
2018-04-02T19:38:01.695053: step 1200, loss 0.00331545, acc 1, learning_rate 0.000766323

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:38:01.884892: step 1200, loss 0.0533459, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1200

2018-04-02T19:38:05.724028: step 1201, loss 0.000808536, acc 1, learning_rate 0.000765215
2018-04-02T19:38:06.797500: step 1202, loss 0.00175311, acc 1, learning_rate 0.000764109
2018-04-02T19:38:07.868559: step 1203, loss 0.00574472, acc 1, learning_rate 0.000763005
2018-04-02T19:38:08.939640: step 1204, loss 0.0632679, acc 0.984375, learning_rate 0.000761903
2018-04-02T19:38:09.964930: step 1205, loss 0.00491594, acc 1, learning_rate 0.000760802
2018-04-02T19:38:11.041917: step 1206, loss 0.000917367, acc 1, learning_rate 0.000759703
2018-04-02T19:38:12.107908: step 1207, loss 0.00534102, acc 1, learning_rate 0.000758606
2018-04-02T19:38:13.180190: step 1208, loss 0.00413985, acc 1, learning_rate 0.000757511
2018-04-02T19:38:14.249149: step 1209, loss 0.00146155, acc 1, learning_rate 0.000756418
2018-04-02T19:38:15.318295: step 1210, loss 0.00247213, acc 1, learning_rate 0.000755327
2018-04-02T19:38:16.394586: step 1211, loss 0.0052082, acc 1, learning_rate 0.000754237
2018-04-02T19:38:17.467684: step 1212, loss 0.00279861, acc 1, learning_rate 0.000753149
2018-04-02T19:38:18.539613: step 1213, loss 0.00338885, acc 1, learning_rate 0.000752063
2018-04-02T19:38:19.617728: step 1214, loss 0.00141781, acc 1, learning_rate 0.000750979
2018-04-02T19:38:20.694238: step 1215, loss 0.00457637, acc 1, learning_rate 0.000749897
2018-04-02T19:38:21.772129: step 1216, loss 0.00563652, acc 1, learning_rate 0.000748816
2018-04-02T19:38:22.840271: step 1217, loss 0.00437901, acc 1, learning_rate 0.000747738
2018-04-02T19:38:23.915027: step 1218, loss 0.00166932, acc 1, learning_rate 0.000746661
2018-04-02T19:38:24.992262: step 1219, loss 0.00600535, acc 1, learning_rate 0.000745585
2018-04-02T19:38:26.059812: step 1220, loss 0.0139107, acc 1, learning_rate 0.000744512
2018-04-02T19:38:27.133081: step 1221, loss 0.00868486, acc 1, learning_rate 0.00074344
2018-04-02T19:38:28.206138: step 1222, loss 0.00361283, acc 1, learning_rate 0.00074237
2018-04-02T19:38:29.284576: step 1223, loss 0.00180774, acc 1, learning_rate 0.000741302
2018-04-02T19:38:30.357777: step 1224, loss 0.00858906, acc 1, learning_rate 0.000740236
2018-04-02T19:38:31.427724: step 1225, loss 0.000795249, acc 1, learning_rate 0.000739172
2018-04-02T19:38:32.505972: step 1226, loss 0.00290921, acc 1, learning_rate 0.000738109
2018-04-02T19:38:33.577030: step 1227, loss 0.0672009, acc 0.984375, learning_rate 0.000737048
2018-04-02T19:38:34.650096: step 1228, loss 0.00148072, acc 1, learning_rate 0.000735989
2018-04-02T19:38:35.730336: step 1229, loss 0.00197108, acc 1, learning_rate 0.000734931
2018-04-02T19:38:36.799829: step 1230, loss 0.00217111, acc 1, learning_rate 0.000733876
2018-04-02T19:38:37.881818: step 1231, loss 0.00436445, acc 1, learning_rate 0.000732822
2018-04-02T19:38:38.952327: step 1232, loss 0.00611085, acc 1, learning_rate 0.000731769
2018-04-02T19:38:40.034452: step 1233, loss 0.00211415, acc 1, learning_rate 0.000730719
2018-04-02T19:38:41.115248: step 1234, loss 0.00411153, acc 1, learning_rate 0.00072967
2018-04-02T19:38:42.192749: step 1235, loss 0.00593609, acc 1, learning_rate 0.000728623
2018-04-02T19:38:43.263668: step 1236, loss 0.00280279, acc 1, learning_rate 0.000727578
2018-04-02T19:38:44.341750: step 1237, loss 0.00167844, acc 1, learning_rate 0.000726535
2018-04-02T19:38:45.412115: step 1238, loss 0.00320842, acc 1, learning_rate 0.000725493
2018-04-02T19:38:46.493695: step 1239, loss 0.00137024, acc 1, learning_rate 0.000724453
2018-04-02T19:38:47.566789: step 1240, loss 0.000962595, acc 1, learning_rate 0.000723415
2018-04-02T19:38:48.635152: step 1241, loss 0.00299312, acc 1, learning_rate 0.000722378
2018-04-02T19:38:49.714642: step 1242, loss 0.0635423, acc 0.984375, learning_rate 0.000721343
2018-04-02T19:38:50.794069: step 1243, loss 0.000837237, acc 1, learning_rate 0.00072031
2018-04-02T19:38:51.874483: step 1244, loss 0.000632761, acc 1, learning_rate 0.000719279
2018-04-02T19:38:52.946812: step 1245, loss 0.00289505, acc 1, learning_rate 0.000718249
2018-04-02T19:38:54.023034: step 1246, loss 0.00233004, acc 1, learning_rate 0.000717221
2018-04-02T19:38:55.095813: step 1247, loss 0.00215674, acc 1, learning_rate 0.000716195
2018-04-02T19:38:56.166171: step 1248, loss 0.00243476, acc 1, learning_rate 0.000715171
2018-04-02T19:38:57.247723: step 1249, loss 0.00172478, acc 1, learning_rate 0.000714148
2018-04-02T19:38:58.330466: step 1250, loss 0.00172411, acc 1, learning_rate 0.000713127
2018-04-02T19:38:59.410132: step 1251, loss 0.00687346, acc 1, learning_rate 0.000712107
2018-04-02T19:39:00.483170: step 1252, loss 0.00227986, acc 1, learning_rate 0.000711089
2018-04-02T19:39:01.552216: step 1253, loss 0.0904238, acc 0.984375, learning_rate 0.000710073
2018-04-02T19:39:02.631527: step 1254, loss 0.00219469, acc 1, learning_rate 0.000709059
2018-04-02T19:39:03.711432: step 1255, loss 0.00106497, acc 1, learning_rate 0.000708046
2018-04-02T19:39:04.779689: step 1256, loss 0.00148912, acc 1, learning_rate 0.000707035
2018-04-02T19:39:05.853548: step 1257, loss 0.00201687, acc 1, learning_rate 0.000706026
2018-04-02T19:39:06.923548: step 1258, loss 0.00181407, acc 1, learning_rate 0.000705018
2018-04-02T19:39:08.003235: step 1259, loss 0.000594827, acc 1, learning_rate 0.000704012
2018-04-02T19:39:09.072988: step 1260, loss 0.00123674, acc 1, learning_rate 0.000703008
2018-04-02T19:39:10.152682: step 1261, loss 0.00230051, acc 1, learning_rate 0.000702006
2018-04-02T19:39:11.228245: step 1262, loss 0.00183974, acc 1, learning_rate 0.000701005
2018-04-02T19:39:12.302527: step 1263, loss 0.00603431, acc 1, learning_rate 0.000700005
2018-04-02T19:39:13.372367: step 1264, loss 0.00743068, acc 1, learning_rate 0.000699008
2018-04-02T19:39:14.445435: step 1265, loss 0.0322659, acc 0.984375, learning_rate 0.000698012
2018-04-02T19:39:15.519973: step 1266, loss 0.00355722, acc 1, learning_rate 0.000697017
2018-04-02T19:39:16.593907: step 1267, loss 0.0170457, acc 0.984375, learning_rate 0.000696025
2018-04-02T19:39:17.665087: step 1268, loss 0.00242078, acc 1, learning_rate 0.000695034
2018-04-02T19:39:18.738675: step 1269, loss 0.00113928, acc 1, learning_rate 0.000694044
2018-04-02T19:39:19.806968: step 1270, loss 0.00312628, acc 1, learning_rate 0.000693057
2018-04-02T19:39:20.884712: step 1271, loss 0.000203552, acc 1, learning_rate 0.000692071
2018-04-02T19:39:21.952319: step 1272, loss 0.00614468, acc 1, learning_rate 0.000691086
2018-04-02T19:39:23.021073: step 1273, loss 0.00156756, acc 1, learning_rate 0.000690103
2018-04-02T19:39:24.089101: step 1274, loss 0.0208589, acc 0.984375, learning_rate 0.000689122
2018-04-02T19:39:25.160135: step 1275, loss 0.00247639, acc 1, learning_rate 0.000688143
2018-04-02T19:39:26.240174: step 1276, loss 0.0022567, acc 1, learning_rate 0.000687165
2018-04-02T19:39:27.313042: step 1277, loss 0.00124198, acc 1, learning_rate 0.000686189
2018-04-02T19:39:28.382540: step 1278, loss 0.00142106, acc 1, learning_rate 0.000685214
2018-04-02T19:39:29.459029: step 1279, loss 0.00313831, acc 1, learning_rate 0.000684241
2018-04-02T19:39:30.527235: step 1280, loss 0.0039301, acc 1, learning_rate 0.00068327
2018-04-02T19:39:31.597666: step 1281, loss 0.000806549, acc 1, learning_rate 0.0006823
2018-04-02T19:39:32.675313: step 1282, loss 0.00151989, acc 1, learning_rate 0.000681332
2018-04-02T19:39:33.745654: step 1283, loss 0.00181334, acc 1, learning_rate 0.000680365
2018-04-02T19:39:34.824015: step 1284, loss 0.00816097, acc 1, learning_rate 0.0006794
2018-04-02T19:39:35.896807: step 1285, loss 0.00158323, acc 1, learning_rate 0.000678437
2018-04-02T19:39:36.966986: step 1286, loss 0.0834797, acc 0.984375, learning_rate 0.000677475
2018-04-02T19:39:38.039040: step 1287, loss 0.00304286, acc 1, learning_rate 0.000676515
2018-04-02T19:39:39.118163: step 1288, loss 0.000607337, acc 1, learning_rate 0.000675556
2018-04-02T19:39:40.192856: step 1289, loss 0.0055354, acc 1, learning_rate 0.000674599
2018-04-02T19:39:41.268174: step 1290, loss 0.0020073, acc 1, learning_rate 0.000673644
2018-04-02T19:39:42.347006: step 1291, loss 0.00205894, acc 1, learning_rate 0.00067269
2018-04-02T19:39:43.427978: step 1292, loss 0.00213261, acc 1, learning_rate 0.000671738
2018-04-02T19:39:44.500674: step 1293, loss 0.00149792, acc 1, learning_rate 0.000670787
2018-04-02T19:39:45.568386: step 1294, loss 0.00480969, acc 1, learning_rate 0.000669838
2018-04-02T19:39:46.647683: step 1295, loss 0.00297584, acc 1, learning_rate 0.000668891
2018-04-02T19:39:47.728600: step 1296, loss 0.0111642, acc 1, learning_rate 0.000667945
2018-04-02T19:39:48.797140: step 1297, loss 0.00473013, acc 1, learning_rate 0.000667001
2018-04-02T19:39:49.868714: step 1298, loss 0.00207705, acc 1, learning_rate 0.000666058
2018-04-02T19:39:50.935524: step 1299, loss 0.00231777, acc 1, learning_rate 0.000665117
2018-04-02T19:39:52.012829: step 1300, loss 0.00165253, acc 1, learning_rate 0.000664177

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:39:52.202302: step 1300, loss 0.0445203, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1300

2018-04-02T19:39:56.353637: step 1301, loss 0.00849531, acc 1, learning_rate 0.000663239
2018-04-02T19:39:57.432978: step 1302, loss 0.0017765, acc 1, learning_rate 0.000662302
2018-04-02T19:39:58.513935: step 1303, loss 0.00240215, acc 1, learning_rate 0.000661368
2018-04-02T19:39:59.592203: step 1304, loss 0.00332641, acc 1, learning_rate 0.000660434
2018-04-02T19:40:00.660720: step 1305, loss 0.00194102, acc 1, learning_rate 0.000659502
2018-04-02T19:40:01.740274: step 1306, loss 0.00125741, acc 1, learning_rate 0.000658572
2018-04-02T19:40:02.812088: step 1307, loss 0.00273301, acc 1, learning_rate 0.000657643
2018-04-02T19:40:03.889189: step 1308, loss 0.092814, acc 0.984375, learning_rate 0.000656716
2018-04-02T19:40:04.971371: step 1309, loss 0.00455398, acc 1, learning_rate 0.00065579
2018-04-02T19:40:06.049761: step 1310, loss 0.00165603, acc 1, learning_rate 0.000654866
2018-04-02T19:40:07.120659: step 1311, loss 0.0015746, acc 1, learning_rate 0.000653944
2018-04-02T19:40:08.198295: step 1312, loss 0.00342893, acc 1, learning_rate 0.000653023
2018-04-02T19:40:09.275340: step 1313, loss 0.0027774, acc 1, learning_rate 0.000652103
2018-04-02T19:40:10.347899: step 1314, loss 0.000799676, acc 1, learning_rate 0.000651185
2018-04-02T19:40:11.422583: step 1315, loss 0.00271551, acc 1, learning_rate 0.000650269
2018-04-02T19:40:12.499126: step 1316, loss 0.00298524, acc 1, learning_rate 0.000649354
2018-04-02T19:40:13.578659: step 1317, loss 0.00127117, acc 1, learning_rate 0.000648441
2018-04-02T19:40:14.658398: step 1318, loss 0.0020419, acc 1, learning_rate 0.000647529
2018-04-02T19:40:15.739134: step 1319, loss 0.065163, acc 0.984375, learning_rate 0.000646618
2018-04-02T19:40:16.810005: step 1320, loss 0.0033589, acc 1, learning_rate 0.000645709
2018-04-02T19:40:17.880272: step 1321, loss 0.00112606, acc 1, learning_rate 0.000644802
2018-04-02T19:40:18.959432: step 1322, loss 0.00156571, acc 1, learning_rate 0.000643896
2018-04-02T19:40:20.033636: step 1323, loss 0.0686687, acc 0.984375, learning_rate 0.000642992
2018-04-02T19:40:21.115419: step 1324, loss 0.00334187, acc 1, learning_rate 0.000642089
2018-04-02T19:40:22.192154: step 1325, loss 0.00318932, acc 1, learning_rate 0.000641188
2018-04-02T19:40:23.270262: step 1326, loss 0.00479914, acc 1, learning_rate 0.000640288
2018-04-02T19:40:24.347487: step 1327, loss 0.00488554, acc 1, learning_rate 0.00063939
2018-04-02T19:40:25.424862: step 1328, loss 0.0631274, acc 0.984375, learning_rate 0.000638493
2018-04-02T19:40:26.503873: step 1329, loss 0.00997063, acc 1, learning_rate 0.000637597
2018-04-02T19:40:27.582580: step 1330, loss 0.00555305, acc 1, learning_rate 0.000636704
2018-04-02T19:40:28.661270: step 1331, loss 0.00111977, acc 1, learning_rate 0.000635811
2018-04-02T19:40:29.727777: step 1332, loss 0.00365121, acc 1, learning_rate 0.00063492
2018-04-02T19:40:30.807720: step 1333, loss 0.000525365, acc 1, learning_rate 0.000634031
2018-04-02T19:40:31.876253: step 1334, loss 0.0059311, acc 1, learning_rate 0.000633143
2018-04-02T19:40:32.957228: step 1335, loss 0.0749642, acc 0.984375, learning_rate 0.000632257
2018-04-02T19:40:34.037005: step 1336, loss 0.00421561, acc 1, learning_rate 0.000631372
2018-04-02T19:40:35.118130: step 1337, loss 0.00645738, acc 1, learning_rate 0.000630488
2018-04-02T19:40:36.206883: step 1338, loss 0.0010585, acc 1, learning_rate 0.000629606
2018-04-02T19:40:37.277942: step 1339, loss 0.00754085, acc 1, learning_rate 0.000628726
2018-04-02T19:40:38.360637: step 1340, loss 0.00242769, acc 1, learning_rate 0.000627846
2018-04-02T19:40:39.439535: step 1341, loss 0.00838805, acc 1, learning_rate 0.000626969
2018-04-02T19:40:40.511583: step 1342, loss 0.00337626, acc 1, learning_rate 0.000626093
2018-04-02T19:40:41.584084: step 1343, loss 0.00419249, acc 1, learning_rate 0.000625218
2018-04-02T19:40:42.666593: step 1344, loss 0.00485948, acc 1, learning_rate 0.000624345
2018-04-02T19:40:43.749214: step 1345, loss 0.00831836, acc 1, learning_rate 0.000623473
2018-04-02T19:40:44.830852: step 1346, loss 0.0013212, acc 1, learning_rate 0.000622602
2018-04-02T19:40:45.911313: step 1347, loss 0.00200918, acc 1, learning_rate 0.000621733
2018-04-02T19:40:46.992710: step 1348, loss 0.0016451, acc 1, learning_rate 0.000620866
2018-04-02T19:40:48.066807: step 1349, loss 0.0018859, acc 1, learning_rate 0.00062
2018-04-02T19:40:49.140117: step 1350, loss 0.00249094, acc 1, learning_rate 0.000619135
2018-04-02T19:40:50.212264: step 1351, loss 0.00308223, acc 1, learning_rate 0.000618272
2018-04-02T19:40:51.286338: step 1352, loss 0.00118912, acc 1, learning_rate 0.00061741
2018-04-02T19:40:52.360455: step 1353, loss 0.00123174, acc 1, learning_rate 0.00061655
2018-04-02T19:40:53.431539: step 1354, loss 0.00296207, acc 1, learning_rate 0.000615691
2018-04-02T19:40:54.512666: step 1355, loss 0.00177778, acc 1, learning_rate 0.000614834
2018-04-02T19:40:55.587178: step 1356, loss 0.00174205, acc 1, learning_rate 0.000613978
2018-04-02T19:40:56.667966: step 1357, loss 0.00571347, acc 1, learning_rate 0.000613123
2018-04-02T19:40:57.745544: step 1358, loss 0.00235044, acc 1, learning_rate 0.00061227
2018-04-02T19:40:58.829270: step 1359, loss 0.000318179, acc 1, learning_rate 0.000611418
2018-04-02T19:40:59.901104: step 1360, loss 0.00205162, acc 1, learning_rate 0.000610568
2018-04-02T19:41:00.982484: step 1361, loss 0.00248203, acc 1, learning_rate 0.000609719
2018-04-02T19:41:02.052015: step 1362, loss 0.00279895, acc 1, learning_rate 0.000608872
2018-04-02T19:41:03.133797: step 1363, loss 0.00158839, acc 1, learning_rate 0.000608026
2018-04-02T19:41:04.201418: step 1364, loss 0.00655752, acc 1, learning_rate 0.000607181
2018-04-02T19:41:05.278792: step 1365, loss 0.00199796, acc 1, learning_rate 0.000606338
2018-04-02T19:41:06.349417: step 1366, loss 0.00384298, acc 1, learning_rate 0.000605496
2018-04-02T19:41:07.420675: step 1367, loss 0.0024573, acc 1, learning_rate 0.000604655
2018-04-02T19:41:08.490216: step 1368, loss 0.00270973, acc 1, learning_rate 0.000603816
2018-04-02T19:41:09.566322: step 1369, loss 0.000533181, acc 1, learning_rate 0.000602979
2018-04-02T19:41:10.639253: step 1370, loss 0.00477271, acc 1, learning_rate 0.000602142
2018-04-02T19:41:11.709819: step 1371, loss 0.00308347, acc 1, learning_rate 0.000601307
2018-04-02T19:41:12.781077: step 1372, loss 0.00413741, acc 1, learning_rate 0.000600474
2018-04-02T19:41:13.854414: step 1373, loss 0.00406464, acc 1, learning_rate 0.000599642
2018-04-02T19:41:14.926807: step 1374, loss 0.00395608, acc 1, learning_rate 0.000598811
2018-04-02T19:41:15.997403: step 1375, loss 0.00397828, acc 1, learning_rate 0.000597982
2018-04-02T19:41:17.072571: step 1376, loss 0.0155021, acc 1, learning_rate 0.000597154
2018-04-02T19:41:18.144226: step 1377, loss 0.000863633, acc 1, learning_rate 0.000596327
2018-04-02T19:41:19.218595: step 1378, loss 0.0088725, acc 1, learning_rate 0.000595502
2018-04-02T19:41:20.290216: step 1379, loss 0.00134322, acc 1, learning_rate 0.000594678
2018-04-02T19:41:21.363179: step 1380, loss 0.00250247, acc 1, learning_rate 0.000593855
2018-04-02T19:41:22.435993: step 1381, loss 0.000327322, acc 1, learning_rate 0.000593034
2018-04-02T19:41:23.504887: step 1382, loss 0.0032876, acc 1, learning_rate 0.000592214
2018-04-02T19:41:24.578096: step 1383, loss 0.00469095, acc 1, learning_rate 0.000591396
2018-04-02T19:41:25.648761: step 1384, loss 0.00124754, acc 1, learning_rate 0.000590579
2018-04-02T19:41:26.717443: step 1385, loss 0.00181091, acc 1, learning_rate 0.000589763
2018-04-02T19:41:27.787735: step 1386, loss 0.00151825, acc 1, learning_rate 0.000588949
2018-04-02T19:41:28.862164: step 1387, loss 0.000697502, acc 1, learning_rate 0.000588136
2018-04-02T19:41:29.931311: step 1388, loss 0.00491678, acc 1, learning_rate 0.000587324
2018-04-02T19:41:31.002998: step 1389, loss 0.00382905, acc 1, learning_rate 0.000586514
2018-04-02T19:41:32.073885: step 1390, loss 0.00259596, acc 1, learning_rate 0.000585705
2018-04-02T19:41:33.141474: step 1391, loss 0.00421232, acc 1, learning_rate 0.000584898
2018-04-02T19:41:34.208209: step 1392, loss 0.00204258, acc 1, learning_rate 0.000584091
2018-04-02T19:41:35.277138: step 1393, loss 0.0382641, acc 0.984375, learning_rate 0.000583287
2018-04-02T19:41:36.344182: step 1394, loss 0.0972558, acc 0.984375, learning_rate 0.000582483
2018-04-02T19:41:37.418039: step 1395, loss 0.00142523, acc 1, learning_rate 0.000581681
2018-04-02T19:41:38.490563: step 1396, loss 0.00176441, acc 1, learning_rate 0.00058088
2018-04-02T19:41:39.560269: step 1397, loss 0.00125541, acc 1, learning_rate 0.00058008
2018-04-02T19:41:40.633092: step 1398, loss 0.00278251, acc 1, learning_rate 0.000579282
2018-04-02T19:41:41.703812: step 1399, loss 0.00175642, acc 1, learning_rate 0.000578485
2018-04-02T19:41:42.773223: step 1400, loss 0.00413396, acc 1, learning_rate 0.00057769

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:41:42.963693: step 1400, loss 0.0438452, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1400

2018-04-02T19:41:47.217487: step 1401, loss 0.00133919, acc 1, learning_rate 0.000576895
2018-04-02T19:41:48.288727: step 1402, loss 0.00141758, acc 1, learning_rate 0.000576103
2018-04-02T19:41:49.359681: step 1403, loss 0.0101138, acc 1, learning_rate 0.000575311
2018-04-02T19:41:50.504298: step 1404, loss 0.00174025, acc 1, learning_rate 0.000574521
2018-04-02T19:41:51.577238: step 1405, loss 0.00167678, acc 1, learning_rate 0.000573732
2018-04-02T19:41:52.645065: step 1406, loss 0.0100415, acc 1, learning_rate 0.000572944
2018-04-02T19:41:53.716116: step 1407, loss 0.00205157, acc 1, learning_rate 0.000572158
2018-04-02T19:41:54.786358: step 1408, loss 0.00152578, acc 1, learning_rate 0.000571373
2018-04-02T19:41:55.856260: step 1409, loss 0.00254217, acc 1, learning_rate 0.000570589
2018-04-02T19:41:56.927601: step 1410, loss 0.00301098, acc 1, learning_rate 0.000569806
2018-04-02T19:41:57.998859: step 1411, loss 0.00621847, acc 1, learning_rate 0.000569025
2018-04-02T19:41:59.071844: step 1412, loss 0.0601107, acc 0.984375, learning_rate 0.000568245
2018-04-02T19:42:00.141640: step 1413, loss 0.00173377, acc 1, learning_rate 0.000567467
2018-04-02T19:42:01.210317: step 1414, loss 0.00275746, acc 1, learning_rate 0.00056669
2018-04-02T19:42:02.280589: step 1415, loss 0.00248186, acc 1, learning_rate 0.000565914
2018-04-02T19:42:03.347602: step 1416, loss 0.00456017, acc 1, learning_rate 0.000565139
2018-04-02T19:42:04.419245: step 1417, loss 0.0571512, acc 0.984375, learning_rate 0.000564366
2018-04-02T19:42:05.491350: step 1418, loss 0.00340073, acc 1, learning_rate 0.000563594
2018-04-02T19:42:06.565219: step 1419, loss 0.00437161, acc 1, learning_rate 0.000562823
2018-04-02T19:42:07.637283: step 1420, loss 0.00138136, acc 1, learning_rate 0.000562053
2018-04-02T19:42:08.709372: step 1421, loss 0.000245423, acc 1, learning_rate 0.000561285
2018-04-02T19:42:09.777807: step 1422, loss 0.00238563, acc 1, learning_rate 0.000560518
2018-04-02T19:42:10.846125: step 1423, loss 0.00416016, acc 1, learning_rate 0.000559752
2018-04-02T19:42:11.918942: step 1424, loss 0.00142698, acc 1, learning_rate 0.000558988
2018-04-02T19:42:12.985606: step 1425, loss 0.00160039, acc 1, learning_rate 0.000558225
2018-04-02T19:42:14.056507: step 1426, loss 0.00457986, acc 1, learning_rate 0.000557463
2018-04-02T19:42:15.130241: step 1427, loss 0.000473685, acc 1, learning_rate 0.000556702
2018-04-02T19:42:16.199210: step 1428, loss 0.0512875, acc 0.984375, learning_rate 0.000555943
2018-04-02T19:42:17.274474: step 1429, loss 0.00166386, acc 1, learning_rate 0.000555185
2018-04-02T19:42:18.345154: step 1430, loss 0.00889171, acc 1, learning_rate 0.000554428
2018-04-02T19:42:19.416742: step 1431, loss 0.00268559, acc 1, learning_rate 0.000553672
2018-04-02T19:42:20.489533: step 1432, loss 0.0665049, acc 0.984375, learning_rate 0.000552918
2018-04-02T19:42:21.561908: step 1433, loss 0.0014897, acc 1, learning_rate 0.000552165
2018-04-02T19:42:22.630173: step 1434, loss 0.0590736, acc 0.984375, learning_rate 0.000551413
2018-04-02T19:42:23.701416: step 1435, loss 0.00405286, acc 1, learning_rate 0.000550663
2018-04-02T19:42:24.772335: step 1436, loss 0.00330787, acc 1, learning_rate 0.000549913
2018-04-02T19:42:25.839844: step 1437, loss 0.00324422, acc 1, learning_rate 0.000549165
2018-04-02T19:42:26.911577: step 1438, loss 0.00460442, acc 1, learning_rate 0.000548418
2018-04-02T19:42:27.984777: step 1439, loss 0.00542108, acc 1, learning_rate 0.000547673
2018-04-02T19:42:29.057755: step 1440, loss 0.00335296, acc 1, learning_rate 0.000546929
2018-04-02T19:42:30.127129: step 1441, loss 0.00224335, acc 1, learning_rate 0.000546185
2018-04-02T19:42:31.195033: step 1442, loss 0.00218818, acc 1, learning_rate 0.000545444
2018-04-02T19:42:32.267159: step 1443, loss 0.00124459, acc 1, learning_rate 0.000544703
2018-04-02T19:42:33.337010: step 1444, loss 0.00103155, acc 1, learning_rate 0.000543964
2018-04-02T19:42:34.409474: step 1445, loss 0.00574771, acc 1, learning_rate 0.000543225
2018-04-02T19:42:35.439263: step 1446, loss 0.000827962, acc 1, learning_rate 0.000542488
2018-04-02T19:42:36.514058: step 1447, loss 0.0034463, acc 1, learning_rate 0.000541753
2018-04-02T19:42:37.585273: step 1448, loss 0.0659432, acc 0.984375, learning_rate 0.000541018
2018-04-02T19:42:38.663722: step 1449, loss 0.000575923, acc 1, learning_rate 0.000540285
2018-04-02T19:42:39.741164: step 1450, loss 0.00312099, acc 1, learning_rate 0.000539553
2018-04-02T19:42:40.811633: step 1451, loss 0.00179476, acc 1, learning_rate 0.000538822
2018-04-02T19:42:41.880844: step 1452, loss 0.0741883, acc 0.984375, learning_rate 0.000538092
2018-04-02T19:42:42.955604: step 1453, loss 0.00718155, acc 1, learning_rate 0.000537364
2018-04-02T19:42:44.024805: step 1454, loss 0.00497473, acc 1, learning_rate 0.000536637
2018-04-02T19:42:45.098285: step 1455, loss 0.00162561, acc 1, learning_rate 0.000535911
2018-04-02T19:42:46.168317: step 1456, loss 0.0626726, acc 0.984375, learning_rate 0.000535186
2018-04-02T19:42:47.239851: step 1457, loss 0.00776296, acc 1, learning_rate 0.000534462
2018-04-02T19:42:48.311126: step 1458, loss 0.00304989, acc 1, learning_rate 0.00053374
2018-04-02T19:42:49.388046: step 1459, loss 0.00158426, acc 1, learning_rate 0.000533019
2018-04-02T19:42:50.460878: step 1460, loss 0.00203196, acc 1, learning_rate 0.000532299
2018-04-02T19:42:51.528194: step 1461, loss 0.00285187, acc 1, learning_rate 0.00053158
2018-04-02T19:42:52.600409: step 1462, loss 0.00222369, acc 1, learning_rate 0.000530863
2018-04-02T19:42:53.675181: step 1463, loss 0.00322478, acc 1, learning_rate 0.000530146
2018-04-02T19:42:54.746343: step 1464, loss 0.00368288, acc 1, learning_rate 0.000529431
2018-04-02T19:42:55.815989: step 1465, loss 0.0660868, acc 0.984375, learning_rate 0.000528717
2018-04-02T19:42:56.890546: step 1466, loss 0.000840377, acc 1, learning_rate 0.000528004
2018-04-02T19:42:57.960218: step 1467, loss 0.00139747, acc 1, learning_rate 0.000527293
2018-04-02T19:42:59.034821: step 1468, loss 0.00159937, acc 1, learning_rate 0.000526582
2018-04-02T19:43:00.106374: step 1469, loss 0.00320801, acc 1, learning_rate 0.000525873
2018-04-02T19:43:01.175599: step 1470, loss 0.00616769, acc 1, learning_rate 0.000525165
2018-04-02T19:43:02.249261: step 1471, loss 0.00227343, acc 1, learning_rate 0.000524458
2018-04-02T19:43:03.319661: step 1472, loss 0.00142499, acc 1, learning_rate 0.000523752
2018-04-02T19:43:04.392370: step 1473, loss 0.00098772, acc 1, learning_rate 0.000523048
2018-04-02T19:43:05.462672: step 1474, loss 0.00298527, acc 1, learning_rate 0.000522344
2018-04-02T19:43:06.541022: step 1475, loss 0.00151517, acc 1, learning_rate 0.000521642
2018-04-02T19:43:07.618223: step 1476, loss 0.00246941, acc 1, learning_rate 0.000520941
2018-04-02T19:43:08.686196: step 1477, loss 0.00299263, acc 1, learning_rate 0.000520241
2018-04-02T19:43:09.764157: step 1478, loss 0.0030675, acc 1, learning_rate 0.000519542
2018-04-02T19:43:10.834697: step 1479, loss 0.0014338, acc 1, learning_rate 0.000518845
2018-04-02T19:43:11.913656: step 1480, loss 0.00464292, acc 1, learning_rate 0.000518148
2018-04-02T19:43:12.991157: step 1481, loss 0.00253537, acc 1, learning_rate 0.000517453
2018-04-02T19:43:14.061562: step 1482, loss 0.00114048, acc 1, learning_rate 0.000516759
2018-04-02T19:43:15.130136: step 1483, loss 0.00191311, acc 1, learning_rate 0.000516066
2018-04-02T19:43:16.201694: step 1484, loss 0.00282213, acc 1, learning_rate 0.000515374
2018-04-02T19:43:17.269704: step 1485, loss 0.00133359, acc 1, learning_rate 0.000514684
2018-04-02T19:43:18.343498: step 1486, loss 0.004243, acc 1, learning_rate 0.000513994
2018-04-02T19:43:19.420863: step 1487, loss 0.000849389, acc 1, learning_rate 0.000513306
2018-04-02T19:43:20.503641: step 1488, loss 0.00897856, acc 1, learning_rate 0.000512619
2018-04-02T19:43:21.580274: step 1489, loss 0.0734418, acc 0.984375, learning_rate 0.000511932
2018-04-02T19:43:22.658163: step 1490, loss 0.000837003, acc 1, learning_rate 0.000511248
2018-04-02T19:43:23.734400: step 1491, loss 0.00268552, acc 1, learning_rate 0.000510564
2018-04-02T19:43:24.804669: step 1492, loss 0.00116538, acc 1, learning_rate 0.000509881
2018-04-02T19:43:25.884184: step 1493, loss 0.00488836, acc 1, learning_rate 0.0005092
2018-04-02T19:43:26.960570: step 1494, loss 0.00500628, acc 1, learning_rate 0.000508519
2018-04-02T19:43:28.027417: step 1495, loss 0.00236704, acc 1, learning_rate 0.00050784
2018-04-02T19:43:29.101593: step 1496, loss 0.00220408, acc 1, learning_rate 0.000507162
2018-04-02T19:43:30.172101: step 1497, loss 0.00175381, acc 1, learning_rate 0.000506485
2018-04-02T19:43:31.250648: step 1498, loss 0.00673448, acc 1, learning_rate 0.000505809
2018-04-02T19:43:32.321497: step 1499, loss 0.00195435, acc 1, learning_rate 0.000505134
2018-04-02T19:43:33.387709: step 1500, loss 0.00235902, acc 1, learning_rate 0.000504461

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:43:33.577575: step 1500, loss 0.0452879, acc 0.982695

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1500

2018-04-02T19:43:38.221386: step 1501, loss 0.00380568, acc 1, learning_rate 0.000503788
2018-04-02T19:43:39.302210: step 1502, loss 0.00379891, acc 1, learning_rate 0.000503117
2018-04-02T19:43:40.381709: step 1503, loss 0.00180461, acc 1, learning_rate 0.000502447
2018-04-02T19:43:41.454785: step 1504, loss 0.00102085, acc 1, learning_rate 0.000501777
2018-04-02T19:43:42.535096: step 1505, loss 0.0019596, acc 1, learning_rate 0.000501109
2018-04-02T19:43:43.612883: step 1506, loss 0.00377849, acc 1, learning_rate 0.000500443
2018-04-02T19:43:44.684643: step 1507, loss 0.000570965, acc 1, learning_rate 0.000499777
2018-04-02T19:43:45.762630: step 1508, loss 0.00197433, acc 1, learning_rate 0.000499112
2018-04-02T19:43:46.844679: step 1509, loss 0.00155183, acc 1, learning_rate 0.000498448
2018-04-02T19:43:47.912885: step 1510, loss 0.0664624, acc 0.984375, learning_rate 0.000497786
2018-04-02T19:43:48.992468: step 1511, loss 0.000421407, acc 1, learning_rate 0.000497125
2018-04-02T19:43:50.063891: step 1512, loss 0.000838296, acc 1, learning_rate 0.000496464
2018-04-02T19:43:51.133474: step 1513, loss 0.00257256, acc 1, learning_rate 0.000495805
2018-04-02T19:43:52.204805: step 1514, loss 0.00433045, acc 1, learning_rate 0.000495147
2018-04-02T19:43:53.272992: step 1515, loss 0.0040455, acc 1, learning_rate 0.00049449
2018-04-02T19:43:54.346895: step 1516, loss 0.00368922, acc 1, learning_rate 0.000493834
2018-04-02T19:43:55.425346: step 1517, loss 0.00797636, acc 1, learning_rate 0.000493179
2018-04-02T19:43:56.506253: step 1518, loss 0.0039556, acc 1, learning_rate 0.000492526
2018-04-02T19:43:57.578226: step 1519, loss 0.000901382, acc 1, learning_rate 0.000491873
2018-04-02T19:43:58.652802: step 1520, loss 0.00443661, acc 1, learning_rate 0.000491221
2018-04-02T19:43:59.732826: step 1521, loss 0.000467656, acc 1, learning_rate 0.000490571
2018-04-02T19:44:00.806581: step 1522, loss 0.00148518, acc 1, learning_rate 0.000489921
2018-04-02T19:44:01.882356: step 1523, loss 0.000855158, acc 1, learning_rate 0.000489273
2018-04-02T19:44:02.958604: step 1524, loss 0.00415417, acc 1, learning_rate 0.000488626
2018-04-02T19:44:04.027944: step 1525, loss 0.00151287, acc 1, learning_rate 0.00048798
2018-04-02T19:44:05.097504: step 1526, loss 0.00326621, acc 1, learning_rate 0.000487335
2018-04-02T19:44:06.156312: step 1527, loss 0.0871224, acc 0.984375, learning_rate 0.000486691
2018-04-02T19:44:07.234838: step 1528, loss 0.00603287, acc 1, learning_rate 0.000486048
2018-04-02T19:44:08.317135: step 1529, loss 0.0022518, acc 1, learning_rate 0.000485406
2018-04-02T19:44:09.389206: step 1530, loss 0.00545341, acc 1, learning_rate 0.000484765
2018-04-02T19:44:10.457415: step 1531, loss 0.0020221, acc 1, learning_rate 0.000484125
2018-04-02T19:44:11.529329: step 1532, loss 0.0711284, acc 0.984375, learning_rate 0.000483487
2018-04-02T19:44:12.598593: step 1533, loss 0.00150169, acc 1, learning_rate 0.000482849
2018-04-02T19:44:13.673709: step 1534, loss 0.00499836, acc 1, learning_rate 0.000482212
2018-04-02T19:44:14.746225: step 1535, loss 0.00146168, acc 1, learning_rate 0.000481577
2018-04-02T19:44:15.821402: step 1536, loss 0.000965672, acc 1, learning_rate 0.000480942
2018-04-02T19:44:16.890521: step 1537, loss 0.124196, acc 0.96875, learning_rate 0.000480309
2018-04-02T19:44:17.969550: step 1538, loss 0.000786733, acc 1, learning_rate 0.000479677
2018-04-02T19:44:19.051068: step 1539, loss 0.00259039, acc 1, learning_rate 0.000479045
2018-04-02T19:44:20.121499: step 1540, loss 0.00325655, acc 1, learning_rate 0.000478415
2018-04-02T19:44:21.197527: step 1541, loss 0.00164984, acc 1, learning_rate 0.000477786
2018-04-02T19:44:22.267343: step 1542, loss 0.00384866, acc 1, learning_rate 0.000477158
2018-04-02T19:44:23.335339: step 1543, loss 0.00326373, acc 1, learning_rate 0.000476531
2018-04-02T19:44:24.406883: step 1544, loss 0.0112775, acc 1, learning_rate 0.000475905
2018-04-02T19:44:25.487710: step 1545, loss 0.0043426, acc 1, learning_rate 0.00047528
2018-04-02T19:44:26.566852: step 1546, loss 0.00263291, acc 1, learning_rate 0.000474656
2018-04-02T19:44:27.638401: step 1547, loss 0.00437205, acc 1, learning_rate 0.000474033
2018-04-02T19:44:28.715468: step 1548, loss 0.00580274, acc 1, learning_rate 0.000473411
2018-04-02T19:44:29.787198: step 1549, loss 0.00605183, acc 1, learning_rate 0.00047279
2018-04-02T19:44:30.864796: step 1550, loss 0.00339605, acc 1, learning_rate 0.00047217
2018-04-02T19:44:31.937351: step 1551, loss 0.00196931, acc 1, learning_rate 0.000471551
2018-04-02T19:44:33.008270: step 1552, loss 0.00256335, acc 1, learning_rate 0.000470934
2018-04-02T19:44:34.081909: step 1553, loss 0.0144, acc 1, learning_rate 0.000470317
2018-04-02T19:44:35.151923: step 1554, loss 0.0021966, acc 1, learning_rate 0.000469701
2018-04-02T19:44:36.224410: step 1555, loss 0.00334975, acc 1, learning_rate 0.000469087
2018-04-02T19:44:37.302361: step 1556, loss 0.0878994, acc 0.984375, learning_rate 0.000468473
2018-04-02T19:44:38.379758: step 1557, loss 0.00223226, acc 1, learning_rate 0.00046786
2018-04-02T19:44:39.452425: step 1558, loss 0.00125734, acc 1, learning_rate 0.000467249
2018-04-02T19:44:40.528874: step 1559, loss 0.00377771, acc 1, learning_rate 0.000466638
2018-04-02T19:44:41.604650: step 1560, loss 0.00157915, acc 1, learning_rate 0.000466028
2018-04-02T19:44:42.678274: step 1561, loss 0.0146293, acc 0.984375, learning_rate 0.00046542
2018-04-02T19:44:43.751124: step 1562, loss 0.00216868, acc 1, learning_rate 0.000464812
2018-04-02T19:44:44.818652: step 1563, loss 0.00013656, acc 1, learning_rate 0.000464206
2018-04-02T19:44:45.886817: step 1564, loss 0.00565594, acc 1, learning_rate 0.0004636
2018-04-02T19:44:46.954783: step 1565, loss 0.00325577, acc 1, learning_rate 0.000462995
2018-04-02T19:44:48.027295: step 1566, loss 0.00273678, acc 1, learning_rate 0.000462392
2018-04-02T19:44:49.107259: step 1567, loss 0.0416101, acc 0.984375, learning_rate 0.000461789
2018-04-02T19:44:50.185477: step 1568, loss 0.00325943, acc 1, learning_rate 0.000461188
2018-04-02T19:44:51.263709: step 1569, loss 0.00450057, acc 1, learning_rate 0.000460587
2018-04-02T19:44:52.336480: step 1570, loss 0.0208102, acc 0.984375, learning_rate 0.000459988
2018-04-02T19:44:53.416563: step 1571, loss 0.00422733, acc 1, learning_rate 0.000459389
2018-04-02T19:44:54.487740: step 1572, loss 0.00143543, acc 1, learning_rate 0.000458792
2018-04-02T19:44:55.566775: step 1573, loss 0.00158645, acc 1, learning_rate 0.000458195
2018-04-02T19:44:56.647351: step 1574, loss 0.00125537, acc 1, learning_rate 0.0004576
2018-04-02T19:44:57.720196: step 1575, loss 0.00349748, acc 1, learning_rate 0.000457005
2018-04-02T19:44:58.799592: step 1576, loss 0.00337476, acc 1, learning_rate 0.000456411
2018-04-02T19:44:59.880615: step 1577, loss 0.0887175, acc 0.984375, learning_rate 0.000455819
2018-04-02T19:45:00.960494: step 1578, loss 0.00538802, acc 1, learning_rate 0.000455227
2018-04-02T19:45:02.043467: step 1579, loss 0.000877515, acc 1, learning_rate 0.000454637
2018-04-02T19:45:03.117545: step 1580, loss 0.00140783, acc 1, learning_rate 0.000454047
2018-04-02T19:45:04.185476: step 1581, loss 0.00330085, acc 1, learning_rate 0.000453458
2018-04-02T19:45:05.256935: step 1582, loss 0.00621354, acc 1, learning_rate 0.000452871
2018-04-02T19:45:06.335400: step 1583, loss 0.0687291, acc 0.984375, learning_rate 0.000452284
2018-04-02T19:45:07.414967: step 1584, loss 0.00579958, acc 1, learning_rate 0.000451698
2018-04-02T19:45:08.493948: step 1585, loss 0.00188993, acc 1, learning_rate 0.000451113
2018-04-02T19:45:09.560625: step 1586, loss 0.00341843, acc 1, learning_rate 0.00045053
2018-04-02T19:45:10.641680: step 1587, loss 0.00741475, acc 1, learning_rate 0.000449947
2018-04-02T19:45:11.721015: step 1588, loss 0.0121148, acc 1, learning_rate 0.000449365
2018-04-02T19:45:12.795794: step 1589, loss 0.00715041, acc 1, learning_rate 0.000448784
2018-04-02T19:45:13.865947: step 1590, loss 0.000414711, acc 1, learning_rate 0.000448204
2018-04-02T19:45:14.936005: step 1591, loss 0.00188213, acc 1, learning_rate 0.000447625
2018-04-02T19:45:16.006544: step 1592, loss 0.00295569, acc 1, learning_rate 0.000447047
2018-04-02T19:45:17.086199: step 1593, loss 0.00459912, acc 1, learning_rate 0.00044647
2018-04-02T19:45:18.156528: step 1594, loss 0.00281058, acc 1, learning_rate 0.000445894
2018-04-02T19:45:19.230562: step 1595, loss 0.00471796, acc 1, learning_rate 0.000445319
2018-04-02T19:45:20.303215: step 1596, loss 0.00128129, acc 1, learning_rate 0.000444745
2018-04-02T19:45:21.379508: step 1597, loss 0.00260058, acc 1, learning_rate 0.000444172
2018-04-02T19:45:22.452984: step 1598, loss 0.00428606, acc 1, learning_rate 0.000443599
2018-04-02T19:45:23.520947: step 1599, loss 0.00353337, acc 1, learning_rate 0.000443028
2018-04-02T19:45:24.601599: step 1600, loss 0.00291188, acc 1, learning_rate 0.000442458

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:45:24.792109: step 1600, loss 0.0455686, acc 0.982695

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1600

2018-04-02T19:45:29.545003: step 1601, loss 0.00681298, acc 1, learning_rate 0.000441888
2018-04-02T19:45:30.617097: step 1602, loss 0.000750054, acc 1, learning_rate 0.00044132
2018-04-02T19:45:31.689306: step 1603, loss 0.00287683, acc 1, learning_rate 0.000440752
2018-04-02T19:45:32.762554: step 1604, loss 0.00263663, acc 1, learning_rate 0.000440186
2018-04-02T19:45:33.831242: step 1605, loss 0.00376028, acc 1, learning_rate 0.00043962
2018-04-02T19:45:34.904942: step 1606, loss 0.00239491, acc 1, learning_rate 0.000439055
2018-04-02T19:45:35.984956: step 1607, loss 0.00204944, acc 1, learning_rate 0.000438492
2018-04-02T19:45:37.063682: step 1608, loss 0.00183046, acc 1, learning_rate 0.000437929
2018-04-02T19:45:38.144381: step 1609, loss 0.00240775, acc 1, learning_rate 0.000437367
2018-04-02T19:45:39.217489: step 1610, loss 0.000792336, acc 1, learning_rate 0.000436806
2018-04-02T19:45:40.294070: step 1611, loss 0.0647529, acc 0.984375, learning_rate 0.000436246
2018-04-02T19:45:41.369082: step 1612, loss 0.0111418, acc 1, learning_rate 0.000435687
2018-04-02T19:45:42.449470: step 1613, loss 0.00340552, acc 1, learning_rate 0.000435129
2018-04-02T19:45:43.531228: step 1614, loss 0.00147628, acc 1, learning_rate 0.000434572
2018-04-02T19:45:44.616061: step 1615, loss 0.00215445, acc 1, learning_rate 0.000434015
2018-04-02T19:45:45.686610: step 1616, loss 0.00342335, acc 1, learning_rate 0.00043346
2018-04-02T19:45:46.766988: step 1617, loss 0.00453323, acc 1, learning_rate 0.000432906
2018-04-02T19:45:47.844307: step 1618, loss 0.000974461, acc 1, learning_rate 0.000432352
2018-04-02T19:45:48.921986: step 1619, loss 0.00203804, acc 1, learning_rate 0.000431799
2018-04-02T19:45:49.992623: step 1620, loss 0.000567894, acc 1, learning_rate 0.000431248
2018-04-02T19:45:51.060273: step 1621, loss 0.00104781, acc 1, learning_rate 0.000430697
2018-04-02T19:45:52.126604: step 1622, loss 0.00218705, acc 1, learning_rate 0.000430147
2018-04-02T19:45:53.195111: step 1623, loss 0.000972396, acc 1, learning_rate 0.000429598
2018-04-02T19:45:54.261489: step 1624, loss 0.00111632, acc 1, learning_rate 0.00042905
2018-04-02T19:45:55.331518: step 1625, loss 0.00290553, acc 1, learning_rate 0.000428503
2018-04-02T19:45:56.409672: step 1626, loss 0.00688329, acc 1, learning_rate 0.000427957
2018-04-02T19:45:57.478239: step 1627, loss 0.00521466, acc 1, learning_rate 0.000427412
2018-04-02T19:45:58.556436: step 1628, loss 0.000770213, acc 1, learning_rate 0.000426867
2018-04-02T19:45:59.631334: step 1629, loss 0.00176541, acc 1, learning_rate 0.000426324
2018-04-02T19:46:00.709278: step 1630, loss 0.00284761, acc 1, learning_rate 0.000425781
2018-04-02T19:46:01.779890: step 1631, loss 0.00240238, acc 1, learning_rate 0.00042524
2018-04-02T19:46:02.860288: step 1632, loss 0.00178223, acc 1, learning_rate 0.000424699
2018-04-02T19:46:03.936189: step 1633, loss 0.00441278, acc 1, learning_rate 0.000424159
2018-04-02T19:46:05.012654: step 1634, loss 0.00296379, acc 1, learning_rate 0.00042362
2018-04-02T19:46:06.082903: step 1635, loss 0.0013772, acc 1, learning_rate 0.000423082
2018-04-02T19:46:07.157240: step 1636, loss 0.00302878, acc 1, learning_rate 0.000422545
2018-04-02T19:46:08.234084: step 1637, loss 0.0895672, acc 0.984375, learning_rate 0.000422008
2018-04-02T19:46:09.303386: step 1638, loss 0.00342669, acc 1, learning_rate 0.000421473
2018-04-02T19:46:10.372479: step 1639, loss 0.000918582, acc 1, learning_rate 0.000420938
2018-04-02T19:46:11.443148: step 1640, loss 0.000792296, acc 1, learning_rate 0.000420405
2018-04-02T19:46:12.518479: step 1641, loss 0.000625348, acc 1, learning_rate 0.000419872
2018-04-02T19:46:13.585116: step 1642, loss 0.00200925, acc 1, learning_rate 0.00041934
2018-04-02T19:46:14.652993: step 1643, loss 0.00181512, acc 1, learning_rate 0.000418809
2018-04-02T19:46:15.729976: step 1644, loss 0.00162291, acc 1, learning_rate 0.000418279
2018-04-02T19:46:16.800832: step 1645, loss 0.00138478, acc 1, learning_rate 0.00041775
2018-04-02T19:46:17.868711: step 1646, loss 0.000950391, acc 1, learning_rate 0.000417222
2018-04-02T19:46:18.952327: step 1647, loss 0.00131991, acc 1, learning_rate 0.000416694
2018-04-02T19:46:20.021403: step 1648, loss 0.000816936, acc 1, learning_rate 0.000416168
2018-04-02T19:46:21.093220: step 1649, loss 0.00200087, acc 1, learning_rate 0.000415642
2018-04-02T19:46:22.163103: step 1650, loss 0.00290279, acc 1, learning_rate 0.000415117
2018-04-02T19:46:23.231965: step 1651, loss 0.0156808, acc 1, learning_rate 0.000414593
2018-04-02T19:46:24.306498: step 1652, loss 0.00237082, acc 1, learning_rate 0.00041407
2018-04-02T19:46:25.376679: step 1653, loss 0.00255649, acc 1, learning_rate 0.000413548
2018-04-02T19:46:26.446688: step 1654, loss 0.00118819, acc 1, learning_rate 0.000413027
2018-04-02T19:46:27.519214: step 1655, loss 0.0021468, acc 1, learning_rate 0.000412506
2018-04-02T19:46:28.585481: step 1656, loss 0.00190897, acc 1, learning_rate 0.000411987
2018-04-02T19:46:29.655389: step 1657, loss 0.00127716, acc 1, learning_rate 0.000411468
2018-04-02T19:46:30.727343: step 1658, loss 0.00144732, acc 1, learning_rate 0.00041095
2018-04-02T19:46:31.798089: step 1659, loss 0.000223473, acc 1, learning_rate 0.000410433
2018-04-02T19:46:32.865981: step 1660, loss 0.0875064, acc 0.984375, learning_rate 0.000409917
2018-04-02T19:46:33.937370: step 1661, loss 0.00234816, acc 1, learning_rate 0.000409402
2018-04-02T19:46:35.007704: step 1662, loss 0.00185093, acc 1, learning_rate 0.000408887
2018-04-02T19:46:36.078717: step 1663, loss 0.00066804, acc 1, learning_rate 0.000408374
2018-04-02T19:46:37.149123: step 1664, loss 0.00078935, acc 1, learning_rate 0.000407861
2018-04-02T19:46:38.221725: step 1665, loss 0.00126266, acc 1, learning_rate 0.000407349
2018-04-02T19:46:39.291783: step 1666, loss 0.00252733, acc 1, learning_rate 0.000406838
2018-04-02T19:46:40.363161: step 1667, loss 0.00187744, acc 1, learning_rate 0.000406328
2018-04-02T19:46:41.431614: step 1668, loss 0.0925004, acc 0.984375, learning_rate 0.000405818
2018-04-02T19:46:42.504507: step 1669, loss 0.00256058, acc 1, learning_rate 0.00040531
2018-04-02T19:46:43.573787: step 1670, loss 0.00417404, acc 1, learning_rate 0.000404802
2018-04-02T19:46:44.645978: step 1671, loss 0.001039, acc 1, learning_rate 0.000404296
2018-04-02T19:46:45.719204: step 1672, loss 0.00199681, acc 1, learning_rate 0.00040379
2018-04-02T19:46:46.791656: step 1673, loss 0.00486942, acc 1, learning_rate 0.000403284
2018-04-02T19:46:47.863529: step 1674, loss 0.00186092, acc 1, learning_rate 0.00040278
2018-04-02T19:46:48.931087: step 1675, loss 0.00276247, acc 1, learning_rate 0.000402277
2018-04-02T19:46:49.999787: step 1676, loss 0.00413278, acc 1, learning_rate 0.000401774
2018-04-02T19:46:51.069180: step 1677, loss 0.00122409, acc 1, learning_rate 0.000401272
2018-04-02T19:46:52.141985: step 1678, loss 0.00155164, acc 1, learning_rate 0.000400772
2018-04-02T19:46:53.211159: step 1679, loss 0.00285008, acc 1, learning_rate 0.000400271
2018-04-02T19:46:54.285608: step 1680, loss 0.000485125, acc 1, learning_rate 0.000399772
2018-04-02T19:46:55.355626: step 1681, loss 0.00121631, acc 1, learning_rate 0.000399274
2018-04-02T19:46:56.426238: step 1682, loss 0.00230793, acc 1, learning_rate 0.000398776
2018-04-02T19:46:57.498705: step 1683, loss 0.00454021, acc 1, learning_rate 0.000398279
2018-04-02T19:46:58.576382: step 1684, loss 0.003366, acc 1, learning_rate 0.000397783
2018-04-02T19:46:59.656417: step 1685, loss 0.0350609, acc 0.984375, learning_rate 0.000397288
2018-04-02T19:47:00.736930: step 1686, loss 0.00121544, acc 1, learning_rate 0.000396794
2018-04-02T19:47:01.764286: step 1687, loss 0.00609132, acc 1, learning_rate 0.000396301
2018-04-02T19:47:02.851087: step 1688, loss 0.007565, acc 1, learning_rate 0.000395808
2018-04-02T19:47:03.923680: step 1689, loss 0.002101, acc 1, learning_rate 0.000395316
2018-04-02T19:47:04.995304: step 1690, loss 0.00446299, acc 1, learning_rate 0.000394825
2018-04-02T19:47:06.080395: step 1691, loss 0.0218896, acc 0.984375, learning_rate 0.000394335
2018-04-02T19:47:07.159969: step 1692, loss 0.0148843, acc 1, learning_rate 0.000393845
2018-04-02T19:47:08.239772: step 1693, loss 0.00173599, acc 1, learning_rate 0.000393357
2018-04-02T19:47:09.316826: step 1694, loss 0.00173948, acc 1, learning_rate 0.000392869
2018-04-02T19:47:10.388886: step 1695, loss 0.0050701, acc 1, learning_rate 0.000392382
2018-04-02T19:47:11.458784: step 1696, loss 0.000956278, acc 1, learning_rate 0.000391896
2018-04-02T19:47:12.534713: step 1697, loss 0.00253796, acc 1, learning_rate 0.000391411
2018-04-02T19:47:13.607392: step 1698, loss 0.00201555, acc 1, learning_rate 0.000390926
2018-04-02T19:47:14.684591: step 1699, loss 0.00144031, acc 1, learning_rate 0.000390442
2018-04-02T19:47:15.754754: step 1700, loss 0.00186819, acc 1, learning_rate 0.00038996

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:47:15.943817: step 1700, loss 0.052364, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1700

2018-04-02T19:47:20.090037: step 1701, loss 0.00130132, acc 1, learning_rate 0.000389477
2018-04-02T19:47:21.171696: step 1702, loss 0.00238358, acc 1, learning_rate 0.000388996
2018-04-02T19:47:22.243790: step 1703, loss 0.00208223, acc 1, learning_rate 0.000388516
2018-04-02T19:47:23.315154: step 1704, loss 0.00236792, acc 1, learning_rate 0.000388036
2018-04-02T19:47:24.382332: step 1705, loss 0.00232094, acc 1, learning_rate 0.000387557
2018-04-02T19:47:25.452697: step 1706, loss 0.00115064, acc 1, learning_rate 0.000387079
2018-04-02T19:47:26.533364: step 1707, loss 0.00593394, acc 1, learning_rate 0.000386602
2018-04-02T19:47:27.610281: step 1708, loss 0.000529315, acc 1, learning_rate 0.000386125
2018-04-02T19:47:28.678002: step 1709, loss 0.00291928, acc 1, learning_rate 0.000385649
2018-04-02T19:47:29.748701: step 1710, loss 0.00375304, acc 1, learning_rate 0.000385174
2018-04-02T19:47:30.816779: step 1711, loss 0.00242224, acc 1, learning_rate 0.0003847
2018-04-02T19:47:31.899416: step 1712, loss 0.00144864, acc 1, learning_rate 0.000384227
2018-04-02T19:47:32.968235: step 1713, loss 0.00308512, acc 1, learning_rate 0.000383754
2018-04-02T19:47:34.042990: step 1714, loss 0.00162803, acc 1, learning_rate 0.000383282
2018-04-02T19:47:35.118748: step 1715, loss 0.00344556, acc 1, learning_rate 0.000382811
2018-04-02T19:47:36.200803: step 1716, loss 0.000722097, acc 1, learning_rate 0.000382341
2018-04-02T19:47:37.271786: step 1717, loss 0.00537531, acc 1, learning_rate 0.000381872
2018-04-02T19:47:38.352278: step 1718, loss 0.00122607, acc 1, learning_rate 0.000381403
2018-04-02T19:47:39.432869: step 1719, loss 0.0012733, acc 1, learning_rate 0.000380935
2018-04-02T19:47:40.513449: step 1720, loss 0.00215755, acc 1, learning_rate 0.000380468
2018-04-02T19:47:41.585203: step 1721, loss 0.000559522, acc 1, learning_rate 0.000380002
2018-04-02T19:47:42.665486: step 1722, loss 0.00205587, acc 1, learning_rate 0.000379536
2018-04-02T19:47:43.737854: step 1723, loss 0.00234369, acc 1, learning_rate 0.000379071
2018-04-02T19:47:44.806674: step 1724, loss 0.00239197, acc 1, learning_rate 0.000378607
2018-04-02T19:47:45.888501: step 1725, loss 0.000981524, acc 1, learning_rate 0.000378144
2018-04-02T19:47:46.967174: step 1726, loss 0.000861672, acc 1, learning_rate 0.000377682
2018-04-02T19:47:48.046635: step 1727, loss 0.069025, acc 0.984375, learning_rate 0.00037722
2018-04-02T19:47:49.130265: step 1728, loss 0.000968066, acc 1, learning_rate 0.000376759
2018-04-02T19:47:50.202039: step 1729, loss 0.00978606, acc 1, learning_rate 0.000376299
2018-04-02T19:47:51.281702: step 1730, loss 0.00188586, acc 1, learning_rate 0.00037584
2018-04-02T19:47:52.361433: step 1731, loss 0.00244801, acc 1, learning_rate 0.000375381
2018-04-02T19:47:53.441518: step 1732, loss 0.00174038, acc 1, learning_rate 0.000374923
2018-04-02T19:47:54.518977: step 1733, loss 0.00177251, acc 1, learning_rate 0.000374466
2018-04-02T19:47:55.598954: step 1734, loss 0.00167402, acc 1, learning_rate 0.00037401
2018-04-02T19:47:56.678472: step 1735, loss 0.00128619, acc 1, learning_rate 0.000373554
2018-04-02T19:47:57.750292: step 1736, loss 0.0836094, acc 0.984375, learning_rate 0.000373099
2018-04-02T19:47:58.827764: step 1737, loss 0.00105829, acc 1, learning_rate 0.000372645
2018-04-02T19:47:59.909266: step 1738, loss 0.00391726, acc 1, learning_rate 0.000372192
2018-04-02T19:48:00.990320: step 1739, loss 0.000697966, acc 1, learning_rate 0.000371739
2018-04-02T19:48:02.057487: step 1740, loss 0.00309666, acc 1, learning_rate 0.000371287
2018-04-02T19:48:03.129610: step 1741, loss 0.0591387, acc 0.984375, learning_rate 0.000370836
2018-04-02T19:48:04.212520: step 1742, loss 0.0041759, acc 1, learning_rate 0.000370386
2018-04-02T19:48:05.284898: step 1743, loss 0.000846413, acc 1, learning_rate 0.000369936
2018-04-02T19:48:06.357556: step 1744, loss 0.000792402, acc 1, learning_rate 0.000369488
2018-04-02T19:48:07.434730: step 1745, loss 0.000377572, acc 1, learning_rate 0.00036904
2018-04-02T19:48:08.512277: step 1746, loss 0.00143665, acc 1, learning_rate 0.000368592
2018-04-02T19:48:09.583160: step 1747, loss 0.00331704, acc 1, learning_rate 0.000368146
2018-04-02T19:48:10.659833: step 1748, loss 0.00154984, acc 1, learning_rate 0.0003677
2018-04-02T19:48:11.738257: step 1749, loss 0.0024038, acc 1, learning_rate 0.000367255
2018-04-02T19:48:12.808798: step 1750, loss 0.00210002, acc 1, learning_rate 0.00036681
2018-04-02T19:48:13.889083: step 1751, loss 0.000743505, acc 1, learning_rate 0.000366367
2018-04-02T19:48:14.957520: step 1752, loss 0.0026764, acc 1, learning_rate 0.000365924
2018-04-02T19:48:16.026538: step 1753, loss 0.00289993, acc 1, learning_rate 0.000365482
2018-04-02T19:48:17.095235: step 1754, loss 0.00544467, acc 1, learning_rate 0.00036504
2018-04-02T19:48:18.161968: step 1755, loss 0.00128061, acc 1, learning_rate 0.0003646
2018-04-02T19:48:19.240341: step 1756, loss 0.00151206, acc 1, learning_rate 0.00036416
2018-04-02T19:48:20.316462: step 1757, loss 0.00650218, acc 1, learning_rate 0.00036372
2018-04-02T19:48:21.395417: step 1758, loss 0.0614414, acc 0.984375, learning_rate 0.000363282
2018-04-02T19:48:22.464377: step 1759, loss 0.00243878, acc 1, learning_rate 0.000362844
2018-04-02T19:48:23.542867: step 1760, loss 0.00383246, acc 1, learning_rate 0.000362407
2018-04-02T19:48:24.617296: step 1761, loss 0.00306383, acc 1, learning_rate 0.000361971
2018-04-02T19:48:25.693938: step 1762, loss 0.00313243, acc 1, learning_rate 0.000361535
2018-04-02T19:48:26.761115: step 1763, loss 0.00137383, acc 1, learning_rate 0.0003611
2018-04-02T19:48:27.843441: step 1764, loss 0.00609757, acc 1, learning_rate 0.000360666
2018-04-02T19:48:28.911920: step 1765, loss 0.000755025, acc 1, learning_rate 0.000360233
2018-04-02T19:48:29.991009: step 1766, loss 0.00141289, acc 1, learning_rate 0.0003598
2018-04-02T19:48:31.065410: step 1767, loss 0.00188172, acc 1, learning_rate 0.000359368
2018-04-02T19:48:32.146771: step 1768, loss 0.077883, acc 0.984375, learning_rate 0.000358937
2018-04-02T19:48:33.218927: step 1769, loss 0.001694, acc 1, learning_rate 0.000358506
2018-04-02T19:48:34.289794: step 1770, loss 0.000842591, acc 1, learning_rate 0.000358077
2018-04-02T19:48:35.370337: step 1771, loss 0.0584051, acc 0.984375, learning_rate 0.000357648
2018-04-02T19:48:36.442343: step 1772, loss 0.000856235, acc 1, learning_rate 0.000357219
2018-04-02T19:48:37.512931: step 1773, loss 0.00159791, acc 1, learning_rate 0.000356792
2018-04-02T19:48:38.594032: step 1774, loss 0.00329547, acc 1, learning_rate 0.000356365
2018-04-02T19:48:39.677521: step 1775, loss 0.00194836, acc 1, learning_rate 0.000355938
2018-04-02T19:48:40.758354: step 1776, loss 0.00204243, acc 1, learning_rate 0.000355513
2018-04-02T19:48:41.829773: step 1777, loss 0.00589388, acc 1, learning_rate 0.000355088
2018-04-02T19:48:42.900918: step 1778, loss 0.0095177, acc 1, learning_rate 0.000354664
2018-04-02T19:48:43.981405: step 1779, loss 0.00214734, acc 1, learning_rate 0.00035424
2018-04-02T19:48:45.054730: step 1780, loss 0.00383766, acc 1, learning_rate 0.000353818
2018-04-02T19:48:46.126434: step 1781, loss 0.0045115, acc 1, learning_rate 0.000353396
2018-04-02T19:48:47.208182: step 1782, loss 0.00305907, acc 1, learning_rate 0.000352974
2018-04-02T19:48:48.288272: step 1783, loss 0.00110302, acc 1, learning_rate 0.000352554
2018-04-02T19:48:49.366320: step 1784, loss 0.0856363, acc 0.984375, learning_rate 0.000352134
2018-04-02T19:48:50.440221: step 1785, loss 0.000507889, acc 1, learning_rate 0.000351715
2018-04-02T19:48:51.510715: step 1786, loss 0.00287719, acc 1, learning_rate 0.000351296
2018-04-02T19:48:52.593798: step 1787, loss 0.0020051, acc 1, learning_rate 0.000350878
2018-04-02T19:48:53.664365: step 1788, loss 0.00221202, acc 1, learning_rate 0.000350461
2018-04-02T19:48:54.734347: step 1789, loss 0.0047684, acc 1, learning_rate 0.000350045
2018-04-02T19:48:55.805930: step 1790, loss 0.00451877, acc 1, learning_rate 0.000349629
2018-04-02T19:48:56.877917: step 1791, loss 0.0010843, acc 1, learning_rate 0.000349214
2018-04-02T19:48:57.947577: step 1792, loss 0.00161697, acc 1, learning_rate 0.000348799
2018-04-02T19:48:59.018455: step 1793, loss 0.00426042, acc 1, learning_rate 0.000348386
2018-04-02T19:49:00.090205: step 1794, loss 0.00220269, acc 1, learning_rate 0.000347973
2018-04-02T19:49:01.161425: step 1795, loss 0.0757952, acc 0.984375, learning_rate 0.000347561
2018-04-02T19:49:02.246259: step 1796, loss 0.00941739, acc 1, learning_rate 0.000347149
2018-04-02T19:49:03.319991: step 1797, loss 0.0538045, acc 0.984375, learning_rate 0.000346738
2018-04-02T19:49:04.401936: step 1798, loss 0.00243025, acc 1, learning_rate 0.000346328
2018-04-02T19:49:05.480953: step 1799, loss 0.000543016, acc 1, learning_rate 0.000345918
2018-04-02T19:49:06.549922: step 1800, loss 0.0034421, acc 1, learning_rate 0.000345509

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:49:06.739116: step 1800, loss 0.0489725, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1800

2018-04-02T19:49:10.609153: step 1801, loss 0.00191641, acc 1, learning_rate 0.000345101
2018-04-02T19:49:11.679134: step 1802, loss 0.00430856, acc 1, learning_rate 0.000344694
2018-04-02T19:49:12.750986: step 1803, loss 0.00342456, acc 1, learning_rate 0.000344287
2018-04-02T19:49:13.829602: step 1804, loss 0.00359455, acc 1, learning_rate 0.000343881
2018-04-02T19:49:14.905393: step 1805, loss 0.00428198, acc 1, learning_rate 0.000343475
2018-04-02T19:49:15.980846: step 1806, loss 0.00291376, acc 1, learning_rate 0.00034307
2018-04-02T19:49:17.049798: step 1807, loss 0.000807173, acc 1, learning_rate 0.000342666
2018-04-02T19:49:18.119556: step 1808, loss 0.00347047, acc 1, learning_rate 0.000342263
2018-04-02T19:49:19.188688: step 1809, loss 0.00268444, acc 1, learning_rate 0.00034186
2018-04-02T19:49:20.259990: step 1810, loss 0.0019923, acc 1, learning_rate 0.000341458
2018-04-02T19:49:21.330905: step 1811, loss 0.00295005, acc 1, learning_rate 0.000341056
2018-04-02T19:49:22.400063: step 1812, loss 0.00142662, acc 1, learning_rate 0.000340655
2018-04-02T19:49:23.471589: step 1813, loss 0.0073832, acc 1, learning_rate 0.000340255
2018-04-02T19:49:24.542366: step 1814, loss 0.0026381, acc 1, learning_rate 0.000339856
2018-04-02T19:49:25.616450: step 1815, loss 0.00282169, acc 1, learning_rate 0.000339457
2018-04-02T19:49:26.688221: step 1816, loss 0.0674887, acc 0.984375, learning_rate 0.000339059
2018-04-02T19:49:27.770057: step 1817, loss 0.00260885, acc 1, learning_rate 0.000338661
2018-04-02T19:49:28.843589: step 1818, loss 0.00113326, acc 1, learning_rate 0.000338265
2018-04-02T19:49:29.923243: step 1819, loss 0.00245292, acc 1, learning_rate 0.000337868
2018-04-02T19:49:30.993103: step 1820, loss 0.0025797, acc 1, learning_rate 0.000337473
2018-04-02T19:49:32.071428: step 1821, loss 0.000701286, acc 1, learning_rate 0.000337078
2018-04-02T19:49:33.139853: step 1822, loss 0.00818237, acc 1, learning_rate 0.000336684
2018-04-02T19:49:34.220763: step 1823, loss 0.00128575, acc 1, learning_rate 0.00033629
2018-04-02T19:49:35.289908: step 1824, loss 0.00265552, acc 1, learning_rate 0.000335897
2018-04-02T19:49:36.370715: step 1825, loss 0.00144404, acc 1, learning_rate 0.000335505
2018-04-02T19:49:37.441865: step 1826, loss 0.00164604, acc 1, learning_rate 0.000335114
2018-04-02T19:49:38.513039: step 1827, loss 0.000874383, acc 1, learning_rate 0.000334723
2018-04-02T19:49:39.582293: step 1828, loss 0.000523422, acc 1, learning_rate 0.000334332
2018-04-02T19:49:40.658548: step 1829, loss 0.0553161, acc 0.984375, learning_rate 0.000333943
2018-04-02T19:49:41.729738: step 1830, loss 0.00385366, acc 1, learning_rate 0.000333554
2018-04-02T19:49:42.809492: step 1831, loss 0.00347409, acc 1, learning_rate 0.000333166
2018-04-02T19:49:43.888987: step 1832, loss 0.00412543, acc 1, learning_rate 0.000332778
2018-04-02T19:49:44.970776: step 1833, loss 0.00324579, acc 1, learning_rate 0.000332391
2018-04-02T19:49:46.042714: step 1834, loss 0.00222711, acc 1, learning_rate 0.000332004
2018-04-02T19:49:47.118514: step 1835, loss 0.00373759, acc 1, learning_rate 0.000331619
2018-04-02T19:49:48.196894: step 1836, loss 0.00509649, acc 1, learning_rate 0.000331234
2018-04-02T19:49:49.271848: step 1837, loss 0.00345507, acc 1, learning_rate 0.000330849
2018-04-02T19:49:50.347642: step 1838, loss 0.00098555, acc 1, learning_rate 0.000330465
2018-04-02T19:49:51.417563: step 1839, loss 0.0137871, acc 1, learning_rate 0.000330082
2018-04-02T19:49:52.498216: step 1840, loss 0.00184278, acc 1, learning_rate 0.0003297
2018-04-02T19:49:53.570326: step 1841, loss 0.00239723, acc 1, learning_rate 0.000329318
2018-04-02T19:49:54.642032: step 1842, loss 0.0852841, acc 0.984375, learning_rate 0.000328936
2018-04-02T19:49:55.718711: step 1843, loss 0.00250534, acc 1, learning_rate 0.000328556
2018-04-02T19:49:56.787212: step 1844, loss 0.00212529, acc 1, learning_rate 0.000328176
2018-04-02T19:49:57.856780: step 1845, loss 0.00297191, acc 1, learning_rate 0.000327796
2018-04-02T19:49:58.924266: step 1846, loss 0.00744516, acc 1, learning_rate 0.000327418
2018-04-02T19:50:00.004273: step 1847, loss 0.00320453, acc 1, learning_rate 0.000327039
2018-04-02T19:50:01.084172: step 1848, loss 0.00176757, acc 1, learning_rate 0.000326662
2018-04-02T19:50:02.155132: step 1849, loss 0.0154987, acc 0.984375, learning_rate 0.000326285
2018-04-02T19:50:03.234293: step 1850, loss 0.00173379, acc 1, learning_rate 0.000325909
2018-04-02T19:50:04.316010: step 1851, loss 0.0130153, acc 1, learning_rate 0.000325533
2018-04-02T19:50:05.410440: step 1852, loss 0.00296436, acc 1, learning_rate 0.000325158
2018-04-02T19:50:06.480701: step 1853, loss 0.00269714, acc 1, learning_rate 0.000324784
2018-04-02T19:50:07.562156: step 1854, loss 0.0019973, acc 1, learning_rate 0.00032441
2018-04-02T19:50:08.640122: step 1855, loss 0.000649546, acc 1, learning_rate 0.000324037
2018-04-02T19:50:09.708914: step 1856, loss 0.00235318, acc 1, learning_rate 0.000323664
2018-04-02T19:50:10.787006: step 1857, loss 0.00237837, acc 1, learning_rate 0.000323293
2018-04-02T19:50:11.865884: step 1858, loss 0.00608105, acc 1, learning_rate 0.000322921
2018-04-02T19:50:12.934126: step 1859, loss 0.00296859, acc 1, learning_rate 0.000322551
2018-04-02T19:50:14.014194: step 1860, loss 0.0623916, acc 0.984375, learning_rate 0.000322181
2018-04-02T19:50:15.086899: step 1861, loss 0.000479701, acc 1, learning_rate 0.000321811
2018-04-02T19:50:16.158094: step 1862, loss 0.00065533, acc 1, learning_rate 0.000321442
2018-04-02T19:50:17.235264: step 1863, loss 0.00172157, acc 1, learning_rate 0.000321074
2018-04-02T19:50:18.312376: step 1864, loss 0.00501672, acc 1, learning_rate 0.000320707
2018-04-02T19:50:19.381969: step 1865, loss 0.00334336, acc 1, learning_rate 0.00032034
2018-04-02T19:50:20.452189: step 1866, loss 0.000992069, acc 1, learning_rate 0.000319973
2018-04-02T19:50:21.518028: step 1867, loss 0.00282329, acc 1, learning_rate 0.000319608
2018-04-02T19:50:22.584255: step 1868, loss 0.00534699, acc 1, learning_rate 0.000319242
2018-04-02T19:50:23.662327: step 1869, loss 0.00134121, acc 1, learning_rate 0.000318878
2018-04-02T19:50:24.730512: step 1870, loss 0.00125635, acc 1, learning_rate 0.000318514
2018-04-02T19:50:25.797828: step 1871, loss 0.00212715, acc 1, learning_rate 0.000318151
2018-04-02T19:50:26.876339: step 1872, loss 0.00169314, acc 1, learning_rate 0.000317788
2018-04-02T19:50:27.957549: step 1873, loss 0.00125059, acc 1, learning_rate 0.000317426
2018-04-02T19:50:29.027686: step 1874, loss 0.0327637, acc 0.984375, learning_rate 0.000317064
2018-04-02T19:50:30.107202: step 1875, loss 0.00155174, acc 1, learning_rate 0.000316703
2018-04-02T19:50:31.187222: step 1876, loss 0.00232887, acc 1, learning_rate 0.000316343
2018-04-02T19:50:32.257603: step 1877, loss 0.000860418, acc 1, learning_rate 0.000315983
2018-04-02T19:50:33.341845: step 1878, loss 0.0021474, acc 1, learning_rate 0.000315624
2018-04-02T19:50:34.419687: step 1879, loss 0.000914354, acc 1, learning_rate 0.000315266
2018-04-02T19:50:35.493003: step 1880, loss 0.00260846, acc 1, learning_rate 0.000314908
2018-04-02T19:50:36.564397: step 1881, loss 0.00405651, acc 1, learning_rate 0.000314551
2018-04-02T19:50:37.634824: step 1882, loss 0.00500043, acc 1, learning_rate 0.000314194
2018-04-02T19:50:38.707687: step 1883, loss 0.00204335, acc 1, learning_rate 0.000313838
2018-04-02T19:50:39.786156: step 1884, loss 0.00242365, acc 1, learning_rate 0.000313482
2018-04-02T19:50:40.854953: step 1885, loss 0.00351411, acc 1, learning_rate 0.000313127
2018-04-02T19:50:41.926959: step 1886, loss 0.00211328, acc 1, learning_rate 0.000312773
2018-04-02T19:50:42.994711: step 1887, loss 0.000445377, acc 1, learning_rate 0.000312419
2018-04-02T19:50:44.063777: step 1888, loss 0.00189868, acc 1, learning_rate 0.000312066
2018-04-02T19:50:45.140471: step 1889, loss 0.00215796, acc 1, learning_rate 0.000311713
2018-04-02T19:50:46.221160: step 1890, loss 0.00142732, acc 1, learning_rate 0.000311361
2018-04-02T19:50:47.304972: step 1891, loss 0.000465458, acc 1, learning_rate 0.00031101
2018-04-02T19:50:48.373959: step 1892, loss 0.00281967, acc 1, learning_rate 0.000310659
2018-04-02T19:50:49.446436: step 1893, loss 0.00206233, acc 1, learning_rate 0.000310309
2018-04-02T19:50:50.518207: step 1894, loss 0.000994325, acc 1, learning_rate 0.000309959
2018-04-02T19:50:51.589907: step 1895, loss 0.000988696, acc 1, learning_rate 0.00030961
2018-04-02T19:50:52.671529: step 1896, loss 0.00288368, acc 1, learning_rate 0.000309261
2018-04-02T19:50:53.751965: step 1897, loss 0.0477947, acc 0.984375, learning_rate 0.000308913
2018-04-02T19:50:54.821405: step 1898, loss 0.0809973, acc 0.984375, learning_rate 0.000308566
2018-04-02T19:50:55.895355: step 1899, loss 0.00331153, acc 1, learning_rate 0.000308219
2018-04-02T19:50:56.974944: step 1900, loss 0.0642435, acc 0.984375, learning_rate 0.000307873

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:50:57.166322: step 1900, loss 0.0457256, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-1900

2018-04-02T19:51:01.263599: step 1901, loss 0.000421679, acc 1, learning_rate 0.000307528
2018-04-02T19:51:02.330085: step 1902, loss 0.00166628, acc 1, learning_rate 0.000307182
2018-04-02T19:51:03.411974: step 1903, loss 0.00496506, acc 1, learning_rate 0.000306838
2018-04-02T19:51:04.484954: step 1904, loss 0.0033769, acc 1, learning_rate 0.000306494
2018-04-02T19:51:05.558202: step 1905, loss 0.0029546, acc 1, learning_rate 0.000306151
2018-04-02T19:51:06.634389: step 1906, loss 0.00237854, acc 1, learning_rate 0.000305808
2018-04-02T19:51:07.702028: step 1907, loss 0.00313442, acc 1, learning_rate 0.000305466
2018-04-02T19:51:08.768904: step 1908, loss 0.00130523, acc 1, learning_rate 0.000305124
2018-04-02T19:51:09.841622: step 1909, loss 0.00125501, acc 1, learning_rate 0.000304783
2018-04-02T19:51:10.910607: step 1910, loss 0.00127974, acc 1, learning_rate 0.000304443
2018-04-02T19:51:11.978430: step 1911, loss 0.0110725, acc 1, learning_rate 0.000304103
2018-04-02T19:51:13.047881: step 1912, loss 0.00115081, acc 1, learning_rate 0.000303763
2018-04-02T19:51:14.116244: step 1913, loss 0.00368148, acc 1, learning_rate 0.000303425
2018-04-02T19:51:15.192561: step 1914, loss 0.00232683, acc 1, learning_rate 0.000303086
2018-04-02T19:51:16.266477: step 1915, loss 0.00173711, acc 1, learning_rate 0.000302749
2018-04-02T19:51:17.334009: step 1916, loss 0.000709958, acc 1, learning_rate 0.000302412
2018-04-02T19:51:18.412188: step 1917, loss 0.00154192, acc 1, learning_rate 0.000302075
2018-04-02T19:51:19.488186: step 1918, loss 0.00166247, acc 1, learning_rate 0.000301739
2018-04-02T19:51:20.565747: step 1919, loss 0.00464731, acc 1, learning_rate 0.000301404
2018-04-02T19:51:21.647421: step 1920, loss 0.00160837, acc 1, learning_rate 0.000301069
2018-04-02T19:51:22.716577: step 1921, loss 0.00155831, acc 1, learning_rate 0.000300734
2018-04-02T19:51:23.788275: step 1922, loss 0.00153969, acc 1, learning_rate 0.000300401
2018-04-02T19:51:24.868815: step 1923, loss 0.00106223, acc 1, learning_rate 0.000300067
2018-04-02T19:51:25.949020: step 1924, loss 0.00391088, acc 1, learning_rate 0.000299735
2018-04-02T19:51:27.029943: step 1925, loss 0.00444923, acc 1, learning_rate 0.000299403
2018-04-02T19:51:28.099094: step 1926, loss 0.00248768, acc 1, learning_rate 0.000299071
2018-04-02T19:51:29.170977: step 1927, loss 0.00159398, acc 1, learning_rate 0.00029874
2018-04-02T19:51:30.199405: step 1928, loss 0.0002512, acc 1, learning_rate 0.00029841
2018-04-02T19:51:31.285643: step 1929, loss 0.00236423, acc 1, learning_rate 0.00029808
2018-04-02T19:51:32.357502: step 1930, loss 0.00291793, acc 1, learning_rate 0.00029775
2018-04-02T19:51:33.427717: step 1931, loss 0.00498497, acc 1, learning_rate 0.000297422
2018-04-02T19:51:34.504757: step 1932, loss 0.0803189, acc 0.96875, learning_rate 0.000297093
2018-04-02T19:51:35.582831: step 1933, loss 0.00151089, acc 1, learning_rate 0.000296766
2018-04-02T19:51:36.652330: step 1934, loss 0.00160217, acc 1, learning_rate 0.000296439
2018-04-02T19:51:37.733154: step 1935, loss 0.00157592, acc 1, learning_rate 0.000296112
2018-04-02T19:51:38.805121: step 1936, loss 0.00201197, acc 1, learning_rate 0.000295786
2018-04-02T19:51:39.879956: step 1937, loss 0.0026664, acc 1, learning_rate 0.00029546
2018-04-02T19:51:40.961486: step 1938, loss 0.00798903, acc 1, learning_rate 0.000295135
2018-04-02T19:51:42.037144: step 1939, loss 0.000680261, acc 1, learning_rate 0.000294811
2018-04-02T19:51:43.150539: step 1940, loss 0.0022876, acc 1, learning_rate 0.000294487
2018-04-02T19:51:44.223433: step 1941, loss 0.00218552, acc 1, learning_rate 0.000294164
2018-04-02T19:51:45.303309: step 1942, loss 0.0775561, acc 0.984375, learning_rate 0.000293841
2018-04-02T19:51:46.374579: step 1943, loss 0.0721921, acc 0.984375, learning_rate 0.000293519
2018-04-02T19:51:47.448037: step 1944, loss 0.00416342, acc 1, learning_rate 0.000293197
2018-04-02T19:51:48.516124: step 1945, loss 0.00317959, acc 1, learning_rate 0.000292876
2018-04-02T19:51:49.595131: step 1946, loss 0.00042942, acc 1, learning_rate 0.000292555
2018-04-02T19:51:50.671062: step 1947, loss 0.00256633, acc 1, learning_rate 0.000292235
2018-04-02T19:51:51.753954: step 1948, loss 0.0670301, acc 0.984375, learning_rate 0.000291915
2018-04-02T19:51:52.834676: step 1949, loss 0.00371071, acc 1, learning_rate 0.000291596
2018-04-02T19:51:53.914453: step 1950, loss 0.00105357, acc 1, learning_rate 0.000291277
2018-04-02T19:51:54.982964: step 1951, loss 0.00374931, acc 1, learning_rate 0.000290959
2018-04-02T19:51:56.056821: step 1952, loss 0.00113171, acc 1, learning_rate 0.000290642
2018-04-02T19:51:57.136067: step 1953, loss 0.0017819, acc 1, learning_rate 0.000290325
2018-04-02T19:51:58.214885: step 1954, loss 0.0019237, acc 1, learning_rate 0.000290008
2018-04-02T19:51:59.283597: step 1955, loss 0.00444734, acc 1, learning_rate 0.000289693
2018-04-02T19:52:00.363215: step 1956, loss 0.00193715, acc 1, learning_rate 0.000289377
2018-04-02T19:52:01.441934: step 1957, loss 0.00276183, acc 1, learning_rate 0.000289062
2018-04-02T19:52:02.513202: step 1958, loss 0.00270762, acc 1, learning_rate 0.000288748
2018-04-02T19:52:03.584935: step 1959, loss 0.00123527, acc 1, learning_rate 0.000288434
2018-04-02T19:52:04.656054: step 1960, loss 0.00415746, acc 1, learning_rate 0.000288121
2018-04-02T19:52:05.736391: step 1961, loss 0.00186732, acc 1, learning_rate 0.000287808
2018-04-02T19:52:06.806851: step 1962, loss 0.00424108, acc 1, learning_rate 0.000287496
2018-04-02T19:52:07.877824: step 1963, loss 0.0017986, acc 1, learning_rate 0.000287184
2018-04-02T19:52:08.945519: step 1964, loss 0.00164737, acc 1, learning_rate 0.000286873
2018-04-02T19:52:10.026234: step 1965, loss 0.00173563, acc 1, learning_rate 0.000286562
2018-04-02T19:52:11.101974: step 1966, loss 0.00232688, acc 1, learning_rate 0.000286252
2018-04-02T19:52:12.171217: step 1967, loss 0.00319073, acc 1, learning_rate 0.000285942
2018-04-02T19:52:13.245546: step 1968, loss 0.0038666, acc 1, learning_rate 0.000285633
2018-04-02T19:52:14.323724: step 1969, loss 0.00212323, acc 1, learning_rate 0.000285324
2018-04-02T19:52:15.398454: step 1970, loss 0.00190153, acc 1, learning_rate 0.000285016
2018-04-02T19:52:16.475277: step 1971, loss 0.00331179, acc 1, learning_rate 0.000284709
2018-04-02T19:52:17.554912: step 1972, loss 0.00107697, acc 1, learning_rate 0.000284401
2018-04-02T19:52:18.622041: step 1973, loss 0.00441489, acc 1, learning_rate 0.000284095
2018-04-02T19:52:19.693608: step 1974, loss 0.00108374, acc 1, learning_rate 0.000283789
2018-04-02T19:52:20.762110: step 1975, loss 0.0909191, acc 0.984375, learning_rate 0.000283483
2018-04-02T19:52:21.834550: step 1976, loss 0.00215808, acc 1, learning_rate 0.000283178
2018-04-02T19:52:22.904667: step 1977, loss 0.00166693, acc 1, learning_rate 0.000282874
2018-04-02T19:52:23.983558: step 1978, loss 0.00279776, acc 1, learning_rate 0.000282569
2018-04-02T19:52:25.062147: step 1979, loss 0.000955294, acc 1, learning_rate 0.000282266
2018-04-02T19:52:26.132538: step 1980, loss 0.00215966, acc 1, learning_rate 0.000281963
2018-04-02T19:52:27.205043: step 1981, loss 0.00561505, acc 1, learning_rate 0.00028166
2018-04-02T19:52:28.285054: step 1982, loss 0.00120359, acc 1, learning_rate 0.000281358
2018-04-02T19:52:29.356349: step 1983, loss 0.00233019, acc 1, learning_rate 0.000281057
2018-04-02T19:52:30.433579: step 1984, loss 0.0233751, acc 0.984375, learning_rate 0.000280756
2018-04-02T19:52:31.516038: step 1985, loss 0.00298111, acc 1, learning_rate 0.000280455
2018-04-02T19:52:32.587750: step 1986, loss 0.000943815, acc 1, learning_rate 0.000280155
2018-04-02T19:52:33.671004: step 1987, loss 0.00840382, acc 1, learning_rate 0.000279856
2018-04-02T19:52:34.744651: step 1988, loss 0.00218235, acc 1, learning_rate 0.000279557
2018-04-02T19:52:35.825929: step 1989, loss 0.00204712, acc 1, learning_rate 0.000279258
2018-04-02T19:52:36.893505: step 1990, loss 0.00327658, acc 1, learning_rate 0.00027896
2018-04-02T19:52:37.970064: step 1991, loss 0.00201717, acc 1, learning_rate 0.000278662
2018-04-02T19:52:39.038631: step 1992, loss 0.00280079, acc 1, learning_rate 0.000278365
2018-04-02T19:52:40.112925: step 1993, loss 0.00266096, acc 1, learning_rate 0.000278069
2018-04-02T19:52:41.191222: step 1994, loss 0.00293665, acc 1, learning_rate 0.000277773
2018-04-02T19:52:42.279365: step 1995, loss 0.00333215, acc 1, learning_rate 0.000277477
2018-04-02T19:52:43.357902: step 1996, loss 0.000789997, acc 1, learning_rate 0.000277182
2018-04-02T19:52:44.426173: step 1997, loss 0.00437701, acc 1, learning_rate 0.000276887
2018-04-02T19:52:45.494365: step 1998, loss 0.00526035, acc 1, learning_rate 0.000276593
2018-04-02T19:52:46.571602: step 1999, loss 0.00714854, acc 1, learning_rate 0.0002763
2018-04-02T19:52:47.648571: step 2000, loss 0.0526504, acc 0.984375, learning_rate 0.000276007

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:52:47.838593: step 2000, loss 0.052584, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-2000

2018-04-02T19:52:52.770763: step 2001, loss 0.000327518, acc 1, learning_rate 0.000275714
2018-04-02T19:52:53.840645: step 2002, loss 0.00151453, acc 1, learning_rate 0.000275422
2018-04-02T19:52:54.917991: step 2003, loss 0.00240896, acc 1, learning_rate 0.00027513
2018-04-02T19:52:55.987210: step 2004, loss 0.000898016, acc 1, learning_rate 0.000274839
2018-04-02T19:52:57.063622: step 2005, loss 0.00664068, acc 1, learning_rate 0.000274548
2018-04-02T19:52:58.137863: step 2006, loss 0.00402451, acc 1, learning_rate 0.000274258
2018-04-02T19:52:59.214031: step 2007, loss 0.00271071, acc 1, learning_rate 0.000273968
2018-04-02T19:53:00.284433: step 2008, loss 0.00321623, acc 1, learning_rate 0.000273679
2018-04-02T19:53:01.353547: step 2009, loss 0.00307962, acc 1, learning_rate 0.00027339
2018-04-02T19:53:02.431035: step 2010, loss 0.00504744, acc 1, learning_rate 0.000273102
2018-04-02T19:53:03.505493: step 2011, loss 0.00135162, acc 1, learning_rate 0.000272814
2018-04-02T19:53:04.583517: step 2012, loss 0.00304173, acc 1, learning_rate 0.000272527
2018-04-02T19:53:05.648517: step 2013, loss 0.00225516, acc 1, learning_rate 0.00027224
2018-04-02T19:53:06.715830: step 2014, loss 0.00150197, acc 1, learning_rate 0.000271954
2018-04-02T19:53:07.783509: step 2015, loss 0.00167985, acc 1, learning_rate 0.000271668
2018-04-02T19:53:08.862611: step 2016, loss 0.00108113, acc 1, learning_rate 0.000271382
2018-04-02T19:53:09.931163: step 2017, loss 0.00263899, acc 1, learning_rate 0.000271097
2018-04-02T19:53:11.000297: step 2018, loss 0.00199608, acc 1, learning_rate 0.000270813
2018-04-02T19:53:12.069988: step 2019, loss 0.00138489, acc 1, learning_rate 0.000270529
2018-04-02T19:53:13.140637: step 2020, loss 0.00379794, acc 1, learning_rate 0.000270245
2018-04-02T19:53:14.209890: step 2021, loss 0.000734443, acc 1, learning_rate 0.000269962
2018-04-02T19:53:15.282654: step 2022, loss 0.00237849, acc 1, learning_rate 0.00026968
2018-04-02T19:53:16.358110: step 2023, loss 0.00293722, acc 1, learning_rate 0.000269397
2018-04-02T19:53:17.427230: step 2024, loss 0.00552997, acc 1, learning_rate 0.000269116
2018-04-02T19:53:18.491507: step 2025, loss 0.00443327, acc 1, learning_rate 0.000268835
2018-04-02T19:53:19.565160: step 2026, loss 0.000226587, acc 1, learning_rate 0.000268554
2018-04-02T19:53:20.635140: step 2027, loss 0.00117029, acc 1, learning_rate 0.000268274
2018-04-02T19:53:21.704217: step 2028, loss 0.00191764, acc 1, learning_rate 0.000267994
2018-04-02T19:53:22.771606: step 2029, loss 0.00363268, acc 1, learning_rate 0.000267715
2018-04-02T19:53:23.839250: step 2030, loss 0.00280483, acc 1, learning_rate 0.000267436
2018-04-02T19:53:24.916570: step 2031, loss 0.000863389, acc 1, learning_rate 0.000267157
2018-04-02T19:53:25.986968: step 2032, loss 0.00202106, acc 1, learning_rate 0.000266879
2018-04-02T19:53:27.063422: step 2033, loss 0.00326441, acc 1, learning_rate 0.000266602
2018-04-02T19:53:28.138284: step 2034, loss 0.0053193, acc 1, learning_rate 0.000266325
2018-04-02T19:53:29.206102: step 2035, loss 0.0022507, acc 1, learning_rate 0.000266048
2018-04-02T19:53:30.273121: step 2036, loss 0.0013041, acc 1, learning_rate 0.000265772
2018-04-02T19:53:31.340514: step 2037, loss 0.00453579, acc 1, learning_rate 0.000265497
2018-04-02T19:53:32.410038: step 2038, loss 0.00109978, acc 1, learning_rate 0.000265221
2018-04-02T19:53:33.479441: step 2039, loss 0.0010503, acc 1, learning_rate 0.000264947
2018-04-02T19:53:34.558653: step 2040, loss 0.00313789, acc 1, learning_rate 0.000264673
2018-04-02T19:53:35.627935: step 2041, loss 0.00257352, acc 1, learning_rate 0.000264399
2018-04-02T19:53:36.703513: step 2042, loss 0.00143623, acc 1, learning_rate 0.000264125
2018-04-02T19:53:37.781500: step 2043, loss 0.00190574, acc 1, learning_rate 0.000263852
2018-04-02T19:53:38.848307: step 2044, loss 0.0929234, acc 0.984375, learning_rate 0.00026358
2018-04-02T19:53:39.917407: step 2045, loss 0.00154226, acc 1, learning_rate 0.000263308
2018-04-02T19:53:40.987511: step 2046, loss 0.00157147, acc 1, learning_rate 0.000263037
2018-04-02T19:53:42.060727: step 2047, loss 0.00473245, acc 1, learning_rate 0.000262765
2018-04-02T19:53:43.137972: step 2048, loss 0.00106038, acc 1, learning_rate 0.000262495
2018-04-02T19:53:44.205685: step 2049, loss 0.000953105, acc 1, learning_rate 0.000262225
2018-04-02T19:53:45.280107: step 2050, loss 0.00191515, acc 1, learning_rate 0.000261955
2018-04-02T19:53:46.358770: step 2051, loss 0.00148163, acc 1, learning_rate 0.000261686
2018-04-02T19:53:47.434546: step 2052, loss 0.00312984, acc 1, learning_rate 0.000261417
2018-04-02T19:53:48.506248: step 2053, loss 0.00093567, acc 1, learning_rate 0.000261148
2018-04-02T19:53:49.574225: step 2054, loss 0.00606582, acc 1, learning_rate 0.00026088
2018-04-02T19:53:50.649927: step 2055, loss 0.0378121, acc 0.984375, learning_rate 0.000260613
2018-04-02T19:53:51.725395: step 2056, loss 0.0013184, acc 1, learning_rate 0.000260346
2018-04-02T19:53:52.798217: step 2057, loss 0.00206488, acc 1, learning_rate 0.000260079
2018-04-02T19:53:53.903200: step 2058, loss 0.000627091, acc 1, learning_rate 0.000259813
2018-04-02T19:53:54.973625: step 2059, loss 0.0035925, acc 1, learning_rate 0.000259547
2018-04-02T19:53:56.041489: step 2060, loss 0.00256797, acc 1, learning_rate 0.000259282
2018-04-02T19:53:57.108418: step 2061, loss 0.00101687, acc 1, learning_rate 0.000259017
2018-04-02T19:53:58.174528: step 2062, loss 0.00288183, acc 1, learning_rate 0.000258753
2018-04-02T19:53:59.241718: step 2063, loss 0.0014219, acc 1, learning_rate 0.000258489
2018-04-02T19:54:00.309830: step 2064, loss 0.00113501, acc 1, learning_rate 0.000258225
2018-04-02T19:54:01.377551: step 2065, loss 0.00145275, acc 1, learning_rate 0.000257962
2018-04-02T19:54:02.446205: step 2066, loss 0.00124059, acc 1, learning_rate 0.0002577
2018-04-02T19:54:03.515056: step 2067, loss 0.0623672, acc 0.984375, learning_rate 0.000257438
2018-04-02T19:54:04.594151: step 2068, loss 0.00105867, acc 1, learning_rate 0.000257176
2018-04-02T19:54:05.673081: step 2069, loss 0.00346091, acc 1, learning_rate 0.000256914
2018-04-02T19:54:06.744398: step 2070, loss 0.00122734, acc 1, learning_rate 0.000256654
2018-04-02T19:54:07.815391: step 2071, loss 0.00138688, acc 1, learning_rate 0.000256393
2018-04-02T19:54:08.892590: step 2072, loss 0.000968817, acc 1, learning_rate 0.000256133
2018-04-02T19:54:09.970117: step 2073, loss 0.000336188, acc 1, learning_rate 0.000255873
2018-04-02T19:54:11.033769: step 2074, loss 0.000914881, acc 1, learning_rate 0.000255614
2018-04-02T19:54:12.102107: step 2075, loss 0.0239436, acc 0.984375, learning_rate 0.000255356
2018-04-02T19:54:13.169074: step 2076, loss 0.00365119, acc 1, learning_rate 0.000255097
2018-04-02T19:54:14.243354: step 2077, loss 0.00192011, acc 1, learning_rate 0.000254839
2018-04-02T19:54:15.319642: step 2078, loss 0.00388399, acc 1, learning_rate 0.000254582
2018-04-02T19:54:16.388953: step 2079, loss 0.00257327, acc 1, learning_rate 0.000254325
2018-04-02T19:54:17.455557: step 2080, loss 0.0579136, acc 0.984375, learning_rate 0.000254068
2018-04-02T19:54:18.521237: step 2081, loss 0.0014658, acc 1, learning_rate 0.000253812
2018-04-02T19:54:19.595377: step 2082, loss 0.00141601, acc 1, learning_rate 0.000253556
2018-04-02T19:54:20.668920: step 2083, loss 0.00295921, acc 1, learning_rate 0.000253301
2018-04-02T19:54:21.740176: step 2084, loss 0.00190629, acc 1, learning_rate 0.000253046
2018-04-02T19:54:22.872948: step 2085, loss 0.00124159, acc 1, learning_rate 0.000252792
2018-04-02T19:54:23.938938: step 2086, loss 0.00203819, acc 1, learning_rate 0.000252538
2018-04-02T19:54:25.007364: step 2087, loss 0.00158002, acc 1, learning_rate 0.000252284
2018-04-02T19:54:26.074424: step 2088, loss 0.00106543, acc 1, learning_rate 0.000252031
2018-04-02T19:54:27.150661: step 2089, loss 0.00265305, acc 1, learning_rate 0.000251778
2018-04-02T19:54:28.222541: step 2090, loss 0.000955531, acc 1, learning_rate 0.000251526
2018-04-02T19:54:29.299445: step 2091, loss 0.000619152, acc 1, learning_rate 0.000251274
2018-04-02T19:54:30.375181: step 2092, loss 0.00150748, acc 1, learning_rate 0.000251022
2018-04-02T19:54:31.451409: step 2093, loss 0.00180989, acc 1, learning_rate 0.000250771
2018-04-02T19:54:32.530897: step 2094, loss 0.00247792, acc 1, learning_rate 0.00025052
2018-04-02T19:54:33.599970: step 2095, loss 0.000944285, acc 1, learning_rate 0.00025027
2018-04-02T19:54:34.670507: step 2096, loss 0.00106093, acc 1, learning_rate 0.00025002
2018-04-02T19:54:35.747909: step 2097, loss 0.00290841, acc 1, learning_rate 0.000249771
2018-04-02T19:54:36.816517: step 2098, loss 0.00632176, acc 1, learning_rate 0.000249522
2018-04-02T19:54:37.900261: step 2099, loss 0.000925294, acc 1, learning_rate 0.000249273
2018-04-02T19:54:38.971809: step 2100, loss 0.00149758, acc 1, learning_rate 0.000249025

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:54:39.160658: step 2100, loss 0.054276, acc 0.978986

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-2100

2018-04-02T19:54:43.066699: step 2101, loss 0.00362192, acc 1, learning_rate 0.000248777
2018-04-02T19:54:44.144746: step 2102, loss 0.00110537, acc 1, learning_rate 0.00024853
2018-04-02T19:54:45.220960: step 2103, loss 0.00166731, acc 1, learning_rate 0.000248283
2018-04-02T19:54:46.296362: step 2104, loss 0.0012394, acc 1, learning_rate 0.000248036
2018-04-02T19:54:47.365434: step 2105, loss 0.00122433, acc 1, learning_rate 0.00024779
2018-04-02T19:54:48.440774: step 2106, loss 0.000649837, acc 1, learning_rate 0.000247545
2018-04-02T19:54:49.506818: step 2107, loss 0.0152853, acc 1, learning_rate 0.000247299
2018-04-02T19:54:50.580127: step 2108, loss 0.00742464, acc 1, learning_rate 0.000247054
2018-04-02T19:54:51.658196: step 2109, loss 0.00200369, acc 1, learning_rate 0.00024681
2018-04-02T19:54:52.732132: step 2110, loss 0.00295673, acc 1, learning_rate 0.000246566
2018-04-02T19:54:53.810104: step 2111, loss 0.000660264, acc 1, learning_rate 0.000246322
2018-04-02T19:54:54.878770: step 2112, loss 0.000895526, acc 1, learning_rate 0.000246079
2018-04-02T19:54:55.944017: step 2113, loss 0.00184506, acc 1, learning_rate 0.000245836
2018-04-02T19:54:57.021261: step 2114, loss 0.000797172, acc 1, learning_rate 0.000245593
2018-04-02T19:54:58.098957: step 2115, loss 0.00101469, acc 1, learning_rate 0.000245351
2018-04-02T19:54:59.177749: step 2116, loss 0.0796201, acc 0.984375, learning_rate 0.00024511
2018-04-02T19:55:00.253721: step 2117, loss 0.00453832, acc 1, learning_rate 0.000244868
2018-04-02T19:55:01.322243: step 2118, loss 0.000551145, acc 1, learning_rate 0.000244628
2018-04-02T19:55:02.400504: step 2119, loss 0.000632905, acc 1, learning_rate 0.000244387
2018-04-02T19:55:03.471126: step 2120, loss 0.00735637, acc 1, learning_rate 0.000244147
2018-04-02T19:55:04.540263: step 2121, loss 0.00248878, acc 1, learning_rate 0.000243907
2018-04-02T19:55:05.614995: step 2122, loss 0.00463271, acc 1, learning_rate 0.000243668
2018-04-02T19:55:06.689432: step 2123, loss 0.00297323, acc 1, learning_rate 0.000243429
2018-04-02T19:55:07.765683: step 2124, loss 0.00161049, acc 1, learning_rate 0.000243191
2018-04-02T19:55:08.832801: step 2125, loss 0.000671207, acc 1, learning_rate 0.000242953
2018-04-02T19:55:09.902769: step 2126, loss 0.000373672, acc 1, learning_rate 0.000242715
2018-04-02T19:55:10.976272: step 2127, loss 0.00263041, acc 1, learning_rate 0.000242478
2018-04-02T19:55:12.048445: step 2128, loss 0.00271147, acc 1, learning_rate 0.000242241
2018-04-02T19:55:13.123478: step 2129, loss 0.000953118, acc 1, learning_rate 0.000242004
2018-04-02T19:55:14.189254: step 2130, loss 0.00203108, acc 1, learning_rate 0.000241768
2018-04-02T19:55:15.256641: step 2131, loss 0.00103785, acc 1, learning_rate 0.000241532
2018-04-02T19:55:16.327787: step 2132, loss 0.0739592, acc 0.984375, learning_rate 0.000241297
2018-04-02T19:55:17.393932: step 2133, loss 0.00216948, acc 1, learning_rate 0.000241062
2018-04-02T19:55:18.462456: step 2134, loss 0.0690329, acc 0.984375, learning_rate 0.000240828
2018-04-02T19:55:19.530826: step 2135, loss 0.00177504, acc 1, learning_rate 0.000240593
2018-04-02T19:55:20.597944: step 2136, loss 0.00386909, acc 1, learning_rate 0.00024036
2018-04-02T19:55:21.667598: step 2137, loss 0.000967773, acc 1, learning_rate 0.000240126
2018-04-02T19:55:22.737641: step 2138, loss 0.00071948, acc 1, learning_rate 0.000239893
2018-04-02T19:55:23.803756: step 2139, loss 0.00119039, acc 1, learning_rate 0.000239661
2018-04-02T19:55:24.870786: step 2140, loss 0.0013352, acc 1, learning_rate 0.000239429
2018-04-02T19:55:25.938328: step 2141, loss 0.0743433, acc 0.984375, learning_rate 0.000239197
2018-04-02T19:55:27.006697: step 2142, loss 0.00156738, acc 1, learning_rate 0.000238965
2018-04-02T19:55:28.149975: step 2143, loss 0.0122213, acc 0.984375, learning_rate 0.000238734
2018-04-02T19:55:29.218024: step 2144, loss 0.00148669, acc 1, learning_rate 0.000238504
2018-04-02T19:55:30.284735: step 2145, loss 0.00248295, acc 1, learning_rate 0.000238273
2018-04-02T19:55:31.354744: step 2146, loss 0.00732002, acc 1, learning_rate 0.000238043
2018-04-02T19:55:32.423607: step 2147, loss 0.00111514, acc 1, learning_rate 0.000237814
2018-04-02T19:55:33.494624: step 2148, loss 0.0752584, acc 0.984375, learning_rate 0.000237585
2018-04-02T19:55:34.568730: step 2149, loss 0.0595466, acc 0.984375, learning_rate 0.000237356
2018-04-02T19:55:35.639165: step 2150, loss 0.00102334, acc 1, learning_rate 0.000237128
2018-04-02T19:55:36.708170: step 2151, loss 0.00151342, acc 1, learning_rate 0.0002369
2018-04-02T19:55:37.776211: step 2152, loss 0.00354089, acc 1, learning_rate 0.000236672
2018-04-02T19:55:38.843228: step 2153, loss 0.00135682, acc 1, learning_rate 0.000236445
2018-04-02T19:55:39.912594: step 2154, loss 0.00075945, acc 1, learning_rate 0.000236218
2018-04-02T19:55:40.978911: step 2155, loss 0.00308924, acc 1, learning_rate 0.000235991
2018-04-02T19:55:42.047025: step 2156, loss 0.00211737, acc 1, learning_rate 0.000235765
2018-04-02T19:55:43.116914: step 2157, loss 0.00408132, acc 1, learning_rate 0.000235539
2018-04-02T19:55:44.183773: step 2158, loss 0.000505158, acc 1, learning_rate 0.000235314
2018-04-02T19:55:45.250870: step 2159, loss 0.00330182, acc 1, learning_rate 0.000235089
2018-04-02T19:55:46.321297: step 2160, loss 0.000903106, acc 1, learning_rate 0.000234865
2018-04-02T19:55:47.389958: step 2161, loss 0.00432497, acc 1, learning_rate 0.00023464
2018-04-02T19:55:48.460767: step 2162, loss 0.00310669, acc 1, learning_rate 0.000234416
2018-04-02T19:55:49.531381: step 2163, loss 0.00258104, acc 1, learning_rate 0.000234193
2018-04-02T19:55:50.597432: step 2164, loss 0.000821246, acc 1, learning_rate 0.00023397
2018-04-02T19:55:51.668869: step 2165, loss 0.00344943, acc 1, learning_rate 0.000233747
2018-04-02T19:55:52.737566: step 2166, loss 0.00191134, acc 1, learning_rate 0.000233525
2018-04-02T19:55:53.806972: step 2167, loss 0.00137169, acc 1, learning_rate 0.000233303
2018-04-02T19:55:54.873766: step 2168, loss 0.00190179, acc 1, learning_rate 0.000233081
2018-04-02T19:55:55.901631: step 2169, loss 0.00364815, acc 1, learning_rate 0.00023286
2018-04-02T19:55:56.973734: step 2170, loss 0.00174385, acc 1, learning_rate 0.000232639
2018-04-02T19:55:58.046383: step 2171, loss 0.00132025, acc 1, learning_rate 0.000232418
2018-04-02T19:55:59.116285: step 2172, loss 0.00262515, acc 1, learning_rate 0.000232198
2018-04-02T19:56:00.186156: step 2173, loss 0.00237694, acc 1, learning_rate 0.000231978
2018-04-02T19:56:01.252569: step 2174, loss 0.00218181, acc 1, learning_rate 0.000231759
2018-04-02T19:56:02.319652: step 2175, loss 0.150237, acc 0.96875, learning_rate 0.00023154
2018-04-02T19:56:03.388867: step 2176, loss 0.00134311, acc 1, learning_rate 0.000231321
2018-04-02T19:56:04.457190: step 2177, loss 0.00105232, acc 1, learning_rate 0.000231103
2018-04-02T19:56:05.525515: step 2178, loss 0.00228469, acc 1, learning_rate 0.000230885
2018-04-02T19:56:06.594282: step 2179, loss 0.00295292, acc 1, learning_rate 0.000230667
2018-04-02T19:56:07.662051: step 2180, loss 0.00578129, acc 1, learning_rate 0.00023045
2018-04-02T19:56:08.728168: step 2181, loss 0.00103617, acc 1, learning_rate 0.000230233
2018-04-02T19:56:09.800912: step 2182, loss 0.00160191, acc 1, learning_rate 0.000230016
2018-04-02T19:56:10.868632: step 2183, loss 0.0842094, acc 0.984375, learning_rate 0.0002298
2018-04-02T19:56:11.935754: step 2184, loss 0.00132856, acc 1, learning_rate 0.000229584
2018-04-02T19:56:13.002878: step 2185, loss 0.00221606, acc 1, learning_rate 0.000229369
2018-04-02T19:56:14.071766: step 2186, loss 0.00237137, acc 1, learning_rate 0.000229154
2018-04-02T19:56:15.140493: step 2187, loss 0.095053, acc 0.984375, learning_rate 0.000228939
2018-04-02T19:56:16.209478: step 2188, loss 0.000827331, acc 1, learning_rate 0.000228725
2018-04-02T19:56:17.277642: step 2189, loss 0.00145003, acc 1, learning_rate 0.000228511
2018-04-02T19:56:18.344736: step 2190, loss 0.00276683, acc 1, learning_rate 0.000228297
2018-04-02T19:56:19.411471: step 2191, loss 0.00219028, acc 1, learning_rate 0.000228084
2018-04-02T19:56:20.479672: step 2192, loss 0.0018869, acc 1, learning_rate 0.000227871
2018-04-02T19:56:21.541010: step 2193, loss 0.00256963, acc 1, learning_rate 0.000227658
2018-04-02T19:56:22.605152: step 2194, loss 0.0728022, acc 0.984375, learning_rate 0.000227446
2018-04-02T19:56:23.669692: step 2195, loss 0.000938148, acc 1, learning_rate 0.000227234
2018-04-02T19:56:24.736765: step 2196, loss 0.00178082, acc 1, learning_rate 0.000227023
2018-04-02T19:56:25.804135: step 2197, loss 0.00338323, acc 1, learning_rate 0.000226811
2018-04-02T19:56:26.873875: step 2198, loss 0.00494517, acc 1, learning_rate 0.0002266
2018-04-02T19:56:27.946386: step 2199, loss 0.00376726, acc 1, learning_rate 0.00022639
2018-04-02T19:56:29.022555: step 2200, loss 0.00452385, acc 1, learning_rate 0.00022618

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:56:29.213061: step 2200, loss 0.0583138, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-2200

2018-04-02T19:56:33.295087: step 2201, loss 0.00505115, acc 1, learning_rate 0.00022597
2018-04-02T19:56:34.373389: step 2202, loss 0.00264783, acc 1, learning_rate 0.000225761
2018-04-02T19:56:35.452252: step 2203, loss 0.00498094, acc 1, learning_rate 0.000225551
2018-04-02T19:56:36.532272: step 2204, loss 0.00102938, acc 1, learning_rate 0.000225343
2018-04-02T19:56:37.603078: step 2205, loss 0.0029782, acc 1, learning_rate 0.000225134
2018-04-02T19:56:38.672500: step 2206, loss 0.0033162, acc 1, learning_rate 0.000224926
2018-04-02T19:56:39.741301: step 2207, loss 0.00318087, acc 1, learning_rate 0.000224719
2018-04-02T19:56:40.816278: step 2208, loss 0.00277312, acc 1, learning_rate 0.000224511
2018-04-02T19:56:41.885327: step 2209, loss 0.00122255, acc 1, learning_rate 0.000224304
2018-04-02T19:56:42.956667: step 2210, loss 0.00205758, acc 1, learning_rate 0.000224097
2018-04-02T19:56:44.033385: step 2211, loss 0.00239248, acc 1, learning_rate 0.000223891
2018-04-02T19:56:45.103625: step 2212, loss 0.00402238, acc 1, learning_rate 0.000223685
2018-04-02T19:56:46.180737: step 2213, loss 0.00121127, acc 1, learning_rate 0.00022348
2018-04-02T19:56:47.249612: step 2214, loss 0.00209719, acc 1, learning_rate 0.000223274
2018-04-02T19:56:48.319388: step 2215, loss 0.0499161, acc 0.984375, learning_rate 0.000223069
2018-04-02T19:56:49.389028: step 2216, loss 0.00489092, acc 1, learning_rate 0.000222865
2018-04-02T19:56:50.455665: step 2217, loss 0.00265631, acc 1, learning_rate 0.00022266
2018-04-02T19:56:51.531027: step 2218, loss 0.0013089, acc 1, learning_rate 0.000222456
2018-04-02T19:56:52.604231: step 2219, loss 0.00467506, acc 1, learning_rate 0.000222253
2018-04-02T19:56:53.675682: step 2220, loss 0.00228602, acc 1, learning_rate 0.00022205
2018-04-02T19:56:54.747499: step 2221, loss 0.00646268, acc 1, learning_rate 0.000221847
2018-04-02T19:56:55.824538: step 2222, loss 0.00442326, acc 1, learning_rate 0.000221644
2018-04-02T19:56:56.892327: step 2223, loss 0.00278324, acc 1, learning_rate 0.000221442
2018-04-02T19:56:57.967370: step 2224, loss 0.0021668, acc 1, learning_rate 0.00022124
2018-04-02T19:56:59.037969: step 2225, loss 0.0042042, acc 1, learning_rate 0.000221038
2018-04-02T19:57:00.110775: step 2226, loss 0.00208734, acc 1, learning_rate 0.000220837
2018-04-02T19:57:01.185237: step 2227, loss 0.00171716, acc 1, learning_rate 0.000220636
2018-04-02T19:57:02.252700: step 2228, loss 0.00225293, acc 1, learning_rate 0.000220435
2018-04-02T19:57:03.328638: step 2229, loss 0.00295805, acc 1, learning_rate 0.000220235
2018-04-02T19:57:04.398135: step 2230, loss 0.00263083, acc 1, learning_rate 0.000220035
2018-04-02T19:57:05.464642: step 2231, loss 0.00245371, acc 1, learning_rate 0.000219836
2018-04-02T19:57:06.532858: step 2232, loss 0.0720181, acc 0.984375, learning_rate 0.000219637
2018-04-02T19:57:07.600678: step 2233, loss 0.00354594, acc 1, learning_rate 0.000219438
2018-04-02T19:57:08.669926: step 2234, loss 0.000645703, acc 1, learning_rate 0.000219239
2018-04-02T19:57:09.742184: step 2235, loss 0.00330622, acc 1, learning_rate 0.000219041
2018-04-02T19:57:10.819137: step 2236, loss 0.00271824, acc 1, learning_rate 0.000218843
2018-04-02T19:57:11.894960: step 2237, loss 0.00216065, acc 1, learning_rate 0.000218645
2018-04-02T19:57:12.962978: step 2238, loss 0.00317251, acc 1, learning_rate 0.000218448
2018-04-02T19:57:14.033650: step 2239, loss 0.00243379, acc 1, learning_rate 0.000218251
2018-04-02T19:57:15.103798: step 2240, loss 0.00461821, acc 1, learning_rate 0.000218054
2018-04-02T19:57:16.173726: step 2241, loss 0.00751436, acc 1, learning_rate 0.000217858
2018-04-02T19:57:17.243546: step 2242, loss 0.00114323, acc 1, learning_rate 0.000217662
2018-04-02T19:57:18.314804: step 2243, loss 0.00302403, acc 1, learning_rate 0.000217467
2018-04-02T19:57:19.389452: step 2244, loss 0.00177747, acc 1, learning_rate 0.000217271
2018-04-02T19:57:20.452976: step 2245, loss 0.00237839, acc 1, learning_rate 0.000217076
2018-04-02T19:57:21.515555: step 2246, loss 0.00173917, acc 1, learning_rate 0.000216882
2018-04-02T19:57:22.588000: step 2247, loss 0.00391131, acc 1, learning_rate 0.000216687
2018-04-02T19:57:23.655331: step 2248, loss 0.000867569, acc 1, learning_rate 0.000216493
2018-04-02T19:57:24.726074: step 2249, loss 0.00200852, acc 1, learning_rate 0.0002163
2018-04-02T19:57:25.802355: step 2250, loss 0.00165541, acc 1, learning_rate 0.000216106
2018-04-02T19:57:26.870070: step 2251, loss 0.0240248, acc 0.984375, learning_rate 0.000215913
2018-04-02T19:57:27.940347: step 2252, loss 0.00226209, acc 1, learning_rate 0.00021572
2018-04-02T19:57:29.006297: step 2253, loss 0.00364304, acc 1, learning_rate 0.000215528
2018-04-02T19:57:30.075680: step 2254, loss 0.00165927, acc 1, learning_rate 0.000215336
2018-04-02T19:57:31.154452: step 2255, loss 0.00401355, acc 1, learning_rate 0.000215144
2018-04-02T19:57:32.223187: step 2256, loss 0.0533086, acc 0.984375, learning_rate 0.000214953
2018-04-02T19:57:33.300120: step 2257, loss 0.000934136, acc 1, learning_rate 0.000214762
2018-04-02T19:57:34.377531: step 2258, loss 0.00132281, acc 1, learning_rate 0.000214571
2018-04-02T19:57:35.456186: step 2259, loss 0.0034175, acc 1, learning_rate 0.00021438
2018-04-02T19:57:36.538890: step 2260, loss 0.00287688, acc 1, learning_rate 0.00021419
2018-04-02T19:57:37.620366: step 2261, loss 0.00153456, acc 1, learning_rate 0.000214
2018-04-02T19:57:38.697362: step 2262, loss 0.000854084, acc 1, learning_rate 0.000213811
2018-04-02T19:57:39.764381: step 2263, loss 0.0721609, acc 0.984375, learning_rate 0.000213621
2018-04-02T19:57:40.847054: step 2264, loss 0.00242561, acc 1, learning_rate 0.000213432
2018-04-02T19:57:41.917905: step 2265, loss 0.00163936, acc 1, learning_rate 0.000213244
2018-04-02T19:57:42.986754: step 2266, loss 0.00510926, acc 1, learning_rate 0.000213056
2018-04-02T19:57:44.064678: step 2267, loss 0.00240815, acc 1, learning_rate 0.000212868
2018-04-02T19:57:45.143145: step 2268, loss 0.00125322, acc 1, learning_rate 0.00021268
2018-04-02T19:57:46.214242: step 2269, loss 0.00186106, acc 1, learning_rate 0.000212493
2018-04-02T19:57:47.284170: step 2270, loss 0.00307928, acc 1, learning_rate 0.000212306
2018-04-02T19:57:48.363086: step 2271, loss 0.00235071, acc 1, learning_rate 0.000212119
2018-04-02T19:57:49.439243: step 2272, loss 0.00120647, acc 1, learning_rate 0.000211932
2018-04-02T19:57:50.515600: step 2273, loss 0.00210454, acc 1, learning_rate 0.000211746
2018-04-02T19:57:51.584446: step 2274, loss 0.000806838, acc 1, learning_rate 0.000211561
2018-04-02T19:57:52.654855: step 2275, loss 0.00200272, acc 1, learning_rate 0.000211375
2018-04-02T19:57:53.723752: step 2276, loss 0.00120566, acc 1, learning_rate 0.00021119
2018-04-02T19:57:54.800614: step 2277, loss 0.00169449, acc 1, learning_rate 0.000211005
2018-04-02T19:57:55.873304: step 2278, loss 0.0087726, acc 1, learning_rate 0.00021082
2018-04-02T19:57:56.953051: step 2279, loss 0.00156779, acc 1, learning_rate 0.000210636
2018-04-02T19:57:58.033466: step 2280, loss 0.000694338, acc 1, learning_rate 0.000210452
2018-04-02T19:57:59.101649: step 2281, loss 0.00415213, acc 1, learning_rate 0.000210269
2018-04-02T19:58:00.174930: step 2282, loss 0.00250595, acc 1, learning_rate 0.000210085
2018-04-02T19:58:01.250912: step 2283, loss 0.00463094, acc 1, learning_rate 0.000209902
2018-04-02T19:58:02.333190: step 2284, loss 0.00190386, acc 1, learning_rate 0.000209719
2018-04-02T19:58:03.403116: step 2285, loss 0.000962593, acc 1, learning_rate 0.000209537
2018-04-02T19:58:04.481413: step 2286, loss 0.00125847, acc 1, learning_rate 0.000209355
2018-04-02T19:58:05.549931: step 2287, loss 0.00142113, acc 1, learning_rate 0.000209173
2018-04-02T19:58:06.619394: step 2288, loss 0.00401798, acc 1, learning_rate 0.000208992
2018-04-02T19:58:07.707919: step 2289, loss 0.113891, acc 0.984375, learning_rate 0.00020881
2018-04-02T19:58:08.782122: step 2290, loss 0.00109098, acc 1, learning_rate 0.000208629
2018-04-02T19:58:09.850428: step 2291, loss 0.00352127, acc 1, learning_rate 0.000208449
2018-04-02T19:58:10.924183: step 2292, loss 0.00205562, acc 1, learning_rate 0.000208268
2018-04-02T19:58:11.991028: step 2293, loss 0.00184656, acc 1, learning_rate 0.000208088
2018-04-02T19:58:13.067353: step 2294, loss 0.0011167, acc 1, learning_rate 0.000207909
2018-04-02T19:58:14.139389: step 2295, loss 0.0180393, acc 0.984375, learning_rate 0.000207729
2018-04-02T19:58:15.215312: step 2296, loss 0.0888874, acc 0.984375, learning_rate 0.00020755
2018-04-02T19:58:16.284356: step 2297, loss 0.0043357, acc 1, learning_rate 0.000207371
2018-04-02T19:58:17.354662: step 2298, loss 0.00131735, acc 1, learning_rate 0.000207193
2018-04-02T19:58:18.426593: step 2299, loss 0.00388742, acc 1, learning_rate 0.000207015
2018-04-02T19:58:19.495891: step 2300, loss 0.00166182, acc 1, learning_rate 0.000206837

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T19:58:19.685649: step 2300, loss 0.0468194, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-2300

2018-04-02T19:58:23.731238: step 2301, loss 0.0012335, acc 1, learning_rate 0.000206659
2018-04-02T19:58:24.806197: step 2302, loss 0.0024002, acc 1, learning_rate 0.000206482
2018-04-02T19:58:25.882148: step 2303, loss 0.00115462, acc 1, learning_rate 0.000206305
2018-04-02T19:58:26.951770: step 2304, loss 0.00341859, acc 1, learning_rate 0.000206128
2018-04-02T19:58:28.020993: step 2305, loss 0.00301192, acc 1, learning_rate 0.000205951
2018-04-02T19:58:29.088743: step 2306, loss 0.00241856, acc 1, learning_rate 0.000205775
2018-04-02T19:58:30.166291: step 2307, loss 0.0012052, acc 1, learning_rate 0.000205599
2018-04-02T19:58:31.244559: step 2308, loss 0.0038107, acc 1, learning_rate 0.000205424
2018-04-02T19:58:32.313894: step 2309, loss 0.00198418, acc 1, learning_rate 0.000205249
2018-04-02T19:58:33.383638: step 2310, loss 0.00199223, acc 1, learning_rate 0.000205074
2018-04-02T19:58:34.455858: step 2311, loss 0.000937336, acc 1, learning_rate 0.000204899
2018-04-02T19:58:35.525036: step 2312, loss 0.00322979, acc 1, learning_rate 0.000204724
2018-04-02T19:58:36.599465: step 2313, loss 0.00116574, acc 1, learning_rate 0.00020455
2018-04-02T19:58:37.672731: step 2314, loss 0.00216182, acc 1, learning_rate 0.000204376
2018-04-02T19:58:38.750313: step 2315, loss 0.00114665, acc 1, learning_rate 0.000204203
2018-04-02T19:58:39.823974: step 2316, loss 0.00118181, acc 1, learning_rate 0.00020403
2018-04-02T19:58:40.900101: step 2317, loss 0.00284283, acc 1, learning_rate 0.000203857
2018-04-02T19:58:41.966882: step 2318, loss 0.00211432, acc 1, learning_rate 0.000203684
2018-04-02T19:58:43.037080: step 2319, loss 0.00141813, acc 1, learning_rate 0.000203512
2018-04-02T19:58:44.113188: step 2320, loss 0.00063598, acc 1, learning_rate 0.00020334
2018-04-02T19:58:45.184337: step 2321, loss 0.00121906, acc 1, learning_rate 0.000203168
2018-04-02T19:58:46.255277: step 2322, loss 0.00113502, acc 1, learning_rate 0.000202996
2018-04-02T19:58:47.325221: step 2323, loss 0.00196021, acc 1, learning_rate 0.000202825
2018-04-02T19:58:48.400317: step 2324, loss 0.00127143, acc 1, learning_rate 0.000202654
2018-04-02T19:58:49.467566: step 2325, loss 0.00199363, acc 1, learning_rate 0.000202483
2018-04-02T19:58:50.536555: step 2326, loss 0.00191951, acc 1, learning_rate 0.000202313
2018-04-02T19:58:51.609982: step 2327, loss 0.0735847, acc 0.984375, learning_rate 0.000202143
2018-04-02T19:58:52.688951: step 2328, loss 0.00223714, acc 1, learning_rate 0.000201973
2018-04-02T19:58:53.772833: step 2329, loss 0.00338815, acc 1, learning_rate 0.000201803
2018-04-02T19:58:54.850382: step 2330, loss 0.00172581, acc 1, learning_rate 0.000201634
2018-04-02T19:58:55.929229: step 2331, loss 0.00116537, acc 1, learning_rate 0.000201465
2018-04-02T19:58:57.007245: step 2332, loss 0.0036739, acc 1, learning_rate 0.000201296
2018-04-02T19:58:58.088643: step 2333, loss 0.000666635, acc 1, learning_rate 0.000201128
2018-04-02T19:58:59.169349: step 2334, loss 0.00080159, acc 1, learning_rate 0.00020096
2018-04-02T19:59:00.239763: step 2335, loss 0.00140555, acc 1, learning_rate 0.000200792
2018-04-02T19:59:01.314356: step 2336, loss 0.00127399, acc 1, learning_rate 0.000200624
2018-04-02T19:59:02.391701: step 2337, loss 0.00246437, acc 1, learning_rate 0.000200457
2018-04-02T19:59:03.464502: step 2338, loss 0.000795275, acc 1, learning_rate 0.00020029
2018-04-02T19:59:04.540269: step 2339, loss 0.000786551, acc 1, learning_rate 0.000200123
2018-04-02T19:59:05.610454: step 2340, loss 0.000514865, acc 1, learning_rate 0.000199957
2018-04-02T19:59:06.685515: step 2341, loss 0.00368438, acc 1, learning_rate 0.000199791
2018-04-02T19:59:07.754222: step 2342, loss 0.117813, acc 0.96875, learning_rate 0.000199625
2018-04-02T19:59:08.830546: step 2343, loss 0.00197381, acc 1, learning_rate 0.000199459
2018-04-02T19:59:09.907112: step 2344, loss 0.00190326, acc 1, learning_rate 0.000199294
2018-04-02T19:59:10.976082: step 2345, loss 0.00151142, acc 1, learning_rate 0.000199129
2018-04-02T19:59:12.055349: step 2346, loss 0.00162724, acc 1, learning_rate 0.000198964
2018-04-02T19:59:13.125496: step 2347, loss 0.00114903, acc 1, learning_rate 0.000198799
2018-04-02T19:59:14.191433: step 2348, loss 0.000356754, acc 1, learning_rate 0.000198635
2018-04-02T19:59:15.259341: step 2349, loss 0.00135478, acc 1, learning_rate 0.000198471
2018-04-02T19:59:16.333125: step 2350, loss 0.00334711, acc 1, learning_rate 0.000198307
2018-04-02T19:59:17.402263: step 2351, loss 0.00113218, acc 1, learning_rate 0.000198144
2018-04-02T19:59:18.475530: step 2352, loss 0.00268889, acc 1, learning_rate 0.000197981
2018-04-02T19:59:19.542701: step 2353, loss 0.00165217, acc 1, learning_rate 0.000197818
2018-04-02T19:59:20.616528: step 2354, loss 0.00223616, acc 1, learning_rate 0.000197655
2018-04-02T19:59:21.686924: step 2355, loss 0.00293298, acc 1, learning_rate 0.000197493
2018-04-02T19:59:22.755402: step 2356, loss 0.00110817, acc 1, learning_rate 0.000197331
2018-04-02T19:59:23.826759: step 2357, loss 0.00177197, acc 1, learning_rate 0.000197169
2018-04-02T19:59:24.905495: step 2358, loss 0.00292807, acc 1, learning_rate 0.000197007
2018-04-02T19:59:25.987209: step 2359, loss 0.00179133, acc 1, learning_rate 0.000196846
2018-04-02T19:59:27.066704: step 2360, loss 0.00179152, acc 1, learning_rate 0.000196685
2018-04-02T19:59:28.145449: step 2361, loss 0.00265474, acc 1, learning_rate 0.000196524
2018-04-02T19:59:29.220235: step 2362, loss 0.00272444, acc 1, learning_rate 0.000196364
2018-04-02T19:59:30.299213: step 2363, loss 0.00143498, acc 1, learning_rate 0.000196203
2018-04-02T19:59:31.370249: step 2364, loss 0.0015592, acc 1, learning_rate 0.000196043
2018-04-02T19:59:32.442025: step 2365, loss 0.0846979, acc 0.984375, learning_rate 0.000195884
2018-04-02T19:59:33.511018: step 2366, loss 0.00161661, acc 1, learning_rate 0.000195724
2018-04-02T19:59:34.580079: step 2367, loss 0.00261061, acc 1, learning_rate 0.000195565
2018-04-02T19:59:35.666874: step 2368, loss 0.00328006, acc 1, learning_rate 0.000195406
2018-04-02T19:59:36.746271: step 2369, loss 0.000751874, acc 1, learning_rate 0.000195248
2018-04-02T19:59:37.819150: step 2370, loss 0.00287088, acc 1, learning_rate 0.000195089
2018-04-02T19:59:38.899333: step 2371, loss 0.00177449, acc 1, learning_rate 0.000194931
2018-04-02T19:59:39.969426: step 2372, loss 0.00282358, acc 1, learning_rate 0.000194773
2018-04-02T19:59:41.043021: step 2373, loss 0.000928196, acc 1, learning_rate 0.000194616
2018-04-02T19:59:42.120628: step 2374, loss 0.00460573, acc 1, learning_rate 0.000194458
2018-04-02T19:59:43.191681: step 2375, loss 0.00150302, acc 1, learning_rate 0.000194301
2018-04-02T19:59:44.259900: step 2376, loss 0.0019163, acc 1, learning_rate 0.000194145
2018-04-02T19:59:45.331806: step 2377, loss 0.00855119, acc 1, learning_rate 0.000193988
2018-04-02T19:59:46.404927: step 2378, loss 0.00471045, acc 1, learning_rate 0.000193832
2018-04-02T19:59:47.486001: step 2379, loss 0.00307264, acc 1, learning_rate 0.000193676
2018-04-02T19:59:48.554333: step 2380, loss 0.00418069, acc 1, learning_rate 0.00019352
2018-04-02T19:59:49.625977: step 2381, loss 0.000330981, acc 1, learning_rate 0.000193365
2018-04-02T19:59:50.695103: step 2382, loss 0.00218418, acc 1, learning_rate 0.000193209
2018-04-02T19:59:51.772437: step 2383, loss 0.00299271, acc 1, learning_rate 0.000193054
2018-04-02T19:59:52.851351: step 2384, loss 0.00123636, acc 1, learning_rate 0.0001929
2018-04-02T19:59:53.921032: step 2385, loss 0.00278748, acc 1, learning_rate 0.000192745
2018-04-02T19:59:54.994236: step 2386, loss 0.00253301, acc 1, learning_rate 0.000192591
2018-04-02T19:59:56.059890: step 2387, loss 0.00181898, acc 1, learning_rate 0.000192437
2018-04-02T19:59:57.132965: step 2388, loss 0.00472847, acc 1, learning_rate 0.000192283
2018-04-02T19:59:58.205057: step 2389, loss 0.00181282, acc 1, learning_rate 0.00019213
2018-04-02T19:59:59.275081: step 2390, loss 0.000246181, acc 1, learning_rate 0.000191977
2018-04-02T20:00:00.389596: step 2391, loss 0.000789311, acc 1, learning_rate 0.000191824
2018-04-02T20:00:01.470824: step 2392, loss 0.00253051, acc 1, learning_rate 0.000191671
2018-04-02T20:00:02.551553: step 2393, loss 0.00218738, acc 1, learning_rate 0.000191519
2018-04-02T20:00:03.631753: step 2394, loss 0.00126397, acc 1, learning_rate 0.000191367
2018-04-02T20:00:04.712052: step 2395, loss 0.00109108, acc 1, learning_rate 0.000191215
2018-04-02T20:00:05.783275: step 2396, loss 0.0765974, acc 0.984375, learning_rate 0.000191063
2018-04-02T20:00:06.863912: step 2397, loss 0.00113167, acc 1, learning_rate 0.000190912
2018-04-02T20:00:07.944300: step 2398, loss 0.00119336, acc 1, learning_rate 0.00019076
2018-04-02T20:00:09.023090: step 2399, loss 0.0016462, acc 1, learning_rate 0.000190609
2018-04-02T20:00:10.095152: step 2400, loss 0.00160971, acc 1, learning_rate 0.000190459

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T20:00:10.285076: step 2400, loss 0.0506312, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522696510/checkpoints/model-2400

2018-04-02T20:00:14.355079: step 2401, loss 0.00405929, acc 1, learning_rate 0.000190308
2018-04-02T20:00:15.432227: step 2402, loss 0.000652292, acc 1, learning_rate 0.000190158
2018-04-02T20:00:16.506094: step 2403, loss 0.00142424, acc 1, learning_rate 0.000190008
2018-04-02T20:00:17.586890: step 2404, loss 0.00041422, acc 1, learning_rate 0.000189859
2018-04-02T20:00:18.660135: step 2405, loss 0.00164162, acc 1, learning_rate 0.000189709
2018-04-02T20:00:19.733790: step 2406, loss 0.00446194, acc 1, learning_rate 0.00018956
2018-04-02T20:00:20.817992: step 2407, loss 0.00307676, acc 1, learning_rate 0.000189411
2018-04-02T20:00:21.892203: step 2408, loss 0.000911954, acc 1, learning_rate 0.000189263
2018-04-02T20:00:22.965619: step 2409, loss 0.00208551, acc 1, learning_rate 0.000189114
2018-04-02T20:00:23.997029: step 2410, loss 0.000499223, acc 1, learning_rate 0.000188966
