2018-04-02 15:08:59.110152: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 15:08:59.111149: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 15:08:59.111190: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-02 15:09:03.252674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K20m
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:02:00.0
Total memory: 4.63GiB
Free memory: 4.56GiB
2018-04-02 15:09:03.517172: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xb86dff0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-04-02 15:09:03.518047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: Tesla K20m
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:83:00.0
Total memory: 4.63GiB
Free memory: 4.56GiB
2018-04-02 15:09:03.790555: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x6a62bd0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-04-02 15:09:03.791470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: Tesla K20m
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:84:00.0
Total memory: 4.63GiB
Free memory: 4.56GiB
2018-04-02 15:09:03.791544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1
2018-04-02 15:09:03.791595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 2
2018-04-02 15:09:03.791647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0
2018-04-02 15:09:03.791921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 2 and 0
2018-04-02 15:09:03.792027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 
2018-04-02 15:09:03.792062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N N 
2018-04-02 15:09:03.792090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y Y 
2018-04-02 15:09:03.792116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   N Y Y 
2018-04-02 15:09:03.792155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0)
2018-04-02 15:09:03.792186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K20m, pci bus id: 0000:83:00.0)
2018-04-02 15:09:03.792216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K20m, pci bus id: 0000:84:00.0)
2018-04-02 15:11:43.627065: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.05
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=100
FILTER_SIZES=3
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=/om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/news-data/non_trump_fake_bodies.txt
NUM_CHECKPOINTS=5
NUM_EPOCHS=10
NUM_FILTERS=128
POSITIVE_DATA_FILE=/om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/news-data/all_real_bodies_noem_no_trump.txt

Loading data...
1000 is mdl
Vocabulary Size: 163592
Train/Dev split: 15384/809
Tensor("conv-maxpool-3/conv:0", shape=(?, 998, 1, 128), dtype=float32) is conv in textcnn
Tensor("conv-maxpool-3/relu:0", shape=(?, 998, 1, 128), dtype=float32) is h in textcnn
Tensor("conv-maxpool-3/pool:0", shape=(?, 1, 1, 128), dtype=float32) is pooled
Tensor("dropout/dropout/mul:0", shape=(?, 128), dtype=float32) is h_drop
Writing to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744

Load word2vec file /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/data/word_embeddings/GoogleNews-vectors-negative300.bin
word2vec file has been loaded
2018-04-02T15:09:46.884872: step 1, loss 0.989413, acc 0.53125, learning_rate 0.005
2018-04-02T15:09:48.067765: step 2, loss 1.56654, acc 0.59375, learning_rate 0.00499185
2018-04-02T15:09:49.247111: step 3, loss 0.916225, acc 0.671875, learning_rate 0.00498372
2018-04-02T15:09:50.419886: step 4, loss 1.18794, acc 0.515625, learning_rate 0.0049756
2018-04-02T15:09:51.596133: step 5, loss 1.17964, acc 0.546875, learning_rate 0.00496749
2018-04-02T15:09:52.766104: step 6, loss 1.10933, acc 0.625, learning_rate 0.0049594
2018-04-02T15:09:53.945981: step 7, loss 0.86063, acc 0.640625, learning_rate 0.00495132
2018-04-02T15:09:55.121886: step 8, loss 0.960966, acc 0.65625, learning_rate 0.00494325
2018-04-02T15:09:56.298933: step 9, loss 0.880528, acc 0.65625, learning_rate 0.0049352
2018-04-02T15:09:57.482525: step 10, loss 1.05741, acc 0.59375, learning_rate 0.00492716
2018-04-02T15:09:58.667716: step 11, loss 0.646482, acc 0.75, learning_rate 0.00491914
2018-04-02T15:09:59.851745: step 12, loss 0.774286, acc 0.671875, learning_rate 0.00491112
2018-04-02T15:10:01.029762: step 13, loss 0.615517, acc 0.734375, learning_rate 0.00490312
2018-04-02T15:10:02.211612: step 14, loss 0.596182, acc 0.671875, learning_rate 0.00489514
2018-04-02T15:10:03.388319: step 15, loss 0.588394, acc 0.703125, learning_rate 0.00488716
2018-04-02T15:10:04.567294: step 16, loss 0.437865, acc 0.78125, learning_rate 0.0048792
2018-04-02T15:10:05.744833: step 17, loss 0.440012, acc 0.765625, learning_rate 0.00487126
2018-04-02T15:10:06.928601: step 18, loss 0.632229, acc 0.75, learning_rate 0.00486333
2018-04-02T15:10:08.108419: step 19, loss 0.624191, acc 0.796875, learning_rate 0.00485541
2018-04-02T15:10:09.289017: step 20, loss 0.509512, acc 0.8125, learning_rate 0.0048475
2018-04-02T15:10:10.472096: step 21, loss 0.389826, acc 0.8125, learning_rate 0.00483961
2018-04-02T15:10:11.654813: step 22, loss 0.3404, acc 0.859375, learning_rate 0.00483172
2018-04-02T15:10:12.831268: step 23, loss 0.396686, acc 0.828125, learning_rate 0.00482386
2018-04-02T15:10:14.011688: step 24, loss 0.34539, acc 0.828125, learning_rate 0.004816
2018-04-02T15:10:15.194439: step 25, loss 0.485384, acc 0.796875, learning_rate 0.00480816
2018-04-02T15:10:16.371606: step 26, loss 0.443276, acc 0.84375, learning_rate 0.00480033
2018-04-02T15:10:17.555005: step 27, loss 0.569468, acc 0.765625, learning_rate 0.00479252
2018-04-02T15:10:18.733068: step 28, loss 0.424575, acc 0.84375, learning_rate 0.00478472
2018-04-02T15:10:19.911875: step 29, loss 0.385488, acc 0.828125, learning_rate 0.00477693
2018-04-02T15:10:21.091767: step 30, loss 0.419961, acc 0.84375, learning_rate 0.00476915
2018-04-02T15:10:22.267564: step 31, loss 0.31164, acc 0.921875, learning_rate 0.00476139
2018-04-02T15:10:23.446715: step 32, loss 0.272178, acc 0.890625, learning_rate 0.00475364
2018-04-02T15:10:24.619874: step 33, loss 0.187479, acc 0.90625, learning_rate 0.0047459
2018-04-02T15:10:25.801778: step 34, loss 0.314875, acc 0.90625, learning_rate 0.00473818
2018-04-02T15:10:26.980844: step 35, loss 0.519647, acc 0.796875, learning_rate 0.00473046
2018-04-02T15:10:28.158139: step 36, loss 0.368063, acc 0.859375, learning_rate 0.00472276
2018-04-02T15:10:29.338991: step 37, loss 0.291029, acc 0.890625, learning_rate 0.00471508
2018-04-02T15:10:30.523238: step 38, loss 0.281036, acc 0.890625, learning_rate 0.0047074
2018-04-02T15:10:31.703091: step 39, loss 0.129359, acc 0.921875, learning_rate 0.00469974
2018-04-02T15:10:32.877043: step 40, loss 0.403957, acc 0.8125, learning_rate 0.0046921
2018-04-02T15:10:34.054132: step 41, loss 0.228137, acc 0.921875, learning_rate 0.00468446
2018-04-02T15:10:35.233363: step 42, loss 0.301873, acc 0.875, learning_rate 0.00467684
2018-04-02T15:10:36.411661: step 43, loss 0.311416, acc 0.84375, learning_rate 0.00466923
2018-04-02T15:10:37.586868: step 44, loss 0.122471, acc 0.953125, learning_rate 0.00466163
2018-04-02T15:10:38.763093: step 45, loss 0.337321, acc 0.90625, learning_rate 0.00465405
2018-04-02T15:10:39.937152: step 46, loss 0.403654, acc 0.859375, learning_rate 0.00464648
2018-04-02T15:10:41.112866: step 47, loss 0.275011, acc 0.84375, learning_rate 0.00463892
2018-04-02T15:10:42.290430: step 48, loss 0.326923, acc 0.875, learning_rate 0.00463137
2018-04-02T15:10:43.463607: step 49, loss 0.201996, acc 0.9375, learning_rate 0.00462383
2018-04-02T15:10:44.638444: step 50, loss 0.211199, acc 0.9375, learning_rate 0.00461631
2018-04-02T15:10:45.813175: step 51, loss 0.246275, acc 0.921875, learning_rate 0.0046088
2018-04-02T15:10:46.989700: step 52, loss 0.244732, acc 0.921875, learning_rate 0.00460131
2018-04-02T15:10:48.163906: step 53, loss 0.22169, acc 0.953125, learning_rate 0.00459382
2018-04-02T15:10:49.342417: step 54, loss 0.313046, acc 0.859375, learning_rate 0.00458635
2018-04-02T15:10:50.514758: step 55, loss 0.227154, acc 0.859375, learning_rate 0.00457889
2018-04-02T15:10:51.687348: step 56, loss 0.245961, acc 0.921875, learning_rate 0.00457144
2018-04-02T15:10:52.862530: step 57, loss 0.250767, acc 0.875, learning_rate 0.00456401
2018-04-02T15:10:54.038549: step 58, loss 0.252365, acc 0.90625, learning_rate 0.00455659
2018-04-02T15:10:55.213362: step 59, loss 0.209095, acc 0.921875, learning_rate 0.00454918
2018-04-02T15:10:56.386863: step 60, loss 0.0944805, acc 1, learning_rate 0.00454178
2018-04-02T15:10:57.560766: step 61, loss 0.185638, acc 0.921875, learning_rate 0.0045344
2018-04-02T15:10:58.734610: step 62, loss 0.098983, acc 0.9375, learning_rate 0.00452702
2018-04-02T15:10:59.913587: step 63, loss 0.20804, acc 0.921875, learning_rate 0.00451966
2018-04-02T15:11:01.091963: step 64, loss 0.199634, acc 0.890625, learning_rate 0.00451231
2018-04-02T15:11:02.266072: step 65, loss 0.168628, acc 0.9375, learning_rate 0.00450498
2018-04-02T15:11:03.434973: step 66, loss 0.258249, acc 0.90625, learning_rate 0.00449765
2018-04-02T15:11:04.612622: step 67, loss 0.183909, acc 0.921875, learning_rate 0.00449034
2018-04-02T15:11:05.793409: step 68, loss 0.23746, acc 0.953125, learning_rate 0.00448304
2018-04-02T15:11:06.970448: step 69, loss 0.183287, acc 0.921875, learning_rate 0.00447575
2018-04-02T15:11:08.142589: step 70, loss 0.187011, acc 0.953125, learning_rate 0.00446848
2018-04-02T15:11:09.316869: step 71, loss 0.324543, acc 0.875, learning_rate 0.00446121
2018-04-02T15:11:10.485257: step 72, loss 0.259685, acc 0.90625, learning_rate 0.00445396
2018-04-02T15:11:11.660275: step 73, loss 0.415705, acc 0.796875, learning_rate 0.00444672
2018-04-02T15:11:12.829907: step 74, loss 0.236138, acc 0.90625, learning_rate 0.0044395
2018-04-02T15:11:14.022589: step 75, loss 0.144147, acc 0.953125, learning_rate 0.00443228
2018-04-02T15:11:15.194044: step 76, loss 0.29159, acc 0.890625, learning_rate 0.00442508
2018-04-02T15:11:16.363200: step 77, loss 0.215819, acc 0.890625, learning_rate 0.00441789
2018-04-02T15:11:17.535646: step 78, loss 0.223313, acc 0.953125, learning_rate 0.00441071
2018-04-02T15:11:18.710702: step 79, loss 0.113083, acc 0.96875, learning_rate 0.00440354
2018-04-02T15:11:19.882560: step 80, loss 0.1456, acc 0.9375, learning_rate 0.00439638
2018-04-02T15:11:21.056443: step 81, loss 0.157402, acc 0.9375, learning_rate 0.00438924
2018-04-02T15:11:22.229759: step 82, loss 0.258824, acc 0.90625, learning_rate 0.00438211
2018-04-02T15:11:23.400713: step 83, loss 0.132763, acc 0.9375, learning_rate 0.00437499
2018-04-02T15:11:24.571511: step 84, loss 0.140201, acc 0.96875, learning_rate 0.00436788
2018-04-02T15:11:25.743238: step 85, loss 0.0804151, acc 0.953125, learning_rate 0.00436079
2018-04-02T15:11:26.912877: step 86, loss 0.197632, acc 0.90625, learning_rate 0.0043537
2018-04-02T15:11:28.087322: step 87, loss 0.0830805, acc 0.984375, learning_rate 0.00434663
2018-04-02T15:11:29.255372: step 88, loss 0.189236, acc 0.9375, learning_rate 0.00433957
2018-04-02T15:11:30.417577: step 89, loss 0.0763987, acc 0.984375, learning_rate 0.00433252
2018-04-02T15:11:31.586985: step 90, loss 0.0986424, acc 0.96875, learning_rate 0.00432548
2018-04-02T15:11:32.760091: step 91, loss 0.32243, acc 0.890625, learning_rate 0.00431846
2018-04-02T15:11:33.931463: step 92, loss 0.167211, acc 0.921875, learning_rate 0.00431144
2018-04-02T15:11:35.103769: step 93, loss 0.0988009, acc 0.96875, learning_rate 0.00430444
2018-04-02T15:11:36.273354: step 94, loss 0.106923, acc 0.953125, learning_rate 0.00429745
2018-04-02T15:11:37.445437: step 95, loss 0.154482, acc 0.921875, learning_rate 0.00429047
2018-04-02T15:11:38.615370: step 96, loss 0.266754, acc 0.90625, learning_rate 0.0042835
2018-04-02T15:11:39.787204: step 97, loss 0.117837, acc 0.96875, learning_rate 0.00427655
2018-04-02T15:11:40.953591: step 98, loss 0.143083, acc 0.96875, learning_rate 0.0042696
2018-04-02T15:11:42.126251: step 99, loss 0.185345, acc 0.9375, learning_rate 0.00426267
2018-04-02T15:11:43.298535: step 100, loss 0.209007, acc 0.9375, learning_rate 0.00425575

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:11:43.799070: step 100, loss 0.0835927, acc 0.97157

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-100

2018-04-02T15:11:48.886136: step 101, loss 0.261392, acc 0.9375, learning_rate 0.00424884
2018-04-02T15:11:50.060152: step 102, loss 0.0858481, acc 0.96875, learning_rate 0.00424194
2018-04-02T15:11:51.236418: step 103, loss 0.217289, acc 0.890625, learning_rate 0.00423505
2018-04-02T15:11:52.407358: step 104, loss 0.261207, acc 0.890625, learning_rate 0.00422818
2018-04-02T15:11:53.571963: step 105, loss 0.170077, acc 0.9375, learning_rate 0.00422131
2018-04-02T15:11:54.742129: step 106, loss 0.112353, acc 0.9375, learning_rate 0.00421446
2018-04-02T15:11:55.910577: step 107, loss 0.119373, acc 0.9375, learning_rate 0.00420762
2018-04-02T15:11:57.079449: step 108, loss 0.0815317, acc 0.96875, learning_rate 0.00420079
2018-04-02T15:11:58.252083: step 109, loss 0.231025, acc 0.921875, learning_rate 0.00419397
2018-04-02T15:11:59.421415: step 110, loss 0.149248, acc 0.953125, learning_rate 0.00418717
2018-04-02T15:12:00.595136: step 111, loss 0.20772, acc 0.921875, learning_rate 0.00418037
2018-04-02T15:12:01.762015: step 112, loss 0.058266, acc 0.96875, learning_rate 0.00417359
2018-04-02T15:12:02.930845: step 113, loss 0.160973, acc 0.953125, learning_rate 0.00416681
2018-04-02T15:12:04.101225: step 114, loss 0.232342, acc 0.921875, learning_rate 0.00416005
2018-04-02T15:12:05.289511: step 115, loss 0.185077, acc 0.921875, learning_rate 0.0041533
2018-04-02T15:12:06.460858: step 116, loss 0.167463, acc 0.9375, learning_rate 0.00414656
2018-04-02T15:12:07.637402: step 117, loss 0.169642, acc 0.96875, learning_rate 0.00413983
2018-04-02T15:12:08.811832: step 118, loss 0.349748, acc 0.921875, learning_rate 0.00413312
2018-04-02T15:12:09.989481: step 119, loss 0.0927697, acc 0.96875, learning_rate 0.00412641
2018-04-02T15:12:11.161090: step 120, loss 0.133548, acc 0.90625, learning_rate 0.00411972
2018-04-02T15:12:12.329591: step 121, loss 0.321849, acc 0.890625, learning_rate 0.00411303
2018-04-02T15:12:13.499406: step 122, loss 0.180321, acc 0.9375, learning_rate 0.00410636
2018-04-02T15:12:14.671512: step 123, loss 0.111072, acc 0.953125, learning_rate 0.0040997
2018-04-02T15:12:15.839695: step 124, loss 0.187838, acc 0.96875, learning_rate 0.00409305
2018-04-02T15:12:17.009225: step 125, loss 0.106075, acc 0.9375, learning_rate 0.00408641
2018-04-02T15:12:18.178165: step 126, loss 0.0752089, acc 0.9375, learning_rate 0.00407978
2018-04-02T15:12:19.351161: step 127, loss 0.224529, acc 0.9375, learning_rate 0.00407316
2018-04-02T15:12:20.520906: step 128, loss 0.200572, acc 0.9375, learning_rate 0.00406656
2018-04-02T15:12:21.691186: step 129, loss 0.214526, acc 0.90625, learning_rate 0.00405996
2018-04-02T15:12:22.860307: step 130, loss 0.146279, acc 0.953125, learning_rate 0.00405338
2018-04-02T15:12:24.027432: step 131, loss 0.0614598, acc 0.96875, learning_rate 0.00404681
2018-04-02T15:12:25.198397: step 132, loss 0.178543, acc 0.90625, learning_rate 0.00404024
2018-04-02T15:12:26.360096: step 133, loss 0.0520153, acc 1, learning_rate 0.00403369
2018-04-02T15:12:27.531780: step 134, loss 0.193394, acc 0.9375, learning_rate 0.00402715
2018-04-02T15:12:28.699197: step 135, loss 0.121713, acc 0.953125, learning_rate 0.00402062
2018-04-02T15:12:29.862963: step 136, loss 0.226709, acc 0.921875, learning_rate 0.0040141
2018-04-02T15:12:31.032506: step 137, loss 0.135412, acc 0.96875, learning_rate 0.0040076
2018-04-02T15:12:32.201996: step 138, loss 0.183613, acc 0.9375, learning_rate 0.0040011
2018-04-02T15:12:33.367626: step 139, loss 0.0352267, acc 1, learning_rate 0.00399461
2018-04-02T15:12:34.534480: step 140, loss 0.242255, acc 0.921875, learning_rate 0.00398814
2018-04-02T15:12:35.701756: step 141, loss 0.437564, acc 0.921875, learning_rate 0.00398167
2018-04-02T15:12:36.870721: step 142, loss 0.137828, acc 0.953125, learning_rate 0.00397522
2018-04-02T15:12:38.038308: step 143, loss 0.0462428, acc 1, learning_rate 0.00396877
2018-04-02T15:12:39.210177: step 144, loss 0.324791, acc 0.90625, learning_rate 0.00396234
2018-04-02T15:12:40.378683: step 145, loss 0.11918, acc 0.953125, learning_rate 0.00395592
2018-04-02T15:12:41.542740: step 146, loss 0.0707064, acc 0.984375, learning_rate 0.00394951
2018-04-02T15:12:42.710682: step 147, loss 0.0755354, acc 0.953125, learning_rate 0.00394311
2018-04-02T15:12:43.878074: step 148, loss 0.206479, acc 0.90625, learning_rate 0.00393672
2018-04-02T15:12:45.047108: step 149, loss 0.04181, acc 0.984375, learning_rate 0.00393034
2018-04-02T15:12:46.215293: step 150, loss 0.125628, acc 0.9375, learning_rate 0.00392397
2018-04-02T15:12:47.383582: step 151, loss 0.201384, acc 0.90625, learning_rate 0.00391761
2018-04-02T15:12:48.549308: step 152, loss 0.12054, acc 0.96875, learning_rate 0.00391126
2018-04-02T15:12:49.715193: step 153, loss 0.0960373, acc 0.953125, learning_rate 0.00390493
2018-04-02T15:12:50.882963: step 154, loss 0.10144, acc 0.96875, learning_rate 0.0038986
2018-04-02T15:12:52.052878: step 155, loss 0.146741, acc 0.96875, learning_rate 0.00389229
2018-04-02T15:12:53.222472: step 156, loss 0.0879432, acc 0.96875, learning_rate 0.00388598
2018-04-02T15:12:54.393005: step 157, loss 0.0469564, acc 1, learning_rate 0.00387969
2018-04-02T15:12:55.556288: step 158, loss 0.0998749, acc 0.96875, learning_rate 0.0038734
2018-04-02T15:12:56.723903: step 159, loss 0.136274, acc 0.9375, learning_rate 0.00386713
2018-04-02T15:12:57.895929: step 160, loss 0.260221, acc 0.921875, learning_rate 0.00386086
2018-04-02T15:12:59.069519: step 161, loss 0.0617591, acc 0.96875, learning_rate 0.00385461
2018-04-02T15:13:00.236655: step 162, loss 0.17026, acc 0.96875, learning_rate 0.00384837
2018-04-02T15:13:01.404166: step 163, loss 0.14251, acc 0.953125, learning_rate 0.00384214
2018-04-02T15:13:02.563533: step 164, loss 0.105244, acc 0.96875, learning_rate 0.00383591
2018-04-02T15:13:03.729747: step 165, loss 0.113633, acc 0.953125, learning_rate 0.0038297
2018-04-02T15:13:04.893820: step 166, loss 0.152808, acc 0.9375, learning_rate 0.0038235
2018-04-02T15:13:06.063751: step 167, loss 0.0608844, acc 0.96875, learning_rate 0.00381731
2018-04-02T15:13:07.232460: step 168, loss 0.126414, acc 0.96875, learning_rate 0.00381113
2018-04-02T15:13:08.402924: step 169, loss 0.154968, acc 0.953125, learning_rate 0.00380496
2018-04-02T15:13:09.570940: step 170, loss 0.0667062, acc 0.984375, learning_rate 0.0037988
2018-04-02T15:13:10.736097: step 171, loss 0.35738, acc 0.921875, learning_rate 0.00379265
2018-04-02T15:13:11.902786: step 172, loss 0.156477, acc 0.9375, learning_rate 0.00378651
2018-04-02T15:13:13.068442: step 173, loss 0.0879673, acc 0.96875, learning_rate 0.00378038
2018-04-02T15:13:14.231525: step 174, loss 0.0635881, acc 0.984375, learning_rate 0.00377426
2018-04-02T15:13:15.400522: step 175, loss 0.219464, acc 0.9375, learning_rate 0.00376815
2018-04-02T15:13:16.568226: step 176, loss 0.0915358, acc 0.96875, learning_rate 0.00376205
2018-04-02T15:13:17.732435: step 177, loss 0.0960703, acc 0.96875, learning_rate 0.00375596
2018-04-02T15:13:18.902384: step 178, loss 0.0694899, acc 0.96875, learning_rate 0.00374988
2018-04-02T15:13:20.070437: step 179, loss 0.224265, acc 0.953125, learning_rate 0.00374382
2018-04-02T15:13:21.240229: step 180, loss 0.0682104, acc 0.984375, learning_rate 0.00373776
2018-04-02T15:13:22.407957: step 181, loss 0.125394, acc 0.953125, learning_rate 0.00373171
2018-04-02T15:13:23.574479: step 182, loss 0.0628387, acc 0.984375, learning_rate 0.00372567
2018-04-02T15:13:24.739743: step 183, loss 0.0410384, acc 0.984375, learning_rate 0.00371964
2018-04-02T15:13:25.908765: step 184, loss 0.065445, acc 0.96875, learning_rate 0.00371362
2018-04-02T15:13:27.078742: step 185, loss 0.110897, acc 0.9375, learning_rate 0.00370762
2018-04-02T15:13:28.243586: step 186, loss 0.10548, acc 0.96875, learning_rate 0.00370162
2018-04-02T15:13:29.408972: step 187, loss 0.131237, acc 0.984375, learning_rate 0.00369563
2018-04-02T15:13:30.574244: step 188, loss 0.00930779, acc 1, learning_rate 0.00368965
2018-04-02T15:13:31.733761: step 189, loss 0.0887779, acc 0.953125, learning_rate 0.00368368
2018-04-02T15:13:32.896272: step 190, loss 0.186776, acc 0.9375, learning_rate 0.00367772
2018-04-02T15:13:34.060590: step 191, loss 0.152488, acc 0.96875, learning_rate 0.00367177
2018-04-02T15:13:35.230383: step 192, loss 0.125793, acc 0.953125, learning_rate 0.00366584
2018-04-02T15:13:36.392808: step 193, loss 0.161808, acc 0.921875, learning_rate 0.00365991
2018-04-02T15:13:37.558238: step 194, loss 0.119679, acc 0.9375, learning_rate 0.00365399
2018-04-02T15:13:38.726438: step 195, loss 0.13584, acc 0.9375, learning_rate 0.00364808
2018-04-02T15:13:39.894357: step 196, loss 0.210027, acc 0.90625, learning_rate 0.00364218
2018-04-02T15:13:41.060316: step 197, loss 0.0698033, acc 0.96875, learning_rate 0.00363629
2018-04-02T15:13:42.226613: step 198, loss 0.223574, acc 0.921875, learning_rate 0.00363041
2018-04-02T15:13:43.391737: step 199, loss 0.102709, acc 0.953125, learning_rate 0.00362454
2018-04-02T15:13:44.554451: step 200, loss 0.0973269, acc 0.96875, learning_rate 0.00361868

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:13:44.765953: step 200, loss 0.0648207, acc 0.976514

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-200

2018-04-02T15:13:49.096282: step 201, loss 0.0398974, acc 1, learning_rate 0.00361283
2018-04-02T15:13:50.260964: step 202, loss 0.137287, acc 0.9375, learning_rate 0.00360699
2018-04-02T15:13:51.426703: step 203, loss 0.06813, acc 0.96875, learning_rate 0.00360116
2018-04-02T15:13:52.592400: step 204, loss 0.111553, acc 0.9375, learning_rate 0.00359534
2018-04-02T15:13:53.756879: step 205, loss 0.154372, acc 0.953125, learning_rate 0.00358953
2018-04-02T15:13:54.925218: step 206, loss 0.0986173, acc 0.96875, learning_rate 0.00358372
2018-04-02T15:13:56.088556: step 207, loss 0.0471086, acc 1, learning_rate 0.00357793
2018-04-02T15:13:57.255407: step 208, loss 0.206413, acc 0.875, learning_rate 0.00357215
2018-04-02T15:13:58.424813: step 209, loss 0.118206, acc 0.9375, learning_rate 0.00356637
2018-04-02T15:13:59.589207: step 210, loss 0.133015, acc 0.96875, learning_rate 0.00356061
2018-04-02T15:14:00.755303: step 211, loss 0.0935664, acc 0.96875, learning_rate 0.00355486
2018-04-02T15:14:01.922346: step 212, loss 0.132907, acc 0.9375, learning_rate 0.00354911
2018-04-02T15:14:03.086933: step 213, loss 0.033535, acc 1, learning_rate 0.00354338
2018-04-02T15:14:04.252979: step 214, loss 0.0923835, acc 0.96875, learning_rate 0.00353765
2018-04-02T15:14:05.424367: step 215, loss 0.158031, acc 0.953125, learning_rate 0.00353194
2018-04-02T15:14:06.585676: step 216, loss 0.15025, acc 0.953125, learning_rate 0.00352623
2018-04-02T15:14:07.752709: step 217, loss 0.133883, acc 0.96875, learning_rate 0.00352053
2018-04-02T15:14:08.917846: step 218, loss 0.163872, acc 0.9375, learning_rate 0.00351485
2018-04-02T15:14:10.083848: step 219, loss 0.14484, acc 0.953125, learning_rate 0.00350917
2018-04-02T15:14:11.249113: step 220, loss 0.115236, acc 0.953125, learning_rate 0.0035035
2018-04-02T15:14:12.416020: step 221, loss 0.0412018, acc 0.984375, learning_rate 0.00349784
2018-04-02T15:14:13.583164: step 222, loss 0.0305259, acc 1, learning_rate 0.00349219
2018-04-02T15:14:14.750730: step 223, loss 0.079295, acc 0.984375, learning_rate 0.00348655
2018-04-02T15:14:15.914906: step 224, loss 0.128554, acc 0.9375, learning_rate 0.00348092
2018-04-02T15:14:17.082590: step 225, loss 0.140675, acc 0.96875, learning_rate 0.0034753
2018-04-02T15:14:18.245407: step 226, loss 0.175054, acc 0.921875, learning_rate 0.00346969
2018-04-02T15:14:19.417042: step 227, loss 0.0874024, acc 0.96875, learning_rate 0.00346409
2018-04-02T15:14:20.579835: step 228, loss 0.112581, acc 0.984375, learning_rate 0.00345849
2018-04-02T15:14:21.732790: step 229, loss 0.192182, acc 0.953125, learning_rate 0.00345291
2018-04-02T15:14:22.903089: step 230, loss 0.136377, acc 0.953125, learning_rate 0.00344733
2018-04-02T15:14:24.072278: step 231, loss 0.119143, acc 0.953125, learning_rate 0.00344177
2018-04-02T15:14:25.235940: step 232, loss 0.149811, acc 0.921875, learning_rate 0.00343621
2018-04-02T15:14:26.398693: step 233, loss 0.0597327, acc 0.96875, learning_rate 0.00343066
2018-04-02T15:14:27.566120: step 234, loss 0.173028, acc 0.9375, learning_rate 0.00342513
2018-04-02T15:14:28.730464: step 235, loss 0.111771, acc 0.96875, learning_rate 0.0034196
2018-04-02T15:14:29.899948: step 236, loss 0.0623717, acc 0.984375, learning_rate 0.00341408
2018-04-02T15:14:31.066421: step 237, loss 0.329235, acc 0.90625, learning_rate 0.00340857
2018-04-02T15:14:32.231867: step 238, loss 0.235935, acc 0.9375, learning_rate 0.00340307
2018-04-02T15:14:33.397028: step 239, loss 0.0375225, acc 0.984375, learning_rate 0.00339758
2018-04-02T15:14:34.561437: step 240, loss 0.198571, acc 0.90625, learning_rate 0.00339209
2018-04-02T15:14:35.768059: step 241, loss 0.0261503, acc 1, learning_rate 0.00338662
2018-04-02T15:14:36.934942: step 242, loss 0.0389334, acc 0.984375, learning_rate 0.00338115
2018-04-02T15:14:38.104241: step 243, loss 0.0188936, acc 1, learning_rate 0.0033757
2018-04-02T15:14:39.271180: step 244, loss 0.027434, acc 1, learning_rate 0.00337025
2018-04-02T15:14:40.434182: step 245, loss 0.0292758, acc 1, learning_rate 0.00336481
2018-04-02T15:14:41.598533: step 246, loss 0.199685, acc 0.96875, learning_rate 0.00335939
2018-04-02T15:14:42.769645: step 247, loss 0.069704, acc 0.96875, learning_rate 0.00335397
2018-04-02T15:14:43.937274: step 248, loss 0.0392145, acc 0.984375, learning_rate 0.00334856
2018-04-02T15:14:45.108687: step 249, loss 0.0141543, acc 1, learning_rate 0.00334316
2018-04-02T15:14:46.275180: step 250, loss 0.0113833, acc 1, learning_rate 0.00333776
2018-04-02T15:14:47.444109: step 251, loss 0.0597503, acc 0.96875, learning_rate 0.00333238
2018-04-02T15:14:48.612674: step 252, loss 0.0681784, acc 0.96875, learning_rate 0.00332701
2018-04-02T15:14:49.779219: step 253, loss 0.0492793, acc 0.984375, learning_rate 0.00332164
2018-04-02T15:14:50.945173: step 254, loss 0.05371, acc 0.984375, learning_rate 0.00331628
2018-04-02T15:14:52.108781: step 255, loss 0.0532334, acc 0.984375, learning_rate 0.00331094
2018-04-02T15:14:53.273818: step 256, loss 0.0407229, acc 0.984375, learning_rate 0.0033056
2018-04-02T15:14:54.438471: step 257, loss 0.0540056, acc 0.984375, learning_rate 0.00330027
2018-04-02T15:14:55.603310: step 258, loss 0.0492712, acc 0.984375, learning_rate 0.00329495
2018-04-02T15:14:56.770738: step 259, loss 0.0193993, acc 1, learning_rate 0.00328963
2018-04-02T15:14:57.936063: step 260, loss 0.0217497, acc 1, learning_rate 0.00328433
2018-04-02T15:14:59.099704: step 261, loss 0.0257753, acc 1, learning_rate 0.00327904
2018-04-02T15:15:00.265493: step 262, loss 0.0376435, acc 0.96875, learning_rate 0.00327375
2018-04-02T15:15:01.431874: step 263, loss 0.0521999, acc 0.984375, learning_rate 0.00326847
2018-04-02T15:15:02.607133: step 264, loss 0.0542743, acc 0.984375, learning_rate 0.0032632
2018-04-02T15:15:03.780644: step 265, loss 0.067734, acc 0.96875, learning_rate 0.00325795
2018-04-02T15:15:04.955942: step 266, loss 0.0291706, acc 1, learning_rate 0.00325269
2018-04-02T15:15:06.122724: step 267, loss 0.0728277, acc 0.96875, learning_rate 0.00324745
2018-04-02T15:15:07.289852: step 268, loss 0.0331445, acc 0.984375, learning_rate 0.00324222
2018-04-02T15:15:08.456414: step 269, loss 0.0366273, acc 0.984375, learning_rate 0.003237
2018-04-02T15:15:09.625238: step 270, loss 0.0395126, acc 0.984375, learning_rate 0.00323178
2018-04-02T15:15:10.796146: step 271, loss 0.0166701, acc 1, learning_rate 0.00322657
2018-04-02T15:15:11.965681: step 272, loss 0.0896013, acc 0.96875, learning_rate 0.00322137
2018-04-02T15:15:13.131073: step 273, loss 0.0716409, acc 0.96875, learning_rate 0.00321618
2018-04-02T15:15:14.297001: step 274, loss 0.041468, acc 0.984375, learning_rate 0.003211
2018-04-02T15:15:15.464399: step 275, loss 0.0182136, acc 1, learning_rate 0.00320583
2018-04-02T15:15:16.630294: step 276, loss 0.120388, acc 0.953125, learning_rate 0.00320067
2018-04-02T15:15:17.794432: step 277, loss 0.0116116, acc 1, learning_rate 0.00319551
2018-04-02T15:15:18.959120: step 278, loss 0.0305858, acc 0.984375, learning_rate 0.00319036
2018-04-02T15:15:20.128366: step 279, loss 0.0107923, acc 1, learning_rate 0.00318523
2018-04-02T15:15:21.284871: step 280, loss 0.0283727, acc 1, learning_rate 0.0031801
2018-04-02T15:15:22.448694: step 281, loss 0.0199334, acc 1, learning_rate 0.00317497
2018-04-02T15:15:23.614269: step 282, loss 0.0494624, acc 0.984375, learning_rate 0.00316986
2018-04-02T15:15:24.768337: step 283, loss 0.0216589, acc 0.984375, learning_rate 0.00316476
2018-04-02T15:15:25.936258: step 284, loss 0.120732, acc 0.984375, learning_rate 0.00315966
2018-04-02T15:15:27.100842: step 285, loss 0.0228595, acc 1, learning_rate 0.00315457
2018-04-02T15:15:28.263601: step 286, loss 0.0180027, acc 1, learning_rate 0.0031495
2018-04-02T15:15:29.425405: step 287, loss 0.022518, acc 0.984375, learning_rate 0.00314443
2018-04-02T15:15:30.590563: step 288, loss 0.0334021, acc 0.984375, learning_rate 0.00313936
2018-04-02T15:15:31.755570: step 289, loss 0.008414, acc 1, learning_rate 0.00313431
2018-04-02T15:15:32.922313: step 290, loss 0.0686986, acc 0.984375, learning_rate 0.00312926
2018-04-02T15:15:34.082691: step 291, loss 0.0458273, acc 0.96875, learning_rate 0.00312423
2018-04-02T15:15:35.249658: step 292, loss 0.0545259, acc 0.96875, learning_rate 0.0031192
2018-04-02T15:15:36.410968: step 293, loss 0.051947, acc 0.984375, learning_rate 0.00311418
2018-04-02T15:15:37.572900: step 294, loss 0.107233, acc 0.984375, learning_rate 0.00310917
2018-04-02T15:15:38.742458: step 295, loss 0.0222342, acc 1, learning_rate 0.00310416
2018-04-02T15:15:39.907065: step 296, loss 0.0283772, acc 0.984375, learning_rate 0.00309917
2018-04-02T15:15:41.072425: step 297, loss 0.0859102, acc 0.984375, learning_rate 0.00309418
2018-04-02T15:15:42.237793: step 298, loss 0.0534426, acc 0.984375, learning_rate 0.0030892
2018-04-02T15:15:43.402707: step 299, loss 0.0673935, acc 0.953125, learning_rate 0.00308423
2018-04-02T15:15:44.564189: step 300, loss 0.0233019, acc 0.984375, learning_rate 0.00307927

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:15:44.775661: step 300, loss 0.0506524, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-300

2018-04-02T15:15:49.133510: step 301, loss 0.0222105, acc 1, learning_rate 0.00307432
2018-04-02T15:15:50.297586: step 302, loss 0.0494463, acc 0.984375, learning_rate 0.00306937
2018-04-02T15:15:51.462039: step 303, loss 0.00461805, acc 1, learning_rate 0.00306444
2018-04-02T15:15:52.623141: step 304, loss 0.0224871, acc 0.984375, learning_rate 0.00305951
2018-04-02T15:15:53.789555: step 305, loss 0.0253489, acc 0.984375, learning_rate 0.00305459
2018-04-02T15:15:54.955194: step 306, loss 0.0303136, acc 0.984375, learning_rate 0.00304967
2018-04-02T15:15:56.121117: step 307, loss 0.0112031, acc 1, learning_rate 0.00304477
2018-04-02T15:15:57.268272: step 308, loss 0.0372386, acc 0.984375, learning_rate 0.00303987
2018-04-02T15:15:58.435153: step 309, loss 0.0186282, acc 1, learning_rate 0.00303499
2018-04-02T15:15:59.599522: step 310, loss 0.0181515, acc 1, learning_rate 0.00303011
2018-04-02T15:16:00.762852: step 311, loss 0.015752, acc 1, learning_rate 0.00302523
2018-04-02T15:16:01.919385: step 312, loss 0.0140038, acc 1, learning_rate 0.00302037
2018-04-02T15:16:03.081642: step 313, loss 0.0222436, acc 1, learning_rate 0.00301551
2018-04-02T15:16:04.238271: step 314, loss 0.0278694, acc 0.984375, learning_rate 0.00301067
2018-04-02T15:16:05.399006: step 315, loss 0.076618, acc 0.984375, learning_rate 0.00300583
2018-04-02T15:16:06.562204: step 316, loss 0.108858, acc 0.96875, learning_rate 0.003001
2018-04-02T15:16:07.721032: step 317, loss 0.0251909, acc 0.984375, learning_rate 0.00299617
2018-04-02T15:16:08.887801: step 318, loss 0.0267611, acc 1, learning_rate 0.00299136
2018-04-02T15:16:10.051270: step 319, loss 0.0347562, acc 0.984375, learning_rate 0.00298655
2018-04-02T15:16:11.219195: step 320, loss 0.00718596, acc 1, learning_rate 0.00298175
2018-04-02T15:16:12.382707: step 321, loss 0.0341291, acc 0.984375, learning_rate 0.00297696
2018-04-02T15:16:13.548226: step 322, loss 0.119213, acc 0.953125, learning_rate 0.00297218
2018-04-02T15:16:14.708708: step 323, loss 0.0200474, acc 1, learning_rate 0.0029674
2018-04-02T15:16:15.873260: step 324, loss 0.0418286, acc 0.984375, learning_rate 0.00296263
2018-04-02T15:16:17.033949: step 325, loss 0.02787, acc 0.984375, learning_rate 0.00295787
2018-04-02T15:16:18.200309: step 326, loss 0.0138021, acc 1, learning_rate 0.00295312
2018-04-02T15:16:19.359542: step 327, loss 0.0189561, acc 0.984375, learning_rate 0.00294838
2018-04-02T15:16:20.513089: step 328, loss 0.0532081, acc 0.984375, learning_rate 0.00294364
2018-04-02T15:16:21.678285: step 329, loss 0.0157592, acc 1, learning_rate 0.00293891
2018-04-02T15:16:22.841182: step 330, loss 0.0676532, acc 0.984375, learning_rate 0.00293419
2018-04-02T15:16:24.002813: step 331, loss 0.083923, acc 0.96875, learning_rate 0.00292948
2018-04-02T15:16:25.170679: step 332, loss 0.0447688, acc 0.984375, learning_rate 0.00292478
2018-04-02T15:16:26.333159: step 333, loss 0.00429426, acc 1, learning_rate 0.00292008
2018-04-02T15:16:27.493307: step 334, loss 0.0124032, acc 1, learning_rate 0.00291539
2018-04-02T15:16:28.658105: step 335, loss 0.0111255, acc 1, learning_rate 0.00291071
2018-04-02T15:16:29.820790: step 336, loss 0.0713253, acc 0.984375, learning_rate 0.00290604
2018-04-02T15:16:30.980221: step 337, loss 0.00749806, acc 1, learning_rate 0.00290137
2018-04-02T15:16:32.140910: step 338, loss 0.0105066, acc 1, learning_rate 0.00289671
2018-04-02T15:16:33.307533: step 339, loss 0.200646, acc 0.953125, learning_rate 0.00289206
2018-04-02T15:16:34.471761: step 340, loss 0.0850443, acc 0.953125, learning_rate 0.00288742
2018-04-02T15:16:35.633721: step 341, loss 0.168911, acc 0.953125, learning_rate 0.00288279
2018-04-02T15:16:36.798630: step 342, loss 0.0251864, acc 1, learning_rate 0.00287816
2018-04-02T15:16:37.961472: step 343, loss 0.0133244, acc 1, learning_rate 0.00287354
2018-04-02T15:16:39.125854: step 344, loss 0.0707867, acc 0.96875, learning_rate 0.00286893
2018-04-02T15:16:40.287809: step 345, loss 0.121716, acc 0.96875, learning_rate 0.00286432
2018-04-02T15:16:41.447721: step 346, loss 0.0102313, acc 1, learning_rate 0.00285973
2018-04-02T15:16:42.612404: step 347, loss 0.026385, acc 0.984375, learning_rate 0.00285514
2018-04-02T15:16:43.780136: step 348, loss 0.0297568, acc 0.984375, learning_rate 0.00285056
2018-04-02T15:16:44.943358: step 349, loss 0.0616368, acc 0.96875, learning_rate 0.00284599
2018-04-02T15:16:46.105768: step 350, loss 0.04733, acc 0.984375, learning_rate 0.00284142
2018-04-02T15:16:47.267770: step 351, loss 0.0263465, acc 0.984375, learning_rate 0.00283686
2018-04-02T15:16:48.426339: step 352, loss 0.00959067, acc 1, learning_rate 0.00283231
2018-04-02T15:16:49.592161: step 353, loss 0.0221766, acc 1, learning_rate 0.00282777
2018-04-02T15:16:50.747492: step 354, loss 0.0334118, acc 0.984375, learning_rate 0.00282323
2018-04-02T15:16:51.906823: step 355, loss 0.0221985, acc 1, learning_rate 0.0028187
2018-04-02T15:16:53.068602: step 356, loss 0.0731082, acc 0.984375, learning_rate 0.00281418
2018-04-02T15:16:54.232099: step 357, loss 0.0178743, acc 1, learning_rate 0.00280967
2018-04-02T15:16:55.407831: step 358, loss 0.0531684, acc 0.984375, learning_rate 0.00280517
2018-04-02T15:16:56.581157: step 359, loss 0.0140219, acc 1, learning_rate 0.00280067
2018-04-02T15:16:57.743069: step 360, loss 0.0226399, acc 1, learning_rate 0.00279618
2018-04-02T15:16:58.912388: step 361, loss 0.0500781, acc 0.984375, learning_rate 0.0027917
2018-04-02T15:17:00.088932: step 362, loss 0.0498996, acc 0.96875, learning_rate 0.00278722
2018-04-02T15:17:01.254203: step 363, loss 0.010024, acc 1, learning_rate 0.00278275
2018-04-02T15:17:02.412270: step 364, loss 0.0305622, acc 0.984375, learning_rate 0.00277829
2018-04-02T15:17:03.583994: step 365, loss 0.0170073, acc 1, learning_rate 0.00277384
2018-04-02T15:17:04.747014: step 366, loss 0.053638, acc 0.984375, learning_rate 0.00276939
2018-04-02T15:17:05.910883: step 367, loss 0.010402, acc 1, learning_rate 0.00276495
2018-04-02T15:17:07.069597: step 368, loss 0.00828843, acc 1, learning_rate 0.00276052
2018-04-02T15:17:08.228897: step 369, loss 0.00497967, acc 1, learning_rate 0.0027561
2018-04-02T15:17:09.389059: step 370, loss 0.0051682, acc 1, learning_rate 0.00275168
2018-04-02T15:17:10.554562: step 371, loss 0.0414489, acc 0.96875, learning_rate 0.00274727
2018-04-02T15:17:11.724086: step 372, loss 0.101211, acc 0.96875, learning_rate 0.00274287
2018-04-02T15:17:12.882198: step 373, loss 0.0265333, acc 0.984375, learning_rate 0.00273848
2018-04-02T15:17:14.059323: step 374, loss 0.0294506, acc 0.984375, learning_rate 0.00273409
2018-04-02T15:17:15.218623: step 375, loss 0.00545091, acc 1, learning_rate 0.00272971
2018-04-02T15:17:16.390677: step 376, loss 0.00569171, acc 1, learning_rate 0.00272534
2018-04-02T15:17:17.549923: step 377, loss 0.046475, acc 0.984375, learning_rate 0.00272097
2018-04-02T15:17:18.720638: step 378, loss 0.0300923, acc 0.984375, learning_rate 0.00271662
2018-04-02T15:17:19.887059: step 379, loss 0.020581, acc 1, learning_rate 0.00271227
2018-04-02T15:17:21.063349: step 380, loss 0.0558715, acc 0.96875, learning_rate 0.00270792
2018-04-02T15:17:22.223515: step 381, loss 0.0631123, acc 0.984375, learning_rate 0.00270359
2018-04-02T15:17:23.384955: step 382, loss 0.0192186, acc 1, learning_rate 0.00269926
2018-04-02T15:17:24.547634: step 383, loss 0.0438982, acc 0.984375, learning_rate 0.00269494
2018-04-02T15:17:25.719532: step 384, loss 0.00733036, acc 1, learning_rate 0.00269062
2018-04-02T15:17:26.895311: step 385, loss 0.0555506, acc 0.984375, learning_rate 0.00268631
2018-04-02T15:17:28.067278: step 386, loss 0.0160578, acc 1, learning_rate 0.00268201
2018-04-02T15:17:29.228797: step 387, loss 0.0117195, acc 1, learning_rate 0.00267772
2018-04-02T15:17:30.391929: step 388, loss 0.0389411, acc 0.96875, learning_rate 0.00267343
2018-04-02T15:17:31.569507: step 389, loss 0.00826683, acc 1, learning_rate 0.00266916
2018-04-02T15:17:32.726050: step 390, loss 0.0751281, acc 0.984375, learning_rate 0.00266488
2018-04-02T15:17:33.895665: step 391, loss 0.0106652, acc 1, learning_rate 0.00266062
2018-04-02T15:17:35.069371: step 392, loss 0.0693946, acc 0.96875, learning_rate 0.00265636
2018-04-02T15:17:36.231877: step 393, loss 0.0853567, acc 0.984375, learning_rate 0.00265211
2018-04-02T15:17:37.405023: step 394, loss 0.00393239, acc 1, learning_rate 0.00264787
2018-04-02T15:17:38.569754: step 395, loss 0.0168626, acc 1, learning_rate 0.00264363
2018-04-02T15:17:39.730532: step 396, loss 0.0542821, acc 0.984375, learning_rate 0.0026394
2018-04-02T15:17:40.895146: step 397, loss 0.00702055, acc 1, learning_rate 0.00263518
2018-04-02T15:17:42.059818: step 398, loss 0.05322, acc 0.984375, learning_rate 0.00263097
2018-04-02T15:17:43.221273: step 399, loss 0.0400191, acc 0.984375, learning_rate 0.00262676
2018-04-02T15:17:44.384777: step 400, loss 0.0163646, acc 1, learning_rate 0.00262256

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:17:44.596037: step 400, loss 0.04671, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-400

2018-04-02T15:17:48.539403: step 401, loss 0.0475613, acc 0.984375, learning_rate 0.00261836
2018-04-02T15:17:49.702394: step 402, loss 0.0135614, acc 1, learning_rate 0.00261417
2018-04-02T15:17:50.862850: step 403, loss 0.0998814, acc 0.9375, learning_rate 0.00260999
2018-04-02T15:17:52.025500: step 404, loss 0.0406942, acc 0.984375, learning_rate 0.00260582
2018-04-02T15:17:53.188441: step 405, loss 0.0232447, acc 1, learning_rate 0.00260165
2018-04-02T15:17:54.356351: step 406, loss 0.0376552, acc 0.984375, learning_rate 0.0025975
2018-04-02T15:17:55.531167: step 407, loss 0.0251221, acc 0.984375, learning_rate 0.00259334
2018-04-02T15:17:56.679677: step 408, loss 0.0116612, acc 1, learning_rate 0.0025892
2018-04-02T15:17:57.849470: step 409, loss 0.0571544, acc 0.96875, learning_rate 0.00258506
2018-04-02T15:17:59.020738: step 410, loss 0.0144239, acc 1, learning_rate 0.00258093
2018-04-02T15:18:00.203126: step 411, loss 0.0132246, acc 1, learning_rate 0.0025768
2018-04-02T15:18:01.365948: step 412, loss 0.00558334, acc 1, learning_rate 0.00257268
2018-04-02T15:18:02.536868: step 413, loss 0.0872809, acc 0.984375, learning_rate 0.00256857
2018-04-02T15:18:03.709255: step 414, loss 0.00992699, acc 1, learning_rate 0.00256447
2018-04-02T15:18:04.881445: step 415, loss 0.00970795, acc 1, learning_rate 0.00256037
2018-04-02T15:18:06.040902: step 416, loss 0.0158939, acc 1, learning_rate 0.00255628
2018-04-02T15:18:07.198588: step 417, loss 0.00986899, acc 1, learning_rate 0.00255219
2018-04-02T15:18:08.357946: step 418, loss 0.0126624, acc 1, learning_rate 0.00254812
2018-04-02T15:18:09.526683: step 419, loss 0.0814245, acc 0.984375, learning_rate 0.00254405
2018-04-02T15:18:10.707337: step 420, loss 0.0497304, acc 0.984375, learning_rate 0.00253998
2018-04-02T15:18:11.868922: step 421, loss 0.0595574, acc 0.96875, learning_rate 0.00253593
2018-04-02T15:18:13.043261: step 422, loss 0.154597, acc 0.953125, learning_rate 0.00253188
2018-04-02T15:18:14.205921: step 423, loss 0.0351503, acc 0.984375, learning_rate 0.00252783
2018-04-02T15:18:15.381246: step 424, loss 0.0128686, acc 1, learning_rate 0.0025238
2018-04-02T15:18:16.545782: step 425, loss 0.0612576, acc 0.953125, learning_rate 0.00251977
2018-04-02T15:18:17.706152: step 426, loss 0.0356578, acc 0.984375, learning_rate 0.00251574
2018-04-02T15:18:18.864181: step 427, loss 0.0881049, acc 0.96875, learning_rate 0.00251173
2018-04-02T15:18:20.040341: step 428, loss 0.00476255, acc 1, learning_rate 0.00250772
2018-04-02T15:18:21.199600: step 429, loss 0.0344329, acc 0.984375, learning_rate 0.00250371
2018-04-02T15:18:22.359520: step 430, loss 0.075727, acc 0.96875, learning_rate 0.00249972
2018-04-02T15:18:23.528511: step 431, loss 0.00659434, acc 1, learning_rate 0.00249573
2018-04-02T15:18:24.691654: step 432, loss 0.117583, acc 0.984375, learning_rate 0.00249174
2018-04-02T15:18:25.859636: step 433, loss 0.0097008, acc 1, learning_rate 0.00248777
2018-04-02T15:18:27.021512: step 434, loss 0.117304, acc 0.96875, learning_rate 0.0024838
2018-04-02T15:18:28.194251: step 435, loss 0.0138196, acc 1, learning_rate 0.00247983
2018-04-02T15:18:29.367156: step 436, loss 0.0107206, acc 1, learning_rate 0.00247588
2018-04-02T15:18:30.521984: step 437, loss 0.0199889, acc 0.984375, learning_rate 0.00247193
2018-04-02T15:18:31.681713: step 438, loss 0.0569272, acc 0.984375, learning_rate 0.00246798
2018-04-02T15:18:32.836751: step 439, loss 0.0267211, acc 1, learning_rate 0.00246404
2018-04-02T15:18:34.012989: step 440, loss 0.0485055, acc 0.96875, learning_rate 0.00246011
2018-04-02T15:18:35.175495: step 441, loss 0.0638908, acc 0.984375, learning_rate 0.00245619
2018-04-02T15:18:36.341711: step 442, loss 0.0513958, acc 0.984375, learning_rate 0.00245227
2018-04-02T15:18:37.505625: step 443, loss 0.0495867, acc 0.96875, learning_rate 0.00244836
2018-04-02T15:18:38.676053: step 444, loss 0.0167116, acc 1, learning_rate 0.00244446
2018-04-02T15:18:39.842265: step 445, loss 0.13764, acc 0.953125, learning_rate 0.00244056
2018-04-02T15:18:41.015580: step 446, loss 0.0299864, acc 0.984375, learning_rate 0.00243667
2018-04-02T15:18:42.179244: step 447, loss 0.0246085, acc 0.984375, learning_rate 0.00243278
2018-04-02T15:18:43.345535: step 448, loss 0.0322393, acc 0.984375, learning_rate 0.0024289
2018-04-02T15:18:44.509438: step 449, loss 0.115126, acc 0.96875, learning_rate 0.00242503
2018-04-02T15:18:45.678255: step 450, loss 0.0926163, acc 0.984375, learning_rate 0.00242117
2018-04-02T15:18:46.844089: step 451, loss 0.211115, acc 0.9375, learning_rate 0.00241731
2018-04-02T15:18:48.015997: step 452, loss 0.00746585, acc 1, learning_rate 0.00241345
2018-04-02T15:18:49.192344: step 453, loss 0.0181619, acc 1, learning_rate 0.00240961
2018-04-02T15:18:50.365495: step 454, loss 0.0153677, acc 1, learning_rate 0.00240577
2018-04-02T15:18:51.535066: step 455, loss 0.0429611, acc 0.96875, learning_rate 0.00240193
2018-04-02T15:18:52.704566: step 456, loss 0.00743661, acc 1, learning_rate 0.00239811
2018-04-02T15:18:53.862548: step 457, loss 0.0382801, acc 0.984375, learning_rate 0.00239428
2018-04-02T15:18:55.037807: step 458, loss 0.0182555, acc 1, learning_rate 0.00239047
2018-04-02T15:18:56.207509: step 459, loss 0.156678, acc 0.953125, learning_rate 0.00238666
2018-04-02T15:18:57.371243: step 460, loss 0.0177019, acc 1, learning_rate 0.00238286
2018-04-02T15:18:58.545718: step 461, loss 0.0102242, acc 1, learning_rate 0.00237906
2018-04-02T15:18:59.711517: step 462, loss 0.102117, acc 0.984375, learning_rate 0.00237527
2018-04-02T15:19:00.887209: step 463, loss 0.0400145, acc 0.984375, learning_rate 0.00237149
2018-04-02T15:19:02.055375: step 464, loss 0.00817927, acc 1, learning_rate 0.00236771
2018-04-02T15:19:03.228534: step 465, loss 0.0447991, acc 0.984375, learning_rate 0.00236394
2018-04-02T15:19:04.391217: step 466, loss 0.0184937, acc 1, learning_rate 0.00236018
2018-04-02T15:19:05.555171: step 467, loss 0.0170363, acc 1, learning_rate 0.00235642
2018-04-02T15:19:06.723633: step 468, loss 0.00942903, acc 1, learning_rate 0.00235267
2018-04-02T15:19:07.894982: step 469, loss 0.0426091, acc 0.96875, learning_rate 0.00234892
2018-04-02T15:19:09.068979: step 470, loss 0.0735205, acc 0.96875, learning_rate 0.00234519
2018-04-02T15:19:10.232848: step 471, loss 0.00552464, acc 1, learning_rate 0.00234145
2018-04-02T15:19:11.407383: step 472, loss 0.0675876, acc 0.96875, learning_rate 0.00233773
2018-04-02T15:19:12.580540: step 473, loss 0.0050778, acc 1, learning_rate 0.002334
2018-04-02T15:19:13.741856: step 474, loss 0.0397647, acc 0.96875, learning_rate 0.00233029
2018-04-02T15:19:14.906670: step 475, loss 0.113023, acc 0.984375, learning_rate 0.00232658
2018-04-02T15:19:16.065029: step 476, loss 0.0294101, acc 0.984375, learning_rate 0.00232288
2018-04-02T15:19:17.235689: step 477, loss 0.00806036, acc 1, learning_rate 0.00231918
2018-04-02T15:19:18.404562: step 478, loss 0.0886156, acc 0.96875, learning_rate 0.00231549
2018-04-02T15:19:19.561376: step 479, loss 0.0153777, acc 1, learning_rate 0.00231181
2018-04-02T15:19:20.723609: step 480, loss 0.0324721, acc 0.984375, learning_rate 0.00230813
2018-04-02T15:19:21.896948: step 481, loss 0.0125537, acc 1, learning_rate 0.00230446
2018-04-02T15:19:23.017188: step 482, loss 0.00466172, acc 1, learning_rate 0.0023008
2018-04-02T15:19:24.180202: step 483, loss 0.00637375, acc 1, learning_rate 0.00229714
2018-04-02T15:19:25.347466: step 484, loss 0.0068619, acc 1, learning_rate 0.00229348
2018-04-02T15:19:26.519611: step 485, loss 0.0371392, acc 0.984375, learning_rate 0.00228984
2018-04-02T15:19:27.678947: step 486, loss 0.0211057, acc 0.984375, learning_rate 0.0022862
2018-04-02T15:19:28.835159: step 487, loss 0.0685683, acc 0.984375, learning_rate 0.00228256
2018-04-02T15:19:30.005216: step 488, loss 0.00879058, acc 1, learning_rate 0.00227893
2018-04-02T15:19:31.176992: step 489, loss 0.0335939, acc 0.984375, learning_rate 0.00227531
2018-04-02T15:19:32.340502: step 490, loss 0.0100076, acc 1, learning_rate 0.00227169
2018-04-02T15:19:33.500358: step 491, loss 0.00464536, acc 1, learning_rate 0.00226808
2018-04-02T15:19:34.670885: step 492, loss 0.00643239, acc 1, learning_rate 0.00226448
2018-04-02T15:19:35.831014: step 493, loss 0.0157773, acc 1, learning_rate 0.00226088
2018-04-02T15:19:36.994818: step 494, loss 0.0174057, acc 0.984375, learning_rate 0.00225728
2018-04-02T15:19:38.156003: step 495, loss 0.0712286, acc 0.984375, learning_rate 0.0022537
2018-04-02T15:19:39.320246: step 496, loss 0.0129322, acc 1, learning_rate 0.00225012
2018-04-02T15:19:40.495101: step 497, loss 0.00760778, acc 1, learning_rate 0.00224654
2018-04-02T15:19:41.667832: step 498, loss 0.0126375, acc 1, learning_rate 0.00224297
2018-04-02T15:19:42.838596: step 499, loss 0.0191113, acc 0.984375, learning_rate 0.00223941
2018-04-02T15:19:44.000796: step 500, loss 0.0174488, acc 1, learning_rate 0.00223585

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:19:44.211820: step 500, loss 0.0490991, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-500

2018-04-02T15:19:48.625696: step 501, loss 0.0115521, acc 1, learning_rate 0.0022323
2018-04-02T15:19:49.794829: step 502, loss 0.00583753, acc 1, learning_rate 0.00222876
2018-04-02T15:19:50.972922: step 503, loss 0.00273114, acc 1, learning_rate 0.00222522
2018-04-02T15:19:52.144852: step 504, loss 0.00862258, acc 1, learning_rate 0.00222168
2018-04-02T15:19:53.321858: step 505, loss 0.0101071, acc 1, learning_rate 0.00221816
2018-04-02T15:19:54.495676: step 506, loss 0.00211503, acc 1, learning_rate 0.00221463
2018-04-02T15:19:55.666006: step 507, loss 0.0598685, acc 0.96875, learning_rate 0.00221112
2018-04-02T15:19:56.825246: step 508, loss 0.00661279, acc 1, learning_rate 0.00220761
2018-04-02T15:19:57.985318: step 509, loss 0.00620719, acc 1, learning_rate 0.0022041
2018-04-02T15:19:59.143628: step 510, loss 0.0127621, acc 1, learning_rate 0.0022006
2018-04-02T15:20:00.306753: step 511, loss 0.00573134, acc 1, learning_rate 0.00219711
2018-04-02T15:20:01.467060: step 512, loss 0.0057818, acc 1, learning_rate 0.00219363
2018-04-02T15:20:02.624790: step 513, loss 0.017018, acc 0.984375, learning_rate 0.00219014
2018-04-02T15:20:03.786171: step 514, loss 0.00352286, acc 1, learning_rate 0.00218667
2018-04-02T15:20:04.938994: step 515, loss 0.00915192, acc 1, learning_rate 0.0021832
2018-04-02T15:20:06.109599: step 516, loss 0.011957, acc 1, learning_rate 0.00217974
2018-04-02T15:20:07.282410: step 517, loss 0.02116, acc 1, learning_rate 0.00217628
2018-04-02T15:20:08.455808: step 518, loss 0.00446048, acc 1, learning_rate 0.00217283
2018-04-02T15:20:09.626594: step 519, loss 0.0271724, acc 0.984375, learning_rate 0.00216938
2018-04-02T15:20:10.788860: step 520, loss 0.0125253, acc 1, learning_rate 0.00216594
2018-04-02T15:20:11.947583: step 521, loss 0.00511074, acc 1, learning_rate 0.0021625
2018-04-02T15:20:13.108453: step 522, loss 0.00871725, acc 1, learning_rate 0.00215907
2018-04-02T15:20:14.266413: step 523, loss 0.00913118, acc 1, learning_rate 0.00215565
2018-04-02T15:20:15.428278: step 524, loss 0.0115592, acc 1, learning_rate 0.00215223
2018-04-02T15:20:16.597296: step 525, loss 0.00372769, acc 1, learning_rate 0.00214882
2018-04-02T15:20:17.771264: step 526, loss 0.0717794, acc 0.96875, learning_rate 0.00214541
2018-04-02T15:20:18.929789: step 527, loss 0.0129232, acc 1, learning_rate 0.00214201
2018-04-02T15:20:20.091926: step 528, loss 0.0961469, acc 0.96875, learning_rate 0.00213862
2018-04-02T15:20:21.253998: step 529, loss 0.0534229, acc 0.984375, learning_rate 0.00213523
2018-04-02T15:20:22.425697: step 530, loss 0.00529738, acc 1, learning_rate 0.00213184
2018-04-02T15:20:23.595486: step 531, loss 0.017031, acc 1, learning_rate 0.00212847
2018-04-02T15:20:24.755147: step 532, loss 0.0618591, acc 0.984375, learning_rate 0.00212509
2018-04-02T15:20:25.917102: step 533, loss 0.0139373, acc 1, learning_rate 0.00212173
2018-04-02T15:20:27.090014: step 534, loss 0.00515425, acc 1, learning_rate 0.00211836
2018-04-02T15:20:28.250829: step 535, loss 0.0057254, acc 1, learning_rate 0.00211501
2018-04-02T15:20:29.425712: step 536, loss 0.00710552, acc 1, learning_rate 0.00211166
2018-04-02T15:20:30.600467: step 537, loss 0.00476405, acc 1, learning_rate 0.00210831
2018-04-02T15:20:31.758430: step 538, loss 0.0223632, acc 0.984375, learning_rate 0.00210497
2018-04-02T15:20:32.930398: step 539, loss 0.0113568, acc 1, learning_rate 0.00210164
2018-04-02T15:20:34.088526: step 540, loss 0.0108368, acc 1, learning_rate 0.00209831
2018-04-02T15:20:35.252661: step 541, loss 0.00317943, acc 1, learning_rate 0.00209499
2018-04-02T15:20:36.425434: step 542, loss 0.00506911, acc 1, learning_rate 0.00209167
2018-04-02T15:20:37.588548: step 543, loss 0.00516517, acc 1, learning_rate 0.00208836
2018-04-02T15:20:38.750923: step 544, loss 0.0139234, acc 1, learning_rate 0.00208506
2018-04-02T15:20:39.922460: step 545, loss 0.00806295, acc 1, learning_rate 0.00208176
2018-04-02T15:20:41.091331: step 546, loss 0.0172499, acc 0.984375, learning_rate 0.00207846
2018-04-02T15:20:42.257056: step 547, loss 0.00461555, acc 1, learning_rate 0.00207517
2018-04-02T15:20:43.428078: step 548, loss 0.0306321, acc 0.984375, learning_rate 0.00207189
2018-04-02T15:20:44.603475: step 549, loss 0.0483931, acc 0.984375, learning_rate 0.00206861
2018-04-02T15:20:45.772561: step 550, loss 0.00770301, acc 1, learning_rate 0.00206533
2018-04-02T15:20:46.927036: step 551, loss 0.00718653, acc 1, learning_rate 0.00206207
2018-04-02T15:20:48.090443: step 552, loss 0.0295428, acc 0.984375, learning_rate 0.0020588
2018-04-02T15:20:49.264049: step 553, loss 0.0153361, acc 1, learning_rate 0.00205555
2018-04-02T15:20:50.426125: step 554, loss 0.00448524, acc 1, learning_rate 0.0020523
2018-04-02T15:20:51.601056: step 555, loss 0.00442618, acc 1, learning_rate 0.00204905
2018-04-02T15:20:52.780508: step 556, loss 0.00794253, acc 1, learning_rate 0.00204581
2018-04-02T15:20:53.953424: step 557, loss 0.00255461, acc 1, learning_rate 0.00204257
2018-04-02T15:20:55.112469: step 558, loss 0.00902268, acc 1, learning_rate 0.00203934
2018-04-02T15:20:56.267102: step 559, loss 0.0047802, acc 1, learning_rate 0.00203612
2018-04-02T15:20:57.440247: step 560, loss 0.00487199, acc 1, learning_rate 0.0020329
2018-04-02T15:20:58.600178: step 561, loss 0.00398355, acc 1, learning_rate 0.00202969
2018-04-02T15:20:59.762378: step 562, loss 0.0147655, acc 0.984375, learning_rate 0.00202648
2018-04-02T15:21:00.921283: step 563, loss 0.00875458, acc 1, learning_rate 0.00202328
2018-04-02T15:21:02.096737: step 564, loss 0.00783886, acc 1, learning_rate 0.00202008
2018-04-02T15:21:03.254489: step 565, loss 0.00232112, acc 1, learning_rate 0.00201689
2018-04-02T15:21:04.428588: step 566, loss 0.0251059, acc 0.984375, learning_rate 0.0020137
2018-04-02T15:21:05.598968: step 567, loss 0.00726781, acc 1, learning_rate 0.00201052
2018-04-02T15:21:06.763320: step 568, loss 0.0109867, acc 1, learning_rate 0.00200734
2018-04-02T15:21:07.932117: step 569, loss 0.00291081, acc 1, learning_rate 0.00200417
2018-04-02T15:21:09.095601: step 570, loss 0.0134138, acc 0.984375, learning_rate 0.002001
2018-04-02T15:21:10.272801: step 571, loss 0.00188316, acc 1, learning_rate 0.00199784
2018-04-02T15:21:11.438380: step 572, loss 0.0567058, acc 0.96875, learning_rate 0.00199469
2018-04-02T15:21:12.601890: step 573, loss 0.0104103, acc 1, learning_rate 0.00199154
2018-04-02T15:21:13.771436: step 574, loss 0.0120485, acc 1, learning_rate 0.00198839
2018-04-02T15:21:14.945794: step 575, loss 0.0062425, acc 1, learning_rate 0.00198525
2018-04-02T15:21:16.105114: step 576, loss 0.00595321, acc 1, learning_rate 0.00198212
2018-04-02T15:21:17.268796: step 577, loss 0.00922914, acc 1, learning_rate 0.00197899
2018-04-02T15:21:18.434279: step 578, loss 0.0208712, acc 1, learning_rate 0.00197586
2018-04-02T15:21:19.605824: step 579, loss 0.00322053, acc 1, learning_rate 0.00197274
2018-04-02T15:21:20.777654: step 580, loss 0.00900009, acc 1, learning_rate 0.00196963
2018-04-02T15:21:21.948638: step 581, loss 0.012382, acc 1, learning_rate 0.00196652
2018-04-02T15:21:23.118786: step 582, loss 0.0180208, acc 0.984375, learning_rate 0.00196342
2018-04-02T15:21:24.289892: step 583, loss 0.00317906, acc 1, learning_rate 0.00196032
2018-04-02T15:21:25.462657: step 584, loss 0.0121939, acc 1, learning_rate 0.00195723
2018-04-02T15:21:26.623007: step 585, loss 0.00762428, acc 1, learning_rate 0.00195414
2018-04-02T15:21:27.770284: step 586, loss 0.018567, acc 0.984375, learning_rate 0.00195106
2018-04-02T15:21:28.933422: step 587, loss 0.0122123, acc 1, learning_rate 0.00194798
2018-04-02T15:21:30.090031: step 588, loss 0.0106584, acc 1, learning_rate 0.00194491
2018-04-02T15:21:31.239468: step 589, loss 0.0152591, acc 0.984375, learning_rate 0.00194184
2018-04-02T15:21:32.403673: step 590, loss 0.0216161, acc 0.984375, learning_rate 0.00193878
2018-04-02T15:21:33.559969: step 591, loss 0.00925077, acc 1, learning_rate 0.00193572
2018-04-02T15:21:34.722156: step 592, loss 0.00358922, acc 1, learning_rate 0.00193267
2018-04-02T15:21:35.882686: step 593, loss 0.0449083, acc 0.984375, learning_rate 0.00192962
2018-04-02T15:21:37.043418: step 594, loss 0.00680099, acc 1, learning_rate 0.00192658
2018-04-02T15:21:38.200779: step 595, loss 0.0224593, acc 0.984375, learning_rate 0.00192354
2018-04-02T15:21:39.365302: step 596, loss 0.0221772, acc 0.984375, learning_rate 0.00192051
2018-04-02T15:21:40.527178: step 597, loss 0.00435528, acc 1, learning_rate 0.00191748
2018-04-02T15:21:41.681791: step 598, loss 0.0109582, acc 1, learning_rate 0.00191446
2018-04-02T15:21:42.841292: step 599, loss 0.0106168, acc 1, learning_rate 0.00191144
2018-04-02T15:21:44.000559: step 600, loss 0.00667715, acc 1, learning_rate 0.00190843

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:21:44.211293: step 600, loss 0.0437439, acc 0.986403

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-600

2018-04-02T15:21:48.253481: step 601, loss 0.0151871, acc 1, learning_rate 0.00190542
2018-04-02T15:21:49.416991: step 602, loss 0.00560942, acc 1, learning_rate 0.00190242
2018-04-02T15:21:50.577217: step 603, loss 0.00530714, acc 1, learning_rate 0.00189942
2018-04-02T15:21:51.741872: step 604, loss 0.032989, acc 0.984375, learning_rate 0.00189643
2018-04-02T15:21:52.903712: step 605, loss 0.00528642, acc 1, learning_rate 0.00189345
2018-04-02T15:21:54.067355: step 606, loss 0.00508972, acc 1, learning_rate 0.00189046
2018-04-02T15:21:55.213700: step 607, loss 0.0050774, acc 1, learning_rate 0.00188749
2018-04-02T15:21:56.376271: step 608, loss 0.0110105, acc 1, learning_rate 0.00188452
2018-04-02T15:21:57.538434: step 609, loss 0.0793305, acc 0.984375, learning_rate 0.00188155
2018-04-02T15:21:58.702491: step 610, loss 0.0067162, acc 1, learning_rate 0.00187859
2018-04-02T15:21:59.861052: step 611, loss 0.00361452, acc 1, learning_rate 0.00187563
2018-04-02T15:22:01.019017: step 612, loss 0.00227435, acc 1, learning_rate 0.00187268
2018-04-02T15:22:02.180936: step 613, loss 0.00312975, acc 1, learning_rate 0.00186973
2018-04-02T15:22:03.331951: step 614, loss 0.0267833, acc 0.984375, learning_rate 0.00186679
2018-04-02T15:22:04.492159: step 615, loss 0.00288481, acc 1, learning_rate 0.00186385
2018-04-02T15:22:05.654110: step 616, loss 0.0068107, acc 1, learning_rate 0.00186092
2018-04-02T15:22:06.820814: step 617, loss 0.0105583, acc 1, learning_rate 0.00185799
2018-04-02T15:22:07.982437: step 618, loss 0.00512691, acc 1, learning_rate 0.00185507
2018-04-02T15:22:09.137114: step 619, loss 0.00413001, acc 1, learning_rate 0.00185215
2018-04-02T15:22:10.294857: step 620, loss 0.0109838, acc 1, learning_rate 0.00184923
2018-04-02T15:22:11.461352: step 621, loss 0.00602739, acc 1, learning_rate 0.00184633
2018-04-02T15:22:12.624129: step 622, loss 0.00951826, acc 1, learning_rate 0.00184342
2018-04-02T15:22:13.780319: step 623, loss 0.00911074, acc 1, learning_rate 0.00184052
2018-04-02T15:22:14.941403: step 624, loss 0.042593, acc 0.984375, learning_rate 0.00183763
2018-04-02T15:22:16.098905: step 625, loss 0.024677, acc 0.984375, learning_rate 0.00183474
2018-04-02T15:22:17.256384: step 626, loss 0.00375102, acc 1, learning_rate 0.00183186
2018-04-02T15:22:18.414755: step 627, loss 0.00501882, acc 1, learning_rate 0.00182898
2018-04-02T15:22:19.574941: step 628, loss 0.00869638, acc 1, learning_rate 0.0018261
2018-04-02T15:22:20.734786: step 629, loss 0.020692, acc 0.984375, learning_rate 0.00182323
2018-04-02T15:22:21.900294: step 630, loss 0.0112458, acc 1, learning_rate 0.00182037
2018-04-02T15:22:23.063941: step 631, loss 0.0768187, acc 0.984375, learning_rate 0.00181751
2018-04-02T15:22:24.224899: step 632, loss 0.00831736, acc 1, learning_rate 0.00181465
2018-04-02T15:22:25.386267: step 633, loss 0.00656282, acc 1, learning_rate 0.0018118
2018-04-02T15:22:26.550162: step 634, loss 0.00298495, acc 1, learning_rate 0.00180895
2018-04-02T15:22:27.703309: step 635, loss 0.00568524, acc 1, learning_rate 0.00180611
2018-04-02T15:22:28.867797: step 636, loss 0.0139511, acc 1, learning_rate 0.00180328
2018-04-02T15:22:30.028070: step 637, loss 0.0188931, acc 1, learning_rate 0.00180044
2018-04-02T15:22:31.186536: step 638, loss 0.00232562, acc 1, learning_rate 0.00179762
2018-04-02T15:22:32.347340: step 639, loss 0.0039588, acc 1, learning_rate 0.00179479
2018-04-02T15:22:33.508048: step 640, loss 0.00627919, acc 1, learning_rate 0.00179198
2018-04-02T15:22:34.667785: step 641, loss 0.0099586, acc 1, learning_rate 0.00178916
2018-04-02T15:22:35.831882: step 642, loss 0.0117775, acc 1, learning_rate 0.00178635
2018-04-02T15:22:36.990532: step 643, loss 0.00424877, acc 1, learning_rate 0.00178355
2018-04-02T15:22:38.150937: step 644, loss 0.0144566, acc 1, learning_rate 0.00178075
2018-04-02T15:22:39.311548: step 645, loss 0.00486496, acc 1, learning_rate 0.00177796
2018-04-02T15:22:40.470397: step 646, loss 0.0163843, acc 0.984375, learning_rate 0.00177517
2018-04-02T15:22:41.632198: step 647, loss 0.0101788, acc 1, learning_rate 0.00177238
2018-04-02T15:22:42.787872: step 648, loss 0.0034746, acc 1, learning_rate 0.0017696
2018-04-02T15:22:43.947336: step 649, loss 0.0202759, acc 0.984375, learning_rate 0.00176682
2018-04-02T15:22:45.109572: step 650, loss 0.00509539, acc 1, learning_rate 0.00176405
2018-04-02T15:22:46.273048: step 651, loss 0.00546484, acc 1, learning_rate 0.00176129
2018-04-02T15:22:47.432294: step 652, loss 0.00549856, acc 1, learning_rate 0.00175852
2018-04-02T15:22:48.592936: step 653, loss 0.0449562, acc 0.984375, learning_rate 0.00175577
2018-04-02T15:22:49.753741: step 654, loss 0.0100862, acc 1, learning_rate 0.00175301
2018-04-02T15:22:50.916315: step 655, loss 0.00631422, acc 1, learning_rate 0.00175026
2018-04-02T15:22:52.075624: step 656, loss 0.00392759, acc 1, learning_rate 0.00174752
2018-04-02T15:22:53.230646: step 657, loss 0.0146034, acc 1, learning_rate 0.00174478
2018-04-02T15:22:54.389725: step 658, loss 0.00411902, acc 1, learning_rate 0.00174205
2018-04-02T15:22:55.546962: step 659, loss 0.0811454, acc 0.96875, learning_rate 0.00173932
2018-04-02T15:22:56.709033: step 660, loss 0.0118796, acc 1, learning_rate 0.00173659
2018-04-02T15:22:57.870105: step 661, loss 0.0113216, acc 1, learning_rate 0.00173387
2018-04-02T15:22:59.033389: step 662, loss 0.0199981, acc 0.984375, learning_rate 0.00173115
2018-04-02T15:23:00.192242: step 663, loss 0.0360212, acc 0.984375, learning_rate 0.00172844
2018-04-02T15:23:01.349740: step 664, loss 0.0468339, acc 0.984375, learning_rate 0.00172573
2018-04-02T15:23:02.509248: step 665, loss 0.00954932, acc 1, learning_rate 0.00172303
2018-04-02T15:23:03.671597: step 666, loss 0.00835618, acc 1, learning_rate 0.00172033
2018-04-02T15:23:04.831420: step 667, loss 0.0218666, acc 0.984375, learning_rate 0.00171764
2018-04-02T15:23:05.993305: step 668, loss 0.0101102, acc 1, learning_rate 0.00171495
2018-04-02T15:23:07.156965: step 669, loss 0.0225571, acc 0.984375, learning_rate 0.00171226
2018-04-02T15:23:08.316151: step 670, loss 0.00647475, acc 1, learning_rate 0.00170958
2018-04-02T15:23:09.477303: step 671, loss 0.00520953, acc 1, learning_rate 0.00170691
2018-04-02T15:23:10.636295: step 672, loss 0.00994062, acc 1, learning_rate 0.00170423
2018-04-02T15:23:11.800684: step 673, loss 0.00573691, acc 1, learning_rate 0.00170157
2018-04-02T15:23:12.961335: step 674, loss 0.0608775, acc 0.984375, learning_rate 0.0016989
2018-04-02T15:23:14.127497: step 675, loss 0.0508114, acc 0.984375, learning_rate 0.00169625
2018-04-02T15:23:15.292273: step 676, loss 0.00611236, acc 1, learning_rate 0.00169359
2018-04-02T15:23:16.452481: step 677, loss 0.00556486, acc 1, learning_rate 0.00169094
2018-04-02T15:23:17.615178: step 678, loss 0.00569455, acc 1, learning_rate 0.0016883
2018-04-02T15:23:18.776732: step 679, loss 0.0108187, acc 1, learning_rate 0.00168566
2018-04-02T15:23:19.938053: step 680, loss 0.0117703, acc 1, learning_rate 0.00168302
2018-04-02T15:23:21.097186: step 681, loss 0.00815772, acc 1, learning_rate 0.00168039
2018-04-02T15:23:22.264196: step 682, loss 0.0169069, acc 1, learning_rate 0.00167776
2018-04-02T15:23:23.416118: step 683, loss 0.0134615, acc 1, learning_rate 0.00167514
2018-04-02T15:23:24.579801: step 684, loss 0.0434291, acc 0.96875, learning_rate 0.00167252
2018-04-02T15:23:25.741223: step 685, loss 0.0127216, acc 1, learning_rate 0.0016699
2018-04-02T15:23:26.904458: step 686, loss 0.00657797, acc 1, learning_rate 0.00166729
2018-04-02T15:23:28.067242: step 687, loss 0.00989457, acc 1, learning_rate 0.00166469
2018-04-02T15:23:29.217205: step 688, loss 0.0334653, acc 0.984375, learning_rate 0.00166209
2018-04-02T15:23:30.376925: step 689, loss 0.0183233, acc 1, learning_rate 0.00165949
2018-04-02T15:23:31.538951: step 690, loss 0.0051597, acc 1, learning_rate 0.00165689
2018-04-02T15:23:32.700927: step 691, loss 0.0242665, acc 0.984375, learning_rate 0.00165431
2018-04-02T15:23:33.862341: step 692, loss 0.00201375, acc 1, learning_rate 0.00165172
2018-04-02T15:23:35.020207: step 693, loss 0.00922539, acc 1, learning_rate 0.00164914
2018-04-02T15:23:36.180012: step 694, loss 0.00725335, acc 1, learning_rate 0.00164657
2018-04-02T15:23:37.339829: step 695, loss 0.00192793, acc 1, learning_rate 0.00164399
2018-04-02T15:23:38.503079: step 696, loss 0.00434893, acc 1, learning_rate 0.00164143
2018-04-02T15:23:39.666288: step 697, loss 0.00263275, acc 1, learning_rate 0.00163886
2018-04-02T15:23:40.824258: step 698, loss 0.0112136, acc 1, learning_rate 0.00163631
2018-04-02T15:23:41.982422: step 699, loss 0.00845494, acc 1, learning_rate 0.00163375
2018-04-02T15:23:43.143425: step 700, loss 0.0125815, acc 1, learning_rate 0.0016312

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:23:43.355651: step 700, loss 0.0418352, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-700

2018-04-02T15:23:47.534027: step 701, loss 0.00682918, acc 1, learning_rate 0.00162866
2018-04-02T15:23:48.701551: step 702, loss 0.0142998, acc 1, learning_rate 0.00162611
2018-04-02T15:23:49.862079: step 703, loss 0.0320977, acc 0.984375, learning_rate 0.00162358
2018-04-02T15:23:51.022689: step 704, loss 0.0057054, acc 1, learning_rate 0.00162104
2018-04-02T15:23:52.181575: step 705, loss 0.0178546, acc 0.984375, learning_rate 0.00161851
2018-04-02T15:23:53.341557: step 706, loss 0.00821103, acc 1, learning_rate 0.00161599
2018-04-02T15:23:54.501178: step 707, loss 0.0221211, acc 0.984375, learning_rate 0.00161347
2018-04-02T15:23:55.658229: step 708, loss 0.00606459, acc 1, learning_rate 0.00161095
2018-04-02T15:23:56.813769: step 709, loss 0.00605505, acc 1, learning_rate 0.00160844
2018-04-02T15:23:57.972103: step 710, loss 0.00685156, acc 1, learning_rate 0.00160593
2018-04-02T15:23:59.128436: step 711, loss 0.00643569, acc 1, learning_rate 0.00160343
2018-04-02T15:24:00.286548: step 712, loss 0.0026662, acc 1, learning_rate 0.00160093
2018-04-02T15:24:01.441822: step 713, loss 0.00433728, acc 1, learning_rate 0.00159843
2018-04-02T15:24:02.603413: step 714, loss 0.011531, acc 1, learning_rate 0.00159594
2018-04-02T15:24:03.763928: step 715, loss 0.0540491, acc 0.984375, learning_rate 0.00159345
2018-04-02T15:24:04.922657: step 716, loss 0.00324151, acc 1, learning_rate 0.00159097
2018-04-02T15:24:06.079396: step 717, loss 0.0133281, acc 1, learning_rate 0.00158849
2018-04-02T15:24:07.237052: step 718, loss 0.0347737, acc 0.984375, learning_rate 0.00158602
2018-04-02T15:24:08.392086: step 719, loss 0.00727467, acc 1, learning_rate 0.00158355
2018-04-02T15:24:09.550852: step 720, loss 0.0111602, acc 1, learning_rate 0.00158108
2018-04-02T15:24:10.707865: step 721, loss 0.0103612, acc 1, learning_rate 0.00157862
2018-04-02T15:24:11.870597: step 722, loss 0.0124787, acc 1, learning_rate 0.00157616
2018-04-02T15:24:12.981362: step 723, loss 0.000912987, acc 1, learning_rate 0.0015737
2018-04-02T15:24:14.136703: step 724, loss 0.00715946, acc 1, learning_rate 0.00157125
2018-04-02T15:24:15.292947: step 725, loss 0.0259786, acc 0.984375, learning_rate 0.00156881
2018-04-02T15:24:16.454086: step 726, loss 0.00476887, acc 1, learning_rate 0.00156637
2018-04-02T15:24:17.612690: step 727, loss 0.00402712, acc 1, learning_rate 0.00156393
2018-04-02T15:24:18.769222: step 728, loss 0.00769757, acc 1, learning_rate 0.00156149
2018-04-02T15:24:19.922545: step 729, loss 0.00454562, acc 1, learning_rate 0.00155906
2018-04-02T15:24:21.087946: step 730, loss 0.00672117, acc 1, learning_rate 0.00155664
2018-04-02T15:24:22.246581: step 731, loss 0.00277347, acc 1, learning_rate 0.00155422
2018-04-02T15:24:23.406891: step 732, loss 0.00556684, acc 1, learning_rate 0.0015518
2018-04-02T15:24:24.561077: step 733, loss 0.00505219, acc 1, learning_rate 0.00154938
2018-04-02T15:24:25.719304: step 734, loss 0.00800796, acc 1, learning_rate 0.00154697
2018-04-02T15:24:26.878873: step 735, loss 0.00585357, acc 1, learning_rate 0.00154457
2018-04-02T15:24:28.033957: step 736, loss 0.0118172, acc 1, learning_rate 0.00154217
2018-04-02T15:24:29.195175: step 737, loss 0.00226981, acc 1, learning_rate 0.00153977
2018-04-02T15:24:30.354904: step 738, loss 0.00378489, acc 1, learning_rate 0.00153737
2018-04-02T15:24:31.482086: step 739, loss 0.00466786, acc 1, learning_rate 0.00153498
2018-04-02T15:24:32.603107: step 740, loss 0.057503, acc 0.984375, learning_rate 0.0015326
2018-04-02T15:24:33.724657: step 741, loss 0.00886845, acc 1, learning_rate 0.00153022
2018-04-02T15:24:34.843772: step 742, loss 0.00667438, acc 1, learning_rate 0.00152784
2018-04-02T15:24:35.959154: step 743, loss 0.0869102, acc 0.984375, learning_rate 0.00152546
2018-04-02T15:24:37.082239: step 744, loss 0.00923798, acc 1, learning_rate 0.00152309
2018-04-02T15:24:38.200611: step 745, loss 0.00508048, acc 1, learning_rate 0.00152073
2018-04-02T15:24:39.322571: step 746, loss 0.00565504, acc 1, learning_rate 0.00151837
2018-04-02T15:24:40.444936: step 747, loss 0.00358912, acc 1, learning_rate 0.00151601
2018-04-02T15:24:41.566217: step 748, loss 0.00329617, acc 1, learning_rate 0.00151365
2018-04-02T15:24:42.684428: step 749, loss 0.0063326, acc 1, learning_rate 0.0015113
2018-04-02T15:24:43.806090: step 750, loss 0.00193002, acc 1, learning_rate 0.00150896
2018-04-02T15:24:44.929043: step 751, loss 0.00315899, acc 1, learning_rate 0.00150661
2018-04-02T15:24:46.048906: step 752, loss 0.00310334, acc 1, learning_rate 0.00150428
2018-04-02T15:24:47.173798: step 753, loss 0.0254557, acc 0.984375, learning_rate 0.00150194
2018-04-02T15:24:48.296162: step 754, loss 0.00474238, acc 1, learning_rate 0.00149961
2018-04-02T15:24:49.412896: step 755, loss 0.00195172, acc 1, learning_rate 0.00149728
2018-04-02T15:24:50.534399: step 756, loss 0.0033957, acc 1, learning_rate 0.00149496
2018-04-02T15:24:51.655571: step 757, loss 0.0040735, acc 1, learning_rate 0.00149264
2018-04-02T15:24:52.773020: step 758, loss 0.00688936, acc 1, learning_rate 0.00149032
2018-04-02T15:24:53.895873: step 759, loss 0.0125149, acc 1, learning_rate 0.00148801
2018-04-02T15:24:55.016178: step 760, loss 0.00886639, acc 1, learning_rate 0.0014857
2018-04-02T15:24:56.136898: step 761, loss 0.0280322, acc 0.984375, learning_rate 0.0014834
2018-04-02T15:24:57.252218: step 762, loss 0.00872204, acc 1, learning_rate 0.0014811
2018-04-02T15:24:58.367821: step 763, loss 0.00571721, acc 1, learning_rate 0.0014788
2018-04-02T15:24:59.488508: step 764, loss 0.00115777, acc 1, learning_rate 0.00147651
2018-04-02T15:25:00.608804: step 765, loss 0.00706849, acc 1, learning_rate 0.00147422
2018-04-02T15:25:01.737451: step 766, loss 0.00295605, acc 1, learning_rate 0.00147194
2018-04-02T15:25:02.873998: step 767, loss 0.00351292, acc 1, learning_rate 0.00146966
2018-04-02T15:25:03.995748: step 768, loss 0.000726193, acc 1, learning_rate 0.00146738
2018-04-02T15:25:05.115117: step 769, loss 0.00904894, acc 1, learning_rate 0.00146511
2018-04-02T15:25:06.238019: step 770, loss 0.00916379, acc 1, learning_rate 0.00146284
2018-04-02T15:25:07.355347: step 771, loss 0.0075666, acc 1, learning_rate 0.00146057
2018-04-02T15:25:08.476228: step 772, loss 0.0116867, acc 1, learning_rate 0.00145831
2018-04-02T15:25:09.600523: step 773, loss 0.00243904, acc 1, learning_rate 0.00145605
2018-04-02T15:25:10.720597: step 774, loss 0.00458496, acc 1, learning_rate 0.00145379
2018-04-02T15:25:11.838359: step 775, loss 0.0028618, acc 1, learning_rate 0.00145154
2018-04-02T15:25:12.960084: step 776, loss 0.0046897, acc 1, learning_rate 0.0014493
2018-04-02T15:25:14.083950: step 777, loss 0.00211733, acc 1, learning_rate 0.00144705
2018-04-02T15:25:15.206730: step 778, loss 0.0045325, acc 1, learning_rate 0.00144481
2018-04-02T15:25:16.328159: step 779, loss 0.0134425, acc 1, learning_rate 0.00144258
2018-04-02T15:25:17.451017: step 780, loss 0.040727, acc 0.96875, learning_rate 0.00144035
2018-04-02T15:25:18.570720: step 781, loss 0.00491952, acc 1, learning_rate 0.00143812
2018-04-02T15:25:19.694205: step 782, loss 0.0108556, acc 1, learning_rate 0.00143589
2018-04-02T15:25:20.816008: step 783, loss 0.00575936, acc 1, learning_rate 0.00143367
2018-04-02T15:25:21.933573: step 784, loss 0.0019602, acc 1, learning_rate 0.00143145
2018-04-02T15:25:23.086446: step 785, loss 0.00180504, acc 1, learning_rate 0.00142924
2018-04-02T15:25:24.217227: step 786, loss 0.00236077, acc 1, learning_rate 0.00142703
2018-04-02T15:25:25.339319: step 787, loss 0.00300867, acc 1, learning_rate 0.00142482
2018-04-02T15:25:26.458584: step 788, loss 0.00323097, acc 1, learning_rate 0.00142262
2018-04-02T15:25:27.585039: step 789, loss 0.00617465, acc 1, learning_rate 0.00142042
2018-04-02T15:25:28.705897: step 790, loss 0.00477917, acc 1, learning_rate 0.00141823
2018-04-02T15:25:29.827539: step 791, loss 0.00383878, acc 1, learning_rate 0.00141603
2018-04-02T15:25:30.945804: step 792, loss 0.0019861, acc 1, learning_rate 0.00141385
2018-04-02T15:25:32.065265: step 793, loss 0.00267522, acc 1, learning_rate 0.00141166
2018-04-02T15:25:33.188006: step 794, loss 0.00224726, acc 1, learning_rate 0.00140948
2018-04-02T15:25:34.307280: step 795, loss 0.00364689, acc 1, learning_rate 0.0014073
2018-04-02T15:25:35.434664: step 796, loss 0.00203, acc 1, learning_rate 0.00140513
2018-04-02T15:25:36.579084: step 797, loss 0.00199604, acc 1, learning_rate 0.00140296
2018-04-02T15:25:37.722588: step 798, loss 0.0101369, acc 1, learning_rate 0.00140079
2018-04-02T15:25:38.867519: step 799, loss 0.00537725, acc 1, learning_rate 0.00139863
2018-04-02T15:25:39.991409: step 800, loss 0.0148961, acc 0.984375, learning_rate 0.00139647

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:25:40.203794: step 800, loss 0.0420977, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-800

2018-04-02T15:25:44.346101: step 801, loss 0.00441688, acc 1, learning_rate 0.00139432
2018-04-02T15:25:45.467312: step 802, loss 0.00243429, acc 1, learning_rate 0.00139216
2018-04-02T15:25:46.601028: step 803, loss 0.00326239, acc 1, learning_rate 0.00139001
2018-04-02T15:25:47.762734: step 804, loss 0.00337513, acc 1, learning_rate 0.00138787
2018-04-02T15:25:48.879041: step 805, loss 0.00223152, acc 1, learning_rate 0.00138573
2018-04-02T15:25:49.999800: step 806, loss 0.00189924, acc 1, learning_rate 0.00138359
2018-04-02T15:25:51.120748: step 807, loss 0.0190662, acc 1, learning_rate 0.00138146
2018-04-02T15:25:52.244244: step 808, loss 0.00650884, acc 1, learning_rate 0.00137933
2018-04-02T15:25:53.356140: step 809, loss 0.00172096, acc 1, learning_rate 0.0013772
2018-04-02T15:25:54.478751: step 810, loss 0.000817868, acc 1, learning_rate 0.00137508
2018-04-02T15:25:55.594376: step 811, loss 0.00151255, acc 1, learning_rate 0.00137296
2018-04-02T15:25:56.713943: step 812, loss 0.0022707, acc 1, learning_rate 0.00137084
2018-04-02T15:25:57.820553: step 813, loss 0.00104289, acc 1, learning_rate 0.00136873
2018-04-02T15:25:58.939056: step 814, loss 0.00356755, acc 1, learning_rate 0.00136662
2018-04-02T15:26:00.058371: step 815, loss 0.0149122, acc 0.984375, learning_rate 0.00136451
2018-04-02T15:26:01.176652: step 816, loss 0.0030803, acc 1, learning_rate 0.00136241
2018-04-02T15:26:02.292995: step 817, loss 0.00573843, acc 1, learning_rate 0.00136031
2018-04-02T15:26:03.403121: step 818, loss 0.00261455, acc 1, learning_rate 0.00135821
2018-04-02T15:26:04.525396: step 819, loss 0.00539823, acc 1, learning_rate 0.00135612
2018-04-02T15:26:05.651285: step 820, loss 0.00596348, acc 1, learning_rate 0.00135403
2018-04-02T15:26:06.771131: step 821, loss 0.001943, acc 1, learning_rate 0.00135195
2018-04-02T15:26:07.893925: step 822, loss 0.00219975, acc 1, learning_rate 0.00134987
2018-04-02T15:26:09.010357: step 823, loss 0.00702831, acc 1, learning_rate 0.00134779
2018-04-02T15:26:10.132795: step 824, loss 0.0123335, acc 1, learning_rate 0.00134571
2018-04-02T15:26:11.253067: step 825, loss 0.00481502, acc 1, learning_rate 0.00134364
2018-04-02T15:26:12.375745: step 826, loss 0.00158391, acc 1, learning_rate 0.00134157
2018-04-02T15:26:13.496723: step 827, loss 0.00208388, acc 1, learning_rate 0.00133951
2018-04-02T15:26:14.616276: step 828, loss 0.00200619, acc 1, learning_rate 0.00133745
2018-04-02T15:26:15.733948: step 829, loss 0.00237228, acc 1, learning_rate 0.00133539
2018-04-02T15:26:16.854388: step 830, loss 0.00472383, acc 1, learning_rate 0.00133334
2018-04-02T15:26:17.970985: step 831, loss 0.00874272, acc 1, learning_rate 0.00133129
2018-04-02T15:26:19.084692: step 832, loss 0.00175207, acc 1, learning_rate 0.00132924
2018-04-02T15:26:20.207718: step 833, loss 0.0290704, acc 0.96875, learning_rate 0.0013272
2018-04-02T15:26:21.328397: step 834, loss 0.00591544, acc 1, learning_rate 0.00132516
2018-04-02T15:26:22.444724: step 835, loss 0.0027611, acc 1, learning_rate 0.00132312
2018-04-02T15:26:23.559197: step 836, loss 0.00305067, acc 1, learning_rate 0.00132108
2018-04-02T15:26:24.718047: step 837, loss 0.00237118, acc 1, learning_rate 0.00131905
2018-04-02T15:26:25.867996: step 838, loss 0.00135951, acc 1, learning_rate 0.00131703
2018-04-02T15:26:26.989307: step 839, loss 0.00342432, acc 1, learning_rate 0.001315
2018-04-02T15:26:28.109089: step 840, loss 0.00124546, acc 1, learning_rate 0.00131298
2018-04-02T15:26:29.232168: step 841, loss 0.0908355, acc 0.984375, learning_rate 0.00131097
2018-04-02T15:26:30.355428: step 842, loss 0.00514296, acc 1, learning_rate 0.00130895
2018-04-02T15:26:31.476018: step 843, loss 0.00358027, acc 1, learning_rate 0.00130694
2018-04-02T15:26:32.593449: step 844, loss 0.00890157, acc 1, learning_rate 0.00130494
2018-04-02T15:26:33.716770: step 845, loss 0.00274362, acc 1, learning_rate 0.00130293
2018-04-02T15:26:34.844483: step 846, loss 0.0135072, acc 0.984375, learning_rate 0.00130093
2018-04-02T15:26:35.976838: step 847, loss 0.0121001, acc 1, learning_rate 0.00129894
2018-04-02T15:26:37.099887: step 848, loss 0.00170543, acc 1, learning_rate 0.00129694
2018-04-02T15:26:38.235346: step 849, loss 0.0715249, acc 0.984375, learning_rate 0.00129495
2018-04-02T15:26:39.387341: step 850, loss 0.00347522, acc 1, learning_rate 0.00129297
2018-04-02T15:26:40.540017: step 851, loss 0.00447565, acc 1, learning_rate 0.00129098
2018-04-02T15:26:41.682658: step 852, loss 0.0208037, acc 0.984375, learning_rate 0.001289
2018-04-02T15:26:42.804369: step 853, loss 0.00313654, acc 1, learning_rate 0.00128703
2018-04-02T15:26:43.934879: step 854, loss 0.00687913, acc 1, learning_rate 0.00128505
2018-04-02T15:26:45.052542: step 855, loss 0.00311071, acc 1, learning_rate 0.00128308
2018-04-02T15:26:46.174458: step 856, loss 0.00451909, acc 1, learning_rate 0.00128111
2018-04-02T15:26:47.292854: step 857, loss 0.00403631, acc 1, learning_rate 0.00127915
2018-04-02T15:26:48.412015: step 858, loss 0.0019237, acc 1, learning_rate 0.00127719
2018-04-02T15:26:49.532211: step 859, loss 0.00831144, acc 1, learning_rate 0.00127523
2018-04-02T15:26:50.656959: step 860, loss 0.0702394, acc 0.984375, learning_rate 0.00127328
2018-04-02T15:26:51.778099: step 861, loss 0.00411561, acc 1, learning_rate 0.00127133
2018-04-02T15:26:52.898477: step 862, loss 0.00119096, acc 1, learning_rate 0.00126938
2018-04-02T15:26:54.015042: step 863, loss 0.00385979, acc 1, learning_rate 0.00126744
2018-04-02T15:26:55.134096: step 864, loss 0.00200162, acc 1, learning_rate 0.00126549
2018-04-02T15:26:56.253709: step 865, loss 0.00510222, acc 1, learning_rate 0.00126356
2018-04-02T15:26:57.379016: step 866, loss 0.0037403, acc 1, learning_rate 0.00126162
2018-04-02T15:26:58.527928: step 867, loss 0.00823107, acc 1, learning_rate 0.00125969
2018-04-02T15:26:59.653417: step 868, loss 0.00790967, acc 1, learning_rate 0.00125776
2018-04-02T15:27:00.780389: step 869, loss 0.00337034, acc 1, learning_rate 0.00125584
2018-04-02T15:27:01.902802: step 870, loss 0.0043086, acc 1, learning_rate 0.00125392
2018-04-02T15:27:03.015921: step 871, loss 0.00611004, acc 1, learning_rate 0.001252
2018-04-02T15:27:04.139369: step 872, loss 0.0258203, acc 0.984375, learning_rate 0.00125008
2018-04-02T15:27:05.265546: step 873, loss 0.00695559, acc 1, learning_rate 0.00124817
2018-04-02T15:27:06.400847: step 874, loss 0.00902325, acc 1, learning_rate 0.00124626
2018-04-02T15:27:07.563013: step 875, loss 0.00414681, acc 1, learning_rate 0.00124435
2018-04-02T15:27:08.699491: step 876, loss 0.00284604, acc 1, learning_rate 0.00124245
2018-04-02T15:27:09.821034: step 877, loss 0.00268616, acc 1, learning_rate 0.00124055
2018-04-02T15:27:10.939573: step 878, loss 0.00204783, acc 1, learning_rate 0.00123866
2018-04-02T15:27:12.092859: step 879, loss 0.00197212, acc 1, learning_rate 0.00123676
2018-04-02T15:27:13.216575: step 880, loss 0.000184551, acc 1, learning_rate 0.00123487
2018-04-02T15:27:14.336046: step 881, loss 0.00313765, acc 1, learning_rate 0.00123299
2018-04-02T15:27:15.476287: step 882, loss 0.00170618, acc 1, learning_rate 0.0012311
2018-04-02T15:27:16.597253: step 883, loss 0.00294327, acc 1, learning_rate 0.00122922
2018-04-02T15:27:17.720432: step 884, loss 0.00664584, acc 1, learning_rate 0.00122734
2018-04-02T15:27:18.834763: step 885, loss 0.00341234, acc 1, learning_rate 0.00122547
2018-04-02T15:27:19.955426: step 886, loss 0.00809271, acc 1, learning_rate 0.0012236
2018-04-02T15:27:21.075963: step 887, loss 0.0110447, acc 1, learning_rate 0.00122173
2018-04-02T15:27:22.192033: step 888, loss 0.00196431, acc 1, learning_rate 0.00121987
2018-04-02T15:27:23.313015: step 889, loss 0.00247734, acc 1, learning_rate 0.001218
2018-04-02T15:27:24.433385: step 890, loss 0.00161271, acc 1, learning_rate 0.00121614
2018-04-02T15:27:25.557561: step 891, loss 0.00112328, acc 1, learning_rate 0.00121429
2018-04-02T15:27:26.675704: step 892, loss 0.00152605, acc 1, learning_rate 0.00121244
2018-04-02T15:27:27.792728: step 893, loss 0.051854, acc 0.984375, learning_rate 0.00121059
2018-04-02T15:27:28.915161: step 894, loss 0.00445043, acc 1, learning_rate 0.00120874
2018-04-02T15:27:30.035668: step 895, loss 0.00687614, acc 1, learning_rate 0.0012069
2018-04-02T15:27:31.153508: step 896, loss 0.017659, acc 0.984375, learning_rate 0.00120506
2018-04-02T15:27:32.277755: step 897, loss 0.0123707, acc 1, learning_rate 0.00120322
2018-04-02T15:27:33.398017: step 898, loss 0.00761451, acc 1, learning_rate 0.00120138
2018-04-02T15:27:34.518511: step 899, loss 0.00380223, acc 1, learning_rate 0.00119955
2018-04-02T15:27:35.638845: step 900, loss 0.00568477, acc 1, learning_rate 0.00119772

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:27:35.850729: step 900, loss 0.0399021, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-900

2018-04-02T15:27:40.292422: step 901, loss 0.00963517, acc 1, learning_rate 0.0011959
2018-04-02T15:27:41.408591: step 902, loss 0.00220329, acc 1, learning_rate 0.00119408
2018-04-02T15:27:42.528265: step 903, loss 0.00259691, acc 1, learning_rate 0.00119226
2018-04-02T15:27:43.685761: step 904, loss 0.00272137, acc 1, learning_rate 0.00119044
2018-04-02T15:27:44.846237: step 905, loss 0.00256999, acc 1, learning_rate 0.00118863
2018-04-02T15:27:45.966245: step 906, loss 0.00222988, acc 1, learning_rate 0.00118682
2018-04-02T15:27:47.088807: step 907, loss 0.0089877, acc 1, learning_rate 0.00118501
2018-04-02T15:27:48.211716: step 908, loss 0.00945314, acc 1, learning_rate 0.00118321
2018-04-02T15:27:49.333773: step 909, loss 0.056137, acc 0.984375, learning_rate 0.00118141
2018-04-02T15:27:50.450255: step 910, loss 0.003365, acc 1, learning_rate 0.00117961
2018-04-02T15:27:51.568900: step 911, loss 0.00241376, acc 1, learning_rate 0.00117781
2018-04-02T15:27:52.685989: step 912, loss 0.0103791, acc 1, learning_rate 0.00117602
2018-04-02T15:27:53.808036: step 913, loss 0.00100136, acc 1, learning_rate 0.00117423
2018-04-02T15:27:54.923657: step 914, loss 0.00687625, acc 1, learning_rate 0.00117245
2018-04-02T15:27:56.040636: step 915, loss 0.00397774, acc 1, learning_rate 0.00117066
2018-04-02T15:27:57.161629: step 916, loss 0.051275, acc 0.984375, learning_rate 0.00116888
2018-04-02T15:27:58.282854: step 917, loss 0.00146072, acc 1, learning_rate 0.00116711
2018-04-02T15:27:59.395671: step 918, loss 0.00288752, acc 1, learning_rate 0.00116533
2018-04-02T15:28:00.516514: step 919, loss 0.0124111, acc 1, learning_rate 0.00116356
2018-04-02T15:28:01.643472: step 920, loss 0.00594735, acc 1, learning_rate 0.00116179
2018-04-02T15:28:02.770024: step 921, loss 0.00231894, acc 1, learning_rate 0.00116003
2018-04-02T15:28:03.891216: step 922, loss 0.00277208, acc 1, learning_rate 0.00115826
2018-04-02T15:28:05.012003: step 923, loss 0.00298709, acc 1, learning_rate 0.0011565
2018-04-02T15:28:06.131511: step 924, loss 0.00327604, acc 1, learning_rate 0.00115475
2018-04-02T15:28:07.250089: step 925, loss 0.00487691, acc 1, learning_rate 0.00115299
2018-04-02T15:28:08.369499: step 926, loss 0.0297025, acc 0.984375, learning_rate 0.00115124
2018-04-02T15:28:09.494713: step 927, loss 0.0150237, acc 0.984375, learning_rate 0.0011495
2018-04-02T15:28:10.608011: step 928, loss 0.051134, acc 0.984375, learning_rate 0.00114775
2018-04-02T15:28:11.738297: step 929, loss 0.00957494, acc 1, learning_rate 0.00114601
2018-04-02T15:28:12.861936: step 930, loss 0.00203268, acc 1, learning_rate 0.00114427
2018-04-02T15:28:13.981721: step 931, loss 0.00178143, acc 1, learning_rate 0.00114253
2018-04-02T15:28:15.108511: step 932, loss 0.0693278, acc 0.984375, learning_rate 0.0011408
2018-04-02T15:28:16.231923: step 933, loss 0.00448301, acc 1, learning_rate 0.00113907
2018-04-02T15:28:17.347794: step 934, loss 0.0024784, acc 1, learning_rate 0.00113734
2018-04-02T15:28:18.463094: step 935, loss 0.00378747, acc 1, learning_rate 0.00113562
2018-04-02T15:28:19.584245: step 936, loss 0.00245569, acc 1, learning_rate 0.00113389
2018-04-02T15:28:20.704663: step 937, loss 0.00584942, acc 1, learning_rate 0.00113218
2018-04-02T15:28:21.825854: step 938, loss 0.00601047, acc 1, learning_rate 0.00113046
2018-04-02T15:28:22.944724: step 939, loss 0.00189573, acc 1, learning_rate 0.00112875
2018-04-02T15:28:24.064421: step 940, loss 0.00313336, acc 1, learning_rate 0.00112704
2018-04-02T15:28:25.186755: step 941, loss 0.00709536, acc 1, learning_rate 0.00112533
2018-04-02T15:28:26.305500: step 942, loss 0.00127448, acc 1, learning_rate 0.00112362
2018-04-02T15:28:27.427260: step 943, loss 0.00103014, acc 1, learning_rate 0.00112192
2018-04-02T15:28:28.547078: step 944, loss 0.00244685, acc 1, learning_rate 0.00112022
2018-04-02T15:28:29.665103: step 945, loss 0.0020412, acc 1, learning_rate 0.00111853
2018-04-02T15:28:30.785517: step 946, loss 0.00360635, acc 1, learning_rate 0.00111683
2018-04-02T15:28:31.901692: step 947, loss 0.00318337, acc 1, learning_rate 0.00111514
2018-04-02T15:28:33.019186: step 948, loss 0.00244283, acc 1, learning_rate 0.00111345
2018-04-02T15:28:34.129583: step 949, loss 0.00441489, acc 1, learning_rate 0.00111177
2018-04-02T15:28:35.251840: step 950, loss 0.00341482, acc 1, learning_rate 0.00111009
2018-04-02T15:28:36.373098: step 951, loss 0.00305623, acc 1, learning_rate 0.00110841
2018-04-02T15:28:37.493133: step 952, loss 0.00208009, acc 1, learning_rate 0.00110673
2018-04-02T15:28:38.604396: step 953, loss 0.00183519, acc 1, learning_rate 0.00110506
2018-04-02T15:28:39.720496: step 954, loss 0.0124884, acc 1, learning_rate 0.00110339
2018-04-02T15:28:40.835820: step 955, loss 0.0263078, acc 0.984375, learning_rate 0.00110172
2018-04-02T15:28:41.959208: step 956, loss 0.00468708, acc 1, learning_rate 0.00110005
2018-04-02T15:28:43.076676: step 957, loss 0.00399672, acc 1, learning_rate 0.00109839
2018-04-02T15:28:44.195506: step 958, loss 0.0292401, acc 0.984375, learning_rate 0.00109673
2018-04-02T15:28:45.312756: step 959, loss 0.00265307, acc 1, learning_rate 0.00109507
2018-04-02T15:28:46.435111: step 960, loss 0.00164163, acc 1, learning_rate 0.00109342
2018-04-02T15:28:47.553899: step 961, loss 0.00392921, acc 1, learning_rate 0.00109177
2018-04-02T15:28:48.674377: step 962, loss 0.00163163, acc 1, learning_rate 0.00109012
2018-04-02T15:28:49.795576: step 963, loss 0.00127095, acc 1, learning_rate 0.00108847
2018-04-02T15:28:50.865294: step 964, loss 0.00391083, acc 1, learning_rate 0.00108683
2018-04-02T15:28:51.992799: step 965, loss 0.00128587, acc 1, learning_rate 0.00108519
2018-04-02T15:28:53.114686: step 966, loss 0.0342886, acc 0.984375, learning_rate 0.00108355
2018-04-02T15:28:54.234846: step 967, loss 0.00363052, acc 1, learning_rate 0.00108191
2018-04-02T15:28:55.354812: step 968, loss 0.0018747, acc 1, learning_rate 0.00108028
2018-04-02T15:28:56.473806: step 969, loss 0.0196748, acc 0.984375, learning_rate 0.00107865
2018-04-02T15:28:57.597362: step 970, loss 0.00386322, acc 1, learning_rate 0.00107702
2018-04-02T15:28:58.715600: step 971, loss 0.00235993, acc 1, learning_rate 0.0010754
2018-04-02T15:28:59.836710: step 972, loss 0.00153191, acc 1, learning_rate 0.00107378
2018-04-02T15:29:00.956262: step 973, loss 0.00299047, acc 1, learning_rate 0.00107216
2018-04-02T15:29:02.072761: step 974, loss 0.00210207, acc 1, learning_rate 0.00107054
2018-04-02T15:29:03.192827: step 975, loss 0.00175728, acc 1, learning_rate 0.00106893
2018-04-02T15:29:04.315438: step 976, loss 0.00247139, acc 1, learning_rate 0.00106732
2018-04-02T15:29:05.430704: step 977, loss 0.00169297, acc 1, learning_rate 0.00106571
2018-04-02T15:29:06.542932: step 978, loss 0.00267052, acc 1, learning_rate 0.0010641
2018-04-02T15:29:07.662347: step 979, loss 0.0022689, acc 1, learning_rate 0.0010625
2018-04-02T15:29:08.780129: step 980, loss 0.00361753, acc 1, learning_rate 0.0010609
2018-04-02T15:29:09.904037: step 981, loss 0.00151318, acc 1, learning_rate 0.0010593
2018-04-02T15:29:11.022732: step 982, loss 0.0101471, acc 1, learning_rate 0.00105771
2018-04-02T15:29:12.145798: step 983, loss 0.00252615, acc 1, learning_rate 0.00105611
2018-04-02T15:29:13.266612: step 984, loss 0.0489228, acc 0.984375, learning_rate 0.00105452
2018-04-02T15:29:14.386397: step 985, loss 0.0685565, acc 0.984375, learning_rate 0.00105294
2018-04-02T15:29:15.508909: step 986, loss 0.00290934, acc 1, learning_rate 0.00105135
2018-04-02T15:29:16.624328: step 987, loss 0.00226507, acc 1, learning_rate 0.00104977
2018-04-02T15:29:17.747688: step 988, loss 0.0540245, acc 0.984375, learning_rate 0.00104819
2018-04-02T15:29:18.867862: step 989, loss 0.00190871, acc 1, learning_rate 0.00104662
2018-04-02T15:29:19.990314: step 990, loss 0.0241545, acc 0.984375, learning_rate 0.00104504
2018-04-02T15:29:21.115369: step 991, loss 0.00221826, acc 1, learning_rate 0.00104347
2018-04-02T15:29:22.237884: step 992, loss 0.010874, acc 1, learning_rate 0.0010419
2018-04-02T15:29:23.360729: step 993, loss 0.00539124, acc 1, learning_rate 0.00104034
2018-04-02T15:29:24.491749: step 994, loss 0.00154096, acc 1, learning_rate 0.00103877
2018-04-02T15:29:25.617198: step 995, loss 0.00468447, acc 1, learning_rate 0.00103721
2018-04-02T15:29:26.737219: step 996, loss 0.00453659, acc 1, learning_rate 0.00103565
2018-04-02T15:29:27.854245: step 997, loss 0.00595352, acc 1, learning_rate 0.0010341
2018-04-02T15:29:28.973576: step 998, loss 0.00402755, acc 1, learning_rate 0.00103254
2018-04-02T15:29:30.093502: step 999, loss 0.00383507, acc 1, learning_rate 0.00103099
2018-04-02T15:29:31.216164: step 1000, loss 0.00745738, acc 1, learning_rate 0.00102945

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:29:31.427222: step 1000, loss 0.0451472, acc 0.983931

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1000

2018-04-02T15:29:35.573883: step 1001, loss 0.00360558, acc 1, learning_rate 0.0010279
2018-04-02T15:29:36.694807: step 1002, loss 0.00298234, acc 1, learning_rate 0.00102636
2018-04-02T15:29:37.813922: step 1003, loss 0.00178857, acc 1, learning_rate 0.00102482
2018-04-02T15:29:38.934422: step 1004, loss 0.00157419, acc 1, learning_rate 0.00102328
2018-04-02T15:29:40.087504: step 1005, loss 0.00933926, acc 1, learning_rate 0.00102174
2018-04-02T15:29:41.209090: step 1006, loss 0.00270303, acc 1, learning_rate 0.00102021
2018-04-02T15:29:42.330842: step 1007, loss 0.0026876, acc 1, learning_rate 0.00101868
2018-04-02T15:29:43.449393: step 1008, loss 0.00273249, acc 1, learning_rate 0.00101715
2018-04-02T15:29:44.570609: step 1009, loss 0.00359578, acc 1, learning_rate 0.00101563
2018-04-02T15:29:45.684761: step 1010, loss 0.0013032, acc 1, learning_rate 0.00101411
2018-04-02T15:29:46.804708: step 1011, loss 0.0139797, acc 0.984375, learning_rate 0.00101259
2018-04-02T15:29:47.926561: step 1012, loss 0.00531793, acc 1, learning_rate 0.00101107
2018-04-02T15:29:49.046995: step 1013, loss 0.00229769, acc 1, learning_rate 0.00100955
2018-04-02T15:29:50.171017: step 1014, loss 0.00128057, acc 1, learning_rate 0.00100804
2018-04-02T15:29:51.291096: step 1015, loss 0.00260421, acc 1, learning_rate 0.00100653
2018-04-02T15:29:52.413755: step 1016, loss 0.0254951, acc 0.984375, learning_rate 0.00100503
2018-04-02T15:29:53.535116: step 1017, loss 0.00526754, acc 1, learning_rate 0.00100352
2018-04-02T15:29:54.659518: step 1018, loss 0.00193611, acc 1, learning_rate 0.00100202
2018-04-02T15:29:55.770961: step 1019, loss 0.0017696, acc 1, learning_rate 0.00100052
2018-04-02T15:29:56.889344: step 1020, loss 0.0764043, acc 0.984375, learning_rate 0.000999021
2018-04-02T15:29:58.012755: step 1021, loss 0.0093812, acc 1, learning_rate 0.000997526
2018-04-02T15:29:59.135642: step 1022, loss 0.00410413, acc 1, learning_rate 0.000996034
2018-04-02T15:30:00.247796: step 1023, loss 0.00174475, acc 1, learning_rate 0.000994544
2018-04-02T15:30:01.369317: step 1024, loss 0.00181119, acc 1, learning_rate 0.000993057
2018-04-02T15:30:02.490436: step 1025, loss 0.00262952, acc 1, learning_rate 0.000991572
2018-04-02T15:30:03.612226: step 1026, loss 0.00760879, acc 1, learning_rate 0.00099009
2018-04-02T15:30:04.726646: step 1027, loss 0.00102786, acc 1, learning_rate 0.00098861
2018-04-02T15:30:05.849489: step 1028, loss 0.0013289, acc 1, learning_rate 0.000987132
2018-04-02T15:30:06.969538: step 1029, loss 0.00619783, acc 1, learning_rate 0.000985657
2018-04-02T15:30:08.088812: step 1030, loss 0.017543, acc 0.984375, learning_rate 0.000984185
2018-04-02T15:30:09.209391: step 1031, loss 0.00395929, acc 1, learning_rate 0.000982715
2018-04-02T15:30:10.330766: step 1032, loss 0.0039745, acc 1, learning_rate 0.000981247
2018-04-02T15:30:11.453324: step 1033, loss 0.00130047, acc 1, learning_rate 0.000979782
2018-04-02T15:30:12.575646: step 1034, loss 0.00287893, acc 1, learning_rate 0.000978319
2018-04-02T15:30:13.697065: step 1035, loss 0.00262739, acc 1, learning_rate 0.000976859
2018-04-02T15:30:14.817735: step 1036, loss 0.00276892, acc 1, learning_rate 0.000975401
2018-04-02T15:30:15.932369: step 1037, loss 0.0013267, acc 1, learning_rate 0.000973945
2018-04-02T15:30:17.049889: step 1038, loss 0.00289757, acc 1, learning_rate 0.000972492
2018-04-02T15:30:18.173900: step 1039, loss 0.0147238, acc 0.984375, learning_rate 0.000971041
2018-04-02T15:30:19.294774: step 1040, loss 0.00584739, acc 1, learning_rate 0.000969593
2018-04-02T15:30:20.415929: step 1041, loss 0.0024263, acc 1, learning_rate 0.000968147
2018-04-02T15:30:21.535597: step 1042, loss 0.00180181, acc 1, learning_rate 0.000966704
2018-04-02T15:30:22.657935: step 1043, loss 0.00141868, acc 1, learning_rate 0.000965263
2018-04-02T15:30:23.780719: step 1044, loss 0.000752858, acc 1, learning_rate 0.000963824
2018-04-02T15:30:24.900358: step 1045, loss 0.00914568, acc 1, learning_rate 0.000962388
2018-04-02T15:30:26.019470: step 1046, loss 0.00225752, acc 1, learning_rate 0.000960954
2018-04-02T15:30:27.143006: step 1047, loss 0.00358081, acc 1, learning_rate 0.000959522
2018-04-02T15:30:28.261991: step 1048, loss 0.00460422, acc 1, learning_rate 0.000958093
2018-04-02T15:30:29.380757: step 1049, loss 0.00242507, acc 1, learning_rate 0.000956667
2018-04-02T15:30:30.494157: step 1050, loss 0.00276914, acc 1, learning_rate 0.000955242
2018-04-02T15:30:31.614424: step 1051, loss 0.00101727, acc 1, learning_rate 0.00095382
2018-04-02T15:30:32.734229: step 1052, loss 0.00357404, acc 1, learning_rate 0.000952401
2018-04-02T15:30:33.852755: step 1053, loss 0.0178639, acc 0.984375, learning_rate 0.000950983
2018-04-02T15:30:34.969649: step 1054, loss 0.0019963, acc 1, learning_rate 0.000949568
2018-04-02T15:30:36.089285: step 1055, loss 0.00655391, acc 1, learning_rate 0.000948156
2018-04-02T15:30:37.209576: step 1056, loss 0.00505523, acc 1, learning_rate 0.000946746
2018-04-02T15:30:38.329875: step 1057, loss 0.00125975, acc 1, learning_rate 0.000945338
2018-04-02T15:30:39.441568: step 1058, loss 0.00221056, acc 1, learning_rate 0.000943932
2018-04-02T15:30:40.562184: step 1059, loss 0.00240124, acc 1, learning_rate 0.000942529
2018-04-02T15:30:41.680069: step 1060, loss 0.000423243, acc 1, learning_rate 0.000941128
2018-04-02T15:30:42.790073: step 1061, loss 0.000800453, acc 1, learning_rate 0.00093973
2018-04-02T15:30:43.906638: step 1062, loss 0.0025322, acc 1, learning_rate 0.000938333
2018-04-02T15:30:45.026470: step 1063, loss 0.000975544, acc 1, learning_rate 0.00093694
2018-04-02T15:30:46.146803: step 1064, loss 0.0672985, acc 0.984375, learning_rate 0.000935548
2018-04-02T15:30:47.266132: step 1065, loss 0.00034608, acc 1, learning_rate 0.000934159
2018-04-02T15:30:48.375890: step 1066, loss 0.0040628, acc 1, learning_rate 0.000932772
2018-04-02T15:30:49.494988: step 1067, loss 0.0030758, acc 1, learning_rate 0.000931387
2018-04-02T15:30:50.615079: step 1068, loss 0.0107044, acc 1, learning_rate 0.000930005
2018-04-02T15:30:51.731201: step 1069, loss 0.0225169, acc 0.984375, learning_rate 0.000928625
2018-04-02T15:30:52.852593: step 1070, loss 0.00178212, acc 1, learning_rate 0.000927247
2018-04-02T15:30:53.968032: step 1071, loss 0.0025752, acc 1, learning_rate 0.000925872
2018-04-02T15:30:55.091028: step 1072, loss 0.00119096, acc 1, learning_rate 0.000924498
2018-04-02T15:30:56.210380: step 1073, loss 0.00384849, acc 1, learning_rate 0.000923128
2018-04-02T15:30:57.330622: step 1074, loss 0.00294524, acc 1, learning_rate 0.000921759
2018-04-02T15:30:58.449971: step 1075, loss 0.000951466, acc 1, learning_rate 0.000920393
2018-04-02T15:30:59.567852: step 1076, loss 0.00887072, acc 1, learning_rate 0.000919029
2018-04-02T15:31:00.689163: step 1077, loss 0.00127328, acc 1, learning_rate 0.000917667
2018-04-02T15:31:01.809856: step 1078, loss 0.106436, acc 0.984375, learning_rate 0.000916307
2018-04-02T15:31:02.930298: step 1079, loss 0.000515305, acc 1, learning_rate 0.00091495
2018-04-02T15:31:04.048911: step 1080, loss 0.00232566, acc 1, learning_rate 0.000913595
2018-04-02T15:31:05.167955: step 1081, loss 0.00204381, acc 1, learning_rate 0.000912242
2018-04-02T15:31:06.286769: step 1082, loss 0.00238754, acc 1, learning_rate 0.000910892
2018-04-02T15:31:07.405100: step 1083, loss 0.00597668, acc 1, learning_rate 0.000909543
2018-04-02T15:31:08.524451: step 1084, loss 0.00185524, acc 1, learning_rate 0.000908197
2018-04-02T15:31:09.647296: step 1085, loss 0.00261357, acc 1, learning_rate 0.000906854
2018-04-02T15:31:10.768501: step 1086, loss 0.000383785, acc 1, learning_rate 0.000905512
2018-04-02T15:31:11.888814: step 1087, loss 0.0135781, acc 1, learning_rate 0.000904173
2018-04-02T15:31:13.008907: step 1088, loss 0.00102499, acc 1, learning_rate 0.000902836
2018-04-02T15:31:14.129605: step 1089, loss 0.00162593, acc 1, learning_rate 0.000901501
2018-04-02T15:31:15.253166: step 1090, loss 0.0119347, acc 1, learning_rate 0.000900168
2018-04-02T15:31:16.367468: step 1091, loss 0.00364357, acc 1, learning_rate 0.000898838
2018-04-02T15:31:17.490883: step 1092, loss 0.00388013, acc 1, learning_rate 0.00089751
2018-04-02T15:31:18.602570: step 1093, loss 0.00202407, acc 1, learning_rate 0.000896184
2018-04-02T15:31:19.720330: step 1094, loss 0.00139715, acc 1, learning_rate 0.00089486
2018-04-02T15:31:20.839078: step 1095, loss 0.00348078, acc 1, learning_rate 0.000893538
2018-04-02T15:31:21.956299: step 1096, loss 0.00613712, acc 1, learning_rate 0.000892219
2018-04-02T15:31:23.076485: step 1097, loss 0.00335335, acc 1, learning_rate 0.000890902
2018-04-02T15:31:24.198855: step 1098, loss 0.00173506, acc 1, learning_rate 0.000889587
2018-04-02T15:31:25.320816: step 1099, loss 0.00452218, acc 1, learning_rate 0.000888274
2018-04-02T15:31:26.444093: step 1100, loss 0.00239991, acc 1, learning_rate 0.000886963

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:31:26.654882: step 1100, loss 0.0391693, acc 0.985167

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1100

2018-04-02T15:31:31.340633: step 1101, loss 0.0487605, acc 0.96875, learning_rate 0.000885655
2018-04-02T15:31:32.500926: step 1102, loss 0.00116076, acc 1, learning_rate 0.000884348
2018-04-02T15:31:33.622920: step 1103, loss 0.00136798, acc 1, learning_rate 0.000883044
2018-04-02T15:31:34.745814: step 1104, loss 0.0130563, acc 1, learning_rate 0.000881742
2018-04-02T15:31:35.865733: step 1105, loss 0.00170592, acc 1, learning_rate 0.000880442
2018-04-02T15:31:36.985827: step 1106, loss 0.00277455, acc 1, learning_rate 0.000879145
2018-04-02T15:31:38.108233: step 1107, loss 0.004897, acc 1, learning_rate 0.000877849
2018-04-02T15:31:39.226068: step 1108, loss 0.00445165, acc 1, learning_rate 0.000876556
2018-04-02T15:31:40.344433: step 1109, loss 0.00179242, acc 1, learning_rate 0.000875265
2018-04-02T15:31:41.459994: step 1110, loss 0.0108522, acc 1, learning_rate 0.000873976
2018-04-02T15:31:42.581738: step 1111, loss 0.0398849, acc 0.984375, learning_rate 0.000872689
2018-04-02T15:31:43.703398: step 1112, loss 0.00176127, acc 1, learning_rate 0.000871404
2018-04-02T15:31:44.815274: step 1113, loss 0.00214482, acc 1, learning_rate 0.000870122
2018-04-02T15:31:45.934181: step 1114, loss 0.000677698, acc 1, learning_rate 0.000868841
2018-04-02T15:31:47.055731: step 1115, loss 0.00137532, acc 1, learning_rate 0.000867563
2018-04-02T15:31:48.170490: step 1116, loss 0.00232904, acc 1, learning_rate 0.000866287
2018-04-02T15:31:49.292406: step 1117, loss 0.077921, acc 0.984375, learning_rate 0.000865013
2018-04-02T15:31:50.408732: step 1118, loss 0.0647971, acc 0.984375, learning_rate 0.000863741
2018-04-02T15:31:51.529909: step 1119, loss 0.00202298, acc 1, learning_rate 0.000862471
2018-04-02T15:31:52.648949: step 1120, loss 0.00216261, acc 1, learning_rate 0.000861203
2018-04-02T15:31:53.768353: step 1121, loss 0.00133375, acc 1, learning_rate 0.000859937
2018-04-02T15:31:54.888442: step 1122, loss 0.00471389, acc 1, learning_rate 0.000858674
2018-04-02T15:31:56.008461: step 1123, loss 0.00134444, acc 1, learning_rate 0.000857412
2018-04-02T15:31:57.132737: step 1124, loss 0.00297234, acc 1, learning_rate 0.000856153
2018-04-02T15:31:58.253471: step 1125, loss 0.00171305, acc 1, learning_rate 0.000854896
2018-04-02T15:31:59.371882: step 1126, loss 0.00605292, acc 1, learning_rate 0.000853641
2018-04-02T15:32:00.492264: step 1127, loss 0.00427926, acc 1, learning_rate 0.000852388
2018-04-02T15:32:01.604201: step 1128, loss 0.00337843, acc 1, learning_rate 0.000851137
2018-04-02T15:32:02.760088: step 1129, loss 0.000909239, acc 1, learning_rate 0.000849888
2018-04-02T15:32:03.889641: step 1130, loss 0.00207125, acc 1, learning_rate 0.000848641
2018-04-02T15:32:05.007461: step 1131, loss 0.00379303, acc 1, learning_rate 0.000847396
2018-04-02T15:32:06.125407: step 1132, loss 0.00181999, acc 1, learning_rate 0.000846153
2018-04-02T15:32:07.245347: step 1133, loss 0.00567647, acc 1, learning_rate 0.000844913
2018-04-02T15:32:08.363921: step 1134, loss 0.00572763, acc 1, learning_rate 0.000843674
2018-04-02T15:32:09.489683: step 1135, loss 0.00266419, acc 1, learning_rate 0.000842438
2018-04-02T15:32:10.613984: step 1136, loss 0.00141392, acc 1, learning_rate 0.000841203
2018-04-02T15:32:11.745717: step 1137, loss 0.00175586, acc 1, learning_rate 0.000839971
2018-04-02T15:32:12.878128: step 1138, loss 0.000571908, acc 1, learning_rate 0.000838741
2018-04-02T15:32:14.000901: step 1139, loss 0.00653458, acc 1, learning_rate 0.000837512
2018-04-02T15:32:15.131008: step 1140, loss 0.00250604, acc 1, learning_rate 0.000836286
2018-04-02T15:32:16.248337: step 1141, loss 0.00300518, acc 1, learning_rate 0.000835062
2018-04-02T15:32:17.373009: step 1142, loss 0.0094146, acc 1, learning_rate 0.00083384
2018-04-02T15:32:18.493603: step 1143, loss 0.00723455, acc 1, learning_rate 0.00083262
2018-04-02T15:32:19.615323: step 1144, loss 0.0346751, acc 0.984375, learning_rate 0.000831401
2018-04-02T15:32:20.738199: step 1145, loss 0.0028639, acc 1, learning_rate 0.000830185
2018-04-02T15:32:21.858350: step 1146, loss 0.0117463, acc 1, learning_rate 0.000828971
2018-04-02T15:32:22.977761: step 1147, loss 0.00319005, acc 1, learning_rate 0.000827759
2018-04-02T15:32:24.098894: step 1148, loss 0.00239837, acc 1, learning_rate 0.000826549
2018-04-02T15:32:25.213070: step 1149, loss 0.00150256, acc 1, learning_rate 0.000825341
2018-04-02T15:32:26.331453: step 1150, loss 0.00334694, acc 1, learning_rate 0.000824135
2018-04-02T15:32:27.450701: step 1151, loss 0.00197598, acc 1, learning_rate 0.000822931
2018-04-02T15:32:28.570034: step 1152, loss 0.00190871, acc 1, learning_rate 0.000821729
2018-04-02T15:32:29.688197: step 1153, loss 0.00171759, acc 1, learning_rate 0.000820529
2018-04-02T15:32:30.807048: step 1154, loss 0.0028168, acc 1, learning_rate 0.000819331
2018-04-02T15:32:31.926719: step 1155, loss 0.0394955, acc 0.984375, learning_rate 0.000818135
2018-04-02T15:32:33.044558: step 1156, loss 0.00913014, acc 1, learning_rate 0.000816941
2018-04-02T15:32:34.158356: step 1157, loss 0.00260927, acc 1, learning_rate 0.000815749
2018-04-02T15:32:35.278245: step 1158, loss 0.00391504, acc 1, learning_rate 0.000814559
2018-04-02T15:32:36.398536: step 1159, loss 0.00755108, acc 1, learning_rate 0.000813371
2018-04-02T15:32:37.521598: step 1160, loss 0.00310063, acc 1, learning_rate 0.000812185
2018-04-02T15:32:38.630160: step 1161, loss 0.096646, acc 0.984375, learning_rate 0.000811001
2018-04-02T15:32:39.750070: step 1162, loss 0.00428466, acc 1, learning_rate 0.000809818
2018-04-02T15:32:40.867786: step 1163, loss 0.000687529, acc 1, learning_rate 0.000808638
2018-04-02T15:32:41.988771: step 1164, loss 0.00142207, acc 1, learning_rate 0.00080746
2018-04-02T15:32:43.106361: step 1165, loss 0.00252078, acc 1, learning_rate 0.000806284
2018-04-02T15:32:44.227680: step 1166, loss 0.00201081, acc 1, learning_rate 0.000805109
2018-04-02T15:32:45.345140: step 1167, loss 0.00207548, acc 1, learning_rate 0.000803937
2018-04-02T15:32:46.460329: step 1168, loss 0.00161156, acc 1, learning_rate 0.000802767
2018-04-02T15:32:47.577689: step 1169, loss 0.000880596, acc 1, learning_rate 0.000801598
2018-04-02T15:32:48.696263: step 1170, loss 0.00150264, acc 1, learning_rate 0.000800432
2018-04-02T15:32:49.805226: step 1171, loss 0.00267896, acc 1, learning_rate 0.000799267
2018-04-02T15:32:50.924576: step 1172, loss 0.000545299, acc 1, learning_rate 0.000798104
2018-04-02T15:32:52.042181: step 1173, loss 0.0114033, acc 1, learning_rate 0.000796944
2018-04-02T15:32:53.163838: step 1174, loss 0.000962814, acc 1, learning_rate 0.000795785
2018-04-02T15:32:54.288391: step 1175, loss 0.00679224, acc 1, learning_rate 0.000794628
2018-04-02T15:32:55.409821: step 1176, loss 0.00167237, acc 1, learning_rate 0.000793473
2018-04-02T15:32:56.528105: step 1177, loss 0.00187314, acc 1, learning_rate 0.00079232
2018-04-02T15:32:57.647237: step 1178, loss 0.00291421, acc 1, learning_rate 0.000791169
2018-04-02T15:32:58.762661: step 1179, loss 0.00144486, acc 1, learning_rate 0.00079002
2018-04-02T15:32:59.881013: step 1180, loss 0.00263428, acc 1, learning_rate 0.000788872
2018-04-02T15:33:01.001078: step 1181, loss 0.00935766, acc 1, learning_rate 0.000787727
2018-04-02T15:33:02.120628: step 1182, loss 0.00160591, acc 1, learning_rate 0.000786584
2018-04-02T15:33:03.240630: step 1183, loss 0.00084831, acc 1, learning_rate 0.000785442
2018-04-02T15:33:04.359117: step 1184, loss 0.00132149, acc 1, learning_rate 0.000784302
2018-04-02T15:33:05.473501: step 1185, loss 0.00130701, acc 1, learning_rate 0.000783164
2018-04-02T15:33:06.589309: step 1186, loss 0.00288806, acc 1, learning_rate 0.000782029
2018-04-02T15:33:07.710141: step 1187, loss 0.000893923, acc 1, learning_rate 0.000780895
2018-04-02T15:33:08.829980: step 1188, loss 0.00263677, acc 1, learning_rate 0.000779763
2018-04-02T15:33:09.949452: step 1189, loss 0.00233509, acc 1, learning_rate 0.000778632
2018-04-02T15:33:11.071052: step 1190, loss 0.00451164, acc 1, learning_rate 0.000777504
2018-04-02T15:33:12.179355: step 1191, loss 0.00146372, acc 1, learning_rate 0.000776377
2018-04-02T15:33:13.298729: step 1192, loss 0.0022232, acc 1, learning_rate 0.000775253
2018-04-02T15:33:14.416435: step 1193, loss 0.00374872, acc 1, learning_rate 0.00077413
2018-04-02T15:33:15.533741: step 1194, loss 0.00262786, acc 1, learning_rate 0.000773009
2018-04-02T15:33:16.655009: step 1195, loss 0.00197052, acc 1, learning_rate 0.00077189
2018-04-02T15:33:17.773833: step 1196, loss 0.002299, acc 1, learning_rate 0.000770773
2018-04-02T15:33:18.883849: step 1197, loss 0.00735427, acc 1, learning_rate 0.000769658
2018-04-02T15:33:20.005520: step 1198, loss 0.00190574, acc 1, learning_rate 0.000768544
2018-04-02T15:33:21.129512: step 1199, loss 0.00386607, acc 1, learning_rate 0.000767433
2018-04-02T15:33:22.245568: step 1200, loss 0.00371023, acc 1, learning_rate 0.000766323

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:33:22.457479: step 1200, loss 0.0424517, acc 0.980222

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1200

2018-04-02T15:33:26.460898: step 1201, loss 0.0467818, acc 0.984375, learning_rate 0.000765215
2018-04-02T15:33:27.579307: step 1202, loss 0.00194615, acc 1, learning_rate 0.000764109
2018-04-02T15:33:28.734855: step 1203, loss 0.00136941, acc 1, learning_rate 0.000763005
2018-04-02T15:33:29.894044: step 1204, loss 0.00133175, acc 1, learning_rate 0.000761903
2018-04-02T15:33:30.984437: step 1205, loss 0.0023033, acc 1, learning_rate 0.000760802
2018-04-02T15:33:32.109074: step 1206, loss 0.0189298, acc 0.984375, learning_rate 0.000759703
2018-04-02T15:33:33.226077: step 1207, loss 0.00140365, acc 1, learning_rate 0.000758606
2018-04-02T15:33:34.347365: step 1208, loss 0.0161522, acc 0.984375, learning_rate 0.000757511
2018-04-02T15:33:35.467460: step 1209, loss 0.0084839, acc 1, learning_rate 0.000756418
2018-04-02T15:33:36.585820: step 1210, loss 0.00122192, acc 1, learning_rate 0.000755327
2018-04-02T15:33:37.706978: step 1211, loss 0.00109641, acc 1, learning_rate 0.000754237
2018-04-02T15:33:38.828408: step 1212, loss 0.000856443, acc 1, learning_rate 0.000753149
2018-04-02T15:33:39.947035: step 1213, loss 0.00214937, acc 1, learning_rate 0.000752063
2018-04-02T15:33:41.067488: step 1214, loss 0.002267, acc 1, learning_rate 0.000750979
2018-04-02T15:33:42.186392: step 1215, loss 0.00575186, acc 1, learning_rate 0.000749897
2018-04-02T15:33:43.303855: step 1216, loss 0.0014587, acc 1, learning_rate 0.000748816
2018-04-02T15:33:44.407923: step 1217, loss 0.00238631, acc 1, learning_rate 0.000747738
2018-04-02T15:33:45.525967: step 1218, loss 0.0210486, acc 0.984375, learning_rate 0.000746661
2018-04-02T15:33:46.644266: step 1219, loss 0.0022419, acc 1, learning_rate 0.000745585
2018-04-02T15:33:47.764776: step 1220, loss 0.00312516, acc 1, learning_rate 0.000744512
2018-04-02T15:33:48.884136: step 1221, loss 0.00361276, acc 1, learning_rate 0.00074344
2018-04-02T15:33:50.005377: step 1222, loss 0.00111913, acc 1, learning_rate 0.00074237
2018-04-02T15:33:51.127894: step 1223, loss 0.00399576, acc 1, learning_rate 0.000741302
2018-04-02T15:33:52.245440: step 1224, loss 0.000996872, acc 1, learning_rate 0.000740236
2018-04-02T15:33:53.365343: step 1225, loss 0.00366038, acc 1, learning_rate 0.000739172
2018-04-02T15:33:54.485321: step 1226, loss 0.00271464, acc 1, learning_rate 0.000738109
2018-04-02T15:33:55.605580: step 1227, loss 0.0277198, acc 0.984375, learning_rate 0.000737048
2018-04-02T15:33:56.726974: step 1228, loss 0.00148033, acc 1, learning_rate 0.000735989
2018-04-02T15:33:57.847140: step 1229, loss 0.0014249, acc 1, learning_rate 0.000734931
2018-04-02T15:33:58.962917: step 1230, loss 0.00143991, acc 1, learning_rate 0.000733876
2018-04-02T15:34:00.082445: step 1231, loss 0.00201305, acc 1, learning_rate 0.000732822
2018-04-02T15:34:01.200180: step 1232, loss 0.00242549, acc 1, learning_rate 0.000731769
2018-04-02T15:34:02.320739: step 1233, loss 0.00188475, acc 1, learning_rate 0.000730719
2018-04-02T15:34:03.442077: step 1234, loss 0.00504918, acc 1, learning_rate 0.00072967
2018-04-02T15:34:04.563113: step 1235, loss 0.00158485, acc 1, learning_rate 0.000728623
2018-04-02T15:34:05.685207: step 1236, loss 0.00848399, acc 1, learning_rate 0.000727578
2018-04-02T15:34:06.800530: step 1237, loss 0.00485493, acc 1, learning_rate 0.000726535
2018-04-02T15:34:07.921230: step 1238, loss 0.00252866, acc 1, learning_rate 0.000725493
2018-04-02T15:34:09.034031: step 1239, loss 0.00197796, acc 1, learning_rate 0.000724453
2018-04-02T15:34:10.152719: step 1240, loss 0.00572323, acc 1, learning_rate 0.000723415
2018-04-02T15:34:11.270670: step 1241, loss 0.00285775, acc 1, learning_rate 0.000722378
2018-04-02T15:34:12.395270: step 1242, loss 0.00250736, acc 1, learning_rate 0.000721343
2018-04-02T15:34:13.516473: step 1243, loss 0.0150585, acc 0.984375, learning_rate 0.00072031
2018-04-02T15:34:14.646084: step 1244, loss 0.00109642, acc 1, learning_rate 0.000719279
2018-04-02T15:34:15.761835: step 1245, loss 0.00125763, acc 1, learning_rate 0.000718249
2018-04-02T15:34:16.881194: step 1246, loss 0.00266309, acc 1, learning_rate 0.000717221
2018-04-02T15:34:17.997775: step 1247, loss 0.00323875, acc 1, learning_rate 0.000716195
2018-04-02T15:34:19.117147: step 1248, loss 0.00133205, acc 1, learning_rate 0.000715171
2018-04-02T15:34:20.234084: step 1249, loss 0.00274799, acc 1, learning_rate 0.000714148
2018-04-02T15:34:21.352098: step 1250, loss 0.00242245, acc 1, learning_rate 0.000713127
2018-04-02T15:34:22.470112: step 1251, loss 0.0114066, acc 1, learning_rate 0.000712107
2018-04-02T15:34:23.585388: step 1252, loss 0.000745361, acc 1, learning_rate 0.000711089
2018-04-02T15:34:24.698834: step 1253, loss 0.00169897, acc 1, learning_rate 0.000710073
2018-04-02T15:34:25.814299: step 1254, loss 0.000747238, acc 1, learning_rate 0.000709059
2018-04-02T15:34:26.935158: step 1255, loss 0.00135178, acc 1, learning_rate 0.000708046
2018-04-02T15:34:28.055157: step 1256, loss 0.00152238, acc 1, learning_rate 0.000707035
2018-04-02T15:34:29.177176: step 1257, loss 0.00339557, acc 1, learning_rate 0.000706026
2018-04-02T15:34:30.296150: step 1258, loss 0.00177529, acc 1, learning_rate 0.000705018
2018-04-02T15:34:31.410002: step 1259, loss 0.00141663, acc 1, learning_rate 0.000704012
2018-04-02T15:34:32.528754: step 1260, loss 0.00176458, acc 1, learning_rate 0.000703008
2018-04-02T15:34:33.646098: step 1261, loss 0.00510807, acc 1, learning_rate 0.000702006
2018-04-02T15:34:34.765914: step 1262, loss 0.0383637, acc 0.984375, learning_rate 0.000701005
2018-04-02T15:34:35.885293: step 1263, loss 0.00132125, acc 1, learning_rate 0.000700005
2018-04-02T15:34:37.004261: step 1264, loss 0.00304345, acc 1, learning_rate 0.000699008
2018-04-02T15:34:38.116457: step 1265, loss 0.000657938, acc 1, learning_rate 0.000698012
2018-04-02T15:34:39.228583: step 1266, loss 0.00185736, acc 1, learning_rate 0.000697017
2018-04-02T15:34:40.346821: step 1267, loss 0.00301354, acc 1, learning_rate 0.000696025
2018-04-02T15:34:41.464063: step 1268, loss 0.000833055, acc 1, learning_rate 0.000695034
2018-04-02T15:34:42.585683: step 1269, loss 0.00186856, acc 1, learning_rate 0.000694044
2018-04-02T15:34:43.705961: step 1270, loss 0.00397196, acc 1, learning_rate 0.000693057
2018-04-02T15:34:44.828579: step 1271, loss 0.000957228, acc 1, learning_rate 0.000692071
2018-04-02T15:34:45.948786: step 1272, loss 0.0025305, acc 1, learning_rate 0.000691086
2018-04-02T15:34:47.067493: step 1273, loss 0.00262523, acc 1, learning_rate 0.000690103
2018-04-02T15:34:48.187897: step 1274, loss 0.00139814, acc 1, learning_rate 0.000689122
2018-04-02T15:34:49.304035: step 1275, loss 0.0547659, acc 0.984375, learning_rate 0.000688143
2018-04-02T15:34:50.419320: step 1276, loss 0.00352065, acc 1, learning_rate 0.000687165
2018-04-02T15:34:51.536952: step 1277, loss 0.00228194, acc 1, learning_rate 0.000686189
2018-04-02T15:34:52.656949: step 1278, loss 0.0056878, acc 1, learning_rate 0.000685214
2018-04-02T15:34:53.775832: step 1279, loss 0.0255261, acc 0.96875, learning_rate 0.000684241
2018-04-02T15:34:54.896907: step 1280, loss 0.0109507, acc 1, learning_rate 0.00068327
2018-04-02T15:34:56.015368: step 1281, loss 0.0039293, acc 1, learning_rate 0.0006823
2018-04-02T15:34:57.137121: step 1282, loss 0.00200506, acc 1, learning_rate 0.000681332
2018-04-02T15:34:58.253243: step 1283, loss 0.00152492, acc 1, learning_rate 0.000680365
2018-04-02T15:34:59.382854: step 1284, loss 0.000574056, acc 1, learning_rate 0.0006794
2018-04-02T15:35:00.507144: step 1285, loss 0.00267682, acc 1, learning_rate 0.000678437
2018-04-02T15:35:01.630225: step 1286, loss 0.00220194, acc 1, learning_rate 0.000677475
2018-04-02T15:35:02.749957: step 1287, loss 0.00141913, acc 1, learning_rate 0.000676515
2018-04-02T15:35:03.865691: step 1288, loss 0.00762959, acc 1, learning_rate 0.000675556
2018-04-02T15:35:04.983775: step 1289, loss 0.00183981, acc 1, learning_rate 0.000674599
2018-04-02T15:35:06.101071: step 1290, loss 0.00142694, acc 1, learning_rate 0.000673644
2018-04-02T15:35:07.217861: step 1291, loss 0.00237415, acc 1, learning_rate 0.00067269
2018-04-02T15:35:08.339973: step 1292, loss 0.000484277, acc 1, learning_rate 0.000671738
2018-04-02T15:35:09.457158: step 1293, loss 0.0741083, acc 0.96875, learning_rate 0.000670787
2018-04-02T15:35:10.573945: step 1294, loss 0.000540073, acc 1, learning_rate 0.000669838
2018-04-02T15:35:11.696657: step 1295, loss 0.00162031, acc 1, learning_rate 0.000668891
2018-04-02T15:35:12.817141: step 1296, loss 0.00309041, acc 1, learning_rate 0.000667945
2018-04-02T15:35:13.934768: step 1297, loss 0.00161983, acc 1, learning_rate 0.000667001
2018-04-02T15:35:15.050296: step 1298, loss 0.00298072, acc 1, learning_rate 0.000666058
2018-04-02T15:35:16.166071: step 1299, loss 0.00244773, acc 1, learning_rate 0.000665117
2018-04-02T15:35:17.283232: step 1300, loss 0.00367668, acc 1, learning_rate 0.000664177

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:35:17.494629: step 1300, loss 0.0461647, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1300

2018-04-02T15:37:56.416479: step 1301, loss 0.00284588, acc 1, learning_rate 0.000663239
2018-04-02T15:37:57.531867: step 1302, loss 0.00754222, acc 1, learning_rate 0.000662302
2018-04-02T15:37:58.647034: step 1303, loss 0.00211207, acc 1, learning_rate 0.000661368
2018-04-02T15:37:59.783407: step 1304, loss 0.00300647, acc 1, learning_rate 0.000660434
2018-04-02T15:38:00.940709: step 1305, loss 0.00226259, acc 1, learning_rate 0.000659502
2018-04-02T15:38:02.073816: step 1306, loss 0.00118288, acc 1, learning_rate 0.000658572
2018-04-02T15:38:03.192701: step 1307, loss 0.00641369, acc 1, learning_rate 0.000657643
2018-04-02T15:38:04.307716: step 1308, loss 0.0316142, acc 0.984375, learning_rate 0.000656716
2018-04-02T15:38:05.422330: step 1309, loss 0.000649852, acc 1, learning_rate 0.00065579
2018-04-02T15:38:06.546416: step 1310, loss 0.00559021, acc 1, learning_rate 0.000654866
2018-04-02T15:38:07.668801: step 1311, loss 0.00949985, acc 1, learning_rate 0.000653944
2018-04-02T15:38:08.788954: step 1312, loss 0.00139239, acc 1, learning_rate 0.000653023
2018-04-02T15:38:09.903525: step 1313, loss 0.000852918, acc 1, learning_rate 0.000652103
2018-04-02T15:38:11.025238: step 1314, loss 0.00242335, acc 1, learning_rate 0.000651185
2018-04-02T15:38:12.142453: step 1315, loss 0.0682586, acc 0.984375, learning_rate 0.000650269
2018-04-02T15:38:13.254206: step 1316, loss 0.00375125, acc 1, learning_rate 0.000649354
2018-04-02T15:38:14.377312: step 1317, loss 0.000645359, acc 1, learning_rate 0.000648441
2018-04-02T15:38:15.501869: step 1318, loss 0.00127155, acc 1, learning_rate 0.000647529
2018-04-02T15:38:16.620237: step 1319, loss 0.00131414, acc 1, learning_rate 0.000646618
2018-04-02T15:38:17.736850: step 1320, loss 0.000692699, acc 1, learning_rate 0.000645709
2018-04-02T15:38:18.853240: step 1321, loss 0.00293785, acc 1, learning_rate 0.000644802
2018-04-02T15:38:19.967388: step 1322, loss 0.00433202, acc 1, learning_rate 0.000643896
2018-04-02T15:38:21.087911: step 1323, loss 0.0011248, acc 1, learning_rate 0.000642992
2018-04-02T15:38:22.201704: step 1324, loss 0.00251245, acc 1, learning_rate 0.000642089
2018-04-02T15:38:23.317328: step 1325, loss 0.00478737, acc 1, learning_rate 0.000641188
2018-04-02T15:38:24.442309: step 1326, loss 0.0262538, acc 0.984375, learning_rate 0.000640288
2018-04-02T15:38:25.563072: step 1327, loss 0.000748068, acc 1, learning_rate 0.00063939
2018-04-02T15:38:26.686557: step 1328, loss 0.000812623, acc 1, learning_rate 0.000638493
2018-04-02T15:38:27.802758: step 1329, loss 0.00201829, acc 1, learning_rate 0.000637597
2018-04-02T15:38:28.916774: step 1330, loss 0.00295944, acc 1, learning_rate 0.000636704
2018-04-02T15:38:30.033074: step 1331, loss 0.000746645, acc 1, learning_rate 0.000635811
2018-04-02T15:38:31.147047: step 1332, loss 0.0336507, acc 0.984375, learning_rate 0.00063492
2018-04-02T15:38:32.267826: step 1333, loss 0.0017693, acc 1, learning_rate 0.000634031
2018-04-02T15:38:33.385664: step 1334, loss 0.00183182, acc 1, learning_rate 0.000633143
2018-04-02T15:38:34.499771: step 1335, loss 0.00049452, acc 1, learning_rate 0.000632257
2018-04-02T15:38:35.614546: step 1336, loss 0.00208755, acc 1, learning_rate 0.000631372
2018-04-02T15:38:36.733477: step 1337, loss 0.00125191, acc 1, learning_rate 0.000630488
2018-04-02T15:38:37.849866: step 1338, loss 0.00122298, acc 1, learning_rate 0.000629606
2018-04-02T15:38:38.967317: step 1339, loss 0.00946855, acc 1, learning_rate 0.000628726
2018-04-02T15:38:40.086423: step 1340, loss 0.00567867, acc 1, learning_rate 0.000627846
2018-04-02T15:38:41.204197: step 1341, loss 0.000795054, acc 1, learning_rate 0.000626969
2018-04-02T15:38:42.323980: step 1342, loss 0.00323667, acc 1, learning_rate 0.000626093
2018-04-02T15:38:43.441272: step 1343, loss 0.00165797, acc 1, learning_rate 0.000625218
2018-04-02T15:38:44.558490: step 1344, loss 0.00142926, acc 1, learning_rate 0.000624345
2018-04-02T15:38:45.678265: step 1345, loss 0.00352503, acc 1, learning_rate 0.000623473
2018-04-02T15:38:46.794743: step 1346, loss 0.00504276, acc 1, learning_rate 0.000622602
2018-04-02T15:38:47.910922: step 1347, loss 0.000763321, acc 1, learning_rate 0.000621733
2018-04-02T15:38:49.030081: step 1348, loss 0.00106055, acc 1, learning_rate 0.000620866
2018-04-02T15:38:50.149704: step 1349, loss 0.0050175, acc 1, learning_rate 0.00062
2018-04-02T15:38:51.267958: step 1350, loss 0.010845, acc 1, learning_rate 0.000619135
2018-04-02T15:38:52.387287: step 1351, loss 0.00108664, acc 1, learning_rate 0.000618272
2018-04-02T15:38:53.500640: step 1352, loss 0.00743844, acc 1, learning_rate 0.00061741
2018-04-02T15:38:54.618448: step 1353, loss 0.00129886, acc 1, learning_rate 0.00061655
2018-04-02T15:38:55.739310: step 1354, loss 0.00104156, acc 1, learning_rate 0.000615691
2018-04-02T15:38:56.855988: step 1355, loss 0.00686569, acc 1, learning_rate 0.000614834
2018-04-02T15:38:57.972313: step 1356, loss 0.00262493, acc 1, learning_rate 0.000613978
2018-04-02T15:38:59.085709: step 1357, loss 0.0937962, acc 0.984375, learning_rate 0.000613123
2018-04-02T15:39:00.206580: step 1358, loss 0.00216931, acc 1, learning_rate 0.00061227
2018-04-02T15:39:01.327507: step 1359, loss 0.000793858, acc 1, learning_rate 0.000611418
2018-04-02T15:39:02.445154: step 1360, loss 0.00451771, acc 1, learning_rate 0.000610568
2018-04-02T15:39:03.563363: step 1361, loss 0.00114723, acc 1, learning_rate 0.000609719
2018-04-02T15:39:04.679845: step 1362, loss 0.00042935, acc 1, learning_rate 0.000608872
2018-04-02T15:39:05.795624: step 1363, loss 0.000567191, acc 1, learning_rate 0.000608026
2018-04-02T15:39:06.917644: step 1364, loss 0.0164334, acc 0.984375, learning_rate 0.000607181
2018-04-02T15:39:08.038412: step 1365, loss 0.00139706, acc 1, learning_rate 0.000606338
2018-04-02T15:39:09.158752: step 1366, loss 0.0565465, acc 0.984375, learning_rate 0.000605496
2018-04-02T15:39:10.274226: step 1367, loss 0.0258531, acc 0.984375, learning_rate 0.000604655
2018-04-02T15:39:11.393554: step 1368, loss 0.00361643, acc 1, learning_rate 0.000603816
2018-04-02T15:39:12.515211: step 1369, loss 0.00141115, acc 1, learning_rate 0.000602979
2018-04-02T15:39:13.634813: step 1370, loss 0.00191978, acc 1, learning_rate 0.000602142
2018-04-02T15:39:14.751986: step 1371, loss 0.00134757, acc 1, learning_rate 0.000601307
2018-04-02T15:39:15.870329: step 1372, loss 0.00574086, acc 1, learning_rate 0.000600474
2018-04-02T15:39:16.989147: step 1373, loss 0.00286827, acc 1, learning_rate 0.000599642
2018-04-02T15:39:18.110862: step 1374, loss 0.00164583, acc 1, learning_rate 0.000598811
2018-04-02T15:39:19.230737: step 1375, loss 0.00385046, acc 1, learning_rate 0.000597982
2018-04-02T15:39:20.354143: step 1376, loss 0.00332838, acc 1, learning_rate 0.000597154
2018-04-02T15:39:21.473125: step 1377, loss 0.00289681, acc 1, learning_rate 0.000596327
2018-04-02T15:39:22.591374: step 1378, loss 0.00463946, acc 1, learning_rate 0.000595502
2018-04-02T15:39:23.710793: step 1379, loss 0.0677807, acc 0.984375, learning_rate 0.000594678
2018-04-02T15:39:24.831081: step 1380, loss 0.00287999, acc 1, learning_rate 0.000593855
2018-04-02T15:39:25.949789: step 1381, loss 0.00267291, acc 1, learning_rate 0.000593034
2018-04-02T15:39:27.070509: step 1382, loss 0.00361868, acc 1, learning_rate 0.000592214
2018-04-02T15:39:28.190930: step 1383, loss 0.00396076, acc 1, learning_rate 0.000591396
2018-04-02T15:39:29.306836: step 1384, loss 0.00285916, acc 1, learning_rate 0.000590579
2018-04-02T15:39:30.424014: step 1385, loss 0.00112969, acc 1, learning_rate 0.000589763
2018-04-02T15:39:31.544136: step 1386, loss 0.0123653, acc 1, learning_rate 0.000588949
2018-04-02T15:39:32.664015: step 1387, loss 0.00365154, acc 1, learning_rate 0.000588136
2018-04-02T15:39:33.783729: step 1388, loss 0.0555083, acc 0.984375, learning_rate 0.000587324
2018-04-02T15:39:34.903945: step 1389, loss 0.00159667, acc 1, learning_rate 0.000586514
2018-04-02T15:39:36.022055: step 1390, loss 0.00192609, acc 1, learning_rate 0.000585705
2018-04-02T15:39:37.142371: step 1391, loss 0.00194502, acc 1, learning_rate 0.000584898
2018-04-02T15:39:38.260423: step 1392, loss 0.00715604, acc 1, learning_rate 0.000584091
2018-04-02T15:39:39.378692: step 1393, loss 0.00217237, acc 1, learning_rate 0.000583287
2018-04-02T15:39:40.502573: step 1394, loss 0.00181251, acc 1, learning_rate 0.000582483
2018-04-02T15:39:41.620774: step 1395, loss 0.00363748, acc 1, learning_rate 0.000581681
2018-04-02T15:39:42.739823: step 1396, loss 0.00602063, acc 1, learning_rate 0.00058088
2018-04-02T15:39:43.859273: step 1397, loss 0.00125986, acc 1, learning_rate 0.00058008
2018-04-02T15:39:44.977012: step 1398, loss 0.00285634, acc 1, learning_rate 0.000579282
2018-04-02T15:39:46.096990: step 1399, loss 0.00223622, acc 1, learning_rate 0.000578485
2018-04-02T15:39:47.211944: step 1400, loss 0.00483342, acc 1, learning_rate 0.00057769

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:39:47.422867: step 1400, loss 0.0410136, acc 0.985167

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1400

2018-04-02T15:39:51.407991: step 1401, loss 0.0029636, acc 1, learning_rate 0.000576895
2018-04-02T15:39:52.566228: step 1402, loss 0.00314472, acc 1, learning_rate 0.000576103
2018-04-02T15:39:53.715852: step 1403, loss 0.00598727, acc 1, learning_rate 0.000575311
2018-04-02T15:39:54.836890: step 1404, loss 0.00211357, acc 1, learning_rate 0.000574521
2018-04-02T15:39:55.954031: step 1405, loss 0.00410295, acc 1, learning_rate 0.000573732
2018-04-02T15:39:57.070841: step 1406, loss 0.00710251, acc 1, learning_rate 0.000572944
2018-04-02T15:39:58.194175: step 1407, loss 0.00259254, acc 1, learning_rate 0.000572158
2018-04-02T15:39:59.309503: step 1408, loss 0.00348077, acc 1, learning_rate 0.000571373
2018-04-02T15:40:00.427049: step 1409, loss 0.00209273, acc 1, learning_rate 0.000570589
2018-04-02T15:40:01.546367: step 1410, loss 0.00140556, acc 1, learning_rate 0.000569806
2018-04-02T15:40:02.665535: step 1411, loss 0.00487916, acc 1, learning_rate 0.000569025
2018-04-02T15:40:03.784205: step 1412, loss 0.00490199, acc 1, learning_rate 0.000568245
2018-04-02T15:40:04.904259: step 1413, loss 0.00294652, acc 1, learning_rate 0.000567467
2018-04-02T15:40:06.016951: step 1414, loss 0.00210383, acc 1, learning_rate 0.00056669
2018-04-02T15:40:07.136776: step 1415, loss 0.00213725, acc 1, learning_rate 0.000565914
2018-04-02T15:40:08.249842: step 1416, loss 0.000974404, acc 1, learning_rate 0.000565139
2018-04-02T15:40:09.366868: step 1417, loss 0.0034073, acc 1, learning_rate 0.000564366
2018-04-02T15:40:10.488267: step 1418, loss 0.00129045, acc 1, learning_rate 0.000563594
2018-04-02T15:40:11.605862: step 1419, loss 0.00209707, acc 1, learning_rate 0.000562823
2018-04-02T15:40:12.725870: step 1420, loss 0.00230894, acc 1, learning_rate 0.000562053
2018-04-02T15:40:13.848631: step 1421, loss 0.00307987, acc 1, learning_rate 0.000561285
2018-04-02T15:40:14.971562: step 1422, loss 0.00118966, acc 1, learning_rate 0.000560518
2018-04-02T15:40:16.091520: step 1423, loss 0.00174881, acc 1, learning_rate 0.000559752
2018-04-02T15:40:17.210002: step 1424, loss 0.0477184, acc 0.984375, learning_rate 0.000558988
2018-04-02T15:40:18.332252: step 1425, loss 0.0033114, acc 1, learning_rate 0.000558225
2018-04-02T15:40:19.453226: step 1426, loss 0.00173532, acc 1, learning_rate 0.000557463
2018-04-02T15:40:20.575876: step 1427, loss 0.00210948, acc 1, learning_rate 0.000556702
2018-04-02T15:40:21.690742: step 1428, loss 0.000923115, acc 1, learning_rate 0.000555943
2018-04-02T15:40:22.808808: step 1429, loss 0.00188427, acc 1, learning_rate 0.000555185
2018-04-02T15:40:23.924353: step 1430, loss 0.0016266, acc 1, learning_rate 0.000554428
2018-04-02T15:40:25.042717: step 1431, loss 0.0437671, acc 0.984375, learning_rate 0.000553672
2018-04-02T15:40:26.166739: step 1432, loss 0.00152239, acc 1, learning_rate 0.000552918
2018-04-02T15:40:27.280782: step 1433, loss 0.0911627, acc 0.96875, learning_rate 0.000552165
2018-04-02T15:40:28.402143: step 1434, loss 0.0012358, acc 1, learning_rate 0.000551413
2018-04-02T15:40:29.517165: step 1435, loss 0.00218217, acc 1, learning_rate 0.000550663
2018-04-02T15:40:30.630261: step 1436, loss 0.00109908, acc 1, learning_rate 0.000549913
2018-04-02T15:40:31.747408: step 1437, loss 0.00151534, acc 1, learning_rate 0.000549165
2018-04-02T15:40:32.871636: step 1438, loss 0.00109136, acc 1, learning_rate 0.000548418
2018-04-02T15:40:33.998054: step 1439, loss 0.00129596, acc 1, learning_rate 0.000547673
2018-04-02T15:40:35.120819: step 1440, loss 0.000698227, acc 1, learning_rate 0.000546929
2018-04-02T15:40:36.230747: step 1441, loss 0.00117588, acc 1, learning_rate 0.000546185
2018-04-02T15:40:37.348514: step 1442, loss 0.00325195, acc 1, learning_rate 0.000545444
2018-04-02T15:40:38.464789: step 1443, loss 0.00283453, acc 1, learning_rate 0.000544703
2018-04-02T15:40:39.582020: step 1444, loss 0.00433014, acc 1, learning_rate 0.000543964
2018-04-02T15:40:40.690137: step 1445, loss 0.00472668, acc 1, learning_rate 0.000543225
2018-04-02T15:40:41.760241: step 1446, loss 0.00100392, acc 1, learning_rate 0.000542488
2018-04-02T15:40:42.880331: step 1447, loss 0.00169945, acc 1, learning_rate 0.000541753
2018-04-02T15:40:43.997324: step 1448, loss 0.00160458, acc 1, learning_rate 0.000541018
2018-04-02T15:40:45.113989: step 1449, loss 0.000425513, acc 1, learning_rate 0.000540285
2018-04-02T15:40:46.231849: step 1450, loss 0.00129159, acc 1, learning_rate 0.000539553
2018-04-02T15:40:47.349564: step 1451, loss 0.0020507, acc 1, learning_rate 0.000538822
2018-04-02T15:40:48.465884: step 1452, loss 0.0237806, acc 1, learning_rate 0.000538092
2018-04-02T15:40:49.584032: step 1453, loss 0.000841901, acc 1, learning_rate 0.000537364
2018-04-02T15:40:50.702838: step 1454, loss 0.000882586, acc 1, learning_rate 0.000536637
2018-04-02T15:40:51.820517: step 1455, loss 0.00424016, acc 1, learning_rate 0.000535911
2018-04-02T15:40:52.937592: step 1456, loss 0.00246016, acc 1, learning_rate 0.000535186
2018-04-02T15:40:54.056938: step 1457, loss 0.00252111, acc 1, learning_rate 0.000534462
2018-04-02T15:40:55.175681: step 1458, loss 0.00136915, acc 1, learning_rate 0.00053374
2018-04-02T15:40:56.294807: step 1459, loss 0.0135132, acc 0.984375, learning_rate 0.000533019
2018-04-02T15:40:57.417587: step 1460, loss 0.00297654, acc 1, learning_rate 0.000532299
2018-04-02T15:40:58.534005: step 1461, loss 0.00264602, acc 1, learning_rate 0.00053158
2018-04-02T15:40:59.656218: step 1462, loss 0.00233405, acc 1, learning_rate 0.000530863
2018-04-02T15:41:00.778176: step 1463, loss 0.00254086, acc 1, learning_rate 0.000530146
2018-04-02T15:41:01.898702: step 1464, loss 0.00271353, acc 1, learning_rate 0.000529431
2018-04-02T15:41:03.018232: step 1465, loss 0.00484727, acc 1, learning_rate 0.000528717
2018-04-02T15:41:04.139488: step 1466, loss 0.00327148, acc 1, learning_rate 0.000528004
2018-04-02T15:41:05.255088: step 1467, loss 0.00150385, acc 1, learning_rate 0.000527293
2018-04-02T15:41:06.370312: step 1468, loss 0.00212995, acc 1, learning_rate 0.000526582
2018-04-02T15:41:07.489546: step 1469, loss 0.00748638, acc 1, learning_rate 0.000525873
2018-04-02T15:41:08.608495: step 1470, loss 0.00150741, acc 1, learning_rate 0.000525165
2018-04-02T15:41:09.724357: step 1471, loss 0.00132233, acc 1, learning_rate 0.000524458
2018-04-02T15:41:10.841930: step 1472, loss 0.00357983, acc 1, learning_rate 0.000523752
2018-04-02T15:41:11.958249: step 1473, loss 0.000774426, acc 1, learning_rate 0.000523048
2018-04-02T15:41:13.077446: step 1474, loss 0.00236388, acc 1, learning_rate 0.000522344
2018-04-02T15:41:14.196680: step 1475, loss 0.00191229, acc 1, learning_rate 0.000521642
2018-04-02T15:41:15.317772: step 1476, loss 0.00445929, acc 1, learning_rate 0.000520941
2018-04-02T15:41:16.439541: step 1477, loss 0.0722058, acc 0.984375, learning_rate 0.000520241
2018-04-02T15:41:17.560641: step 1478, loss 0.00254588, acc 1, learning_rate 0.000519542
2018-04-02T15:41:18.681277: step 1479, loss 0.0136956, acc 1, learning_rate 0.000518845
2018-04-02T15:41:19.797059: step 1480, loss 0.00564177, acc 1, learning_rate 0.000518148
2018-04-02T15:41:20.917467: step 1481, loss 0.000663068, acc 1, learning_rate 0.000517453
2018-04-02T15:41:22.035039: step 1482, loss 0.000536929, acc 1, learning_rate 0.000516759
2018-04-02T15:41:23.155783: step 1483, loss 0.00101256, acc 1, learning_rate 0.000516066
2018-04-02T15:41:24.273406: step 1484, loss 0.00225057, acc 1, learning_rate 0.000515374
2018-04-02T15:41:25.393072: step 1485, loss 0.0153529, acc 0.984375, learning_rate 0.000514684
2018-04-02T15:41:26.514755: step 1486, loss 0.00130859, acc 1, learning_rate 0.000513994
2018-04-02T15:41:27.633878: step 1487, loss 0.0106875, acc 1, learning_rate 0.000513306
2018-04-02T15:41:28.752120: step 1488, loss 0.000949487, acc 1, learning_rate 0.000512619
2018-04-02T15:41:29.870984: step 1489, loss 0.00158585, acc 1, learning_rate 0.000511932
2018-04-02T15:41:30.989156: step 1490, loss 0.000807192, acc 1, learning_rate 0.000511248
2018-04-02T15:41:32.108748: step 1491, loss 0.0018196, acc 1, learning_rate 0.000510564
2018-04-02T15:41:33.227459: step 1492, loss 0.00162906, acc 1, learning_rate 0.000509881
2018-04-02T15:41:34.346897: step 1493, loss 0.00144757, acc 1, learning_rate 0.0005092
2018-04-02T15:41:35.464190: step 1494, loss 0.000739087, acc 1, learning_rate 0.000508519
2018-04-02T15:41:36.581355: step 1495, loss 0.00291795, acc 1, learning_rate 0.00050784
2018-04-02T15:41:37.699602: step 1496, loss 0.000470777, acc 1, learning_rate 0.000507162
2018-04-02T15:41:38.815822: step 1497, loss 0.00111884, acc 1, learning_rate 0.000506485
2018-04-02T15:41:39.933322: step 1498, loss 0.00286669, acc 1, learning_rate 0.000505809
2018-04-02T15:41:41.049608: step 1499, loss 0.00521007, acc 1, learning_rate 0.000505134
2018-04-02T15:41:42.169699: step 1500, loss 0.00141675, acc 1, learning_rate 0.000504461

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:41:42.380183: step 1500, loss 0.0423209, acc 0.980222

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1500

2018-04-02T15:41:45.991580: step 1501, loss 0.00218653, acc 1, learning_rate 0.000503788
2018-04-02T15:41:47.113420: step 1502, loss 0.00218344, acc 1, learning_rate 0.000503117
2018-04-02T15:41:48.270175: step 1503, loss 0.00212273, acc 1, learning_rate 0.000502447
2018-04-02T15:41:49.399887: step 1504, loss 0.00244582, acc 1, learning_rate 0.000501777
2018-04-02T15:41:50.520498: step 1505, loss 0.0028349, acc 1, learning_rate 0.000501109
2018-04-02T15:41:51.640045: step 1506, loss 0.00324571, acc 1, learning_rate 0.000500443
2018-04-02T15:41:52.761667: step 1507, loss 0.00686334, acc 1, learning_rate 0.000499777
2018-04-02T15:41:53.881847: step 1508, loss 0.0539709, acc 0.984375, learning_rate 0.000499112
2018-04-02T15:41:54.999952: step 1509, loss 0.0015002, acc 1, learning_rate 0.000498448
2018-04-02T15:41:56.117270: step 1510, loss 0.00140038, acc 1, learning_rate 0.000497786
2018-04-02T15:41:57.231996: step 1511, loss 0.0022221, acc 1, learning_rate 0.000497125
2018-04-02T15:41:58.354943: step 1512, loss 0.00131092, acc 1, learning_rate 0.000496464
2018-04-02T15:41:59.472978: step 1513, loss 0.00278852, acc 1, learning_rate 0.000495805
2018-04-02T15:42:00.592192: step 1514, loss 0.00265492, acc 1, learning_rate 0.000495147
2018-04-02T15:42:01.712051: step 1515, loss 0.00415267, acc 1, learning_rate 0.00049449
2018-04-02T15:42:02.830526: step 1516, loss 0.00104406, acc 1, learning_rate 0.000493834
2018-04-02T15:42:03.944082: step 1517, loss 0.00110972, acc 1, learning_rate 0.000493179
2018-04-02T15:42:05.075292: step 1518, loss 0.00296392, acc 1, learning_rate 0.000492526
2018-04-02T15:42:06.224995: step 1519, loss 0.00152249, acc 1, learning_rate 0.000491873
2018-04-02T15:42:07.381185: step 1520, loss 0.00643738, acc 1, learning_rate 0.000491221
2018-04-02T15:42:08.533324: step 1521, loss 0.0020432, acc 1, learning_rate 0.000490571
2018-04-02T15:42:09.686915: step 1522, loss 0.00382458, acc 1, learning_rate 0.000489921
2018-04-02T15:42:10.839026: step 1523, loss 0.00242752, acc 1, learning_rate 0.000489273
2018-04-02T15:42:11.962863: step 1524, loss 0.0548895, acc 0.984375, learning_rate 0.000488626
2018-04-02T15:42:13.077718: step 1525, loss 0.0374968, acc 0.984375, learning_rate 0.00048798
2018-04-02T15:42:14.202006: step 1526, loss 0.0014433, acc 1, learning_rate 0.000487335
2018-04-02T15:42:15.333346: step 1527, loss 0.00420267, acc 1, learning_rate 0.000486691
2018-04-02T15:42:16.474918: step 1528, loss 0.00233895, acc 1, learning_rate 0.000486048
2018-04-02T15:42:17.623217: step 1529, loss 0.00202971, acc 1, learning_rate 0.000485406
2018-04-02T15:42:18.767526: step 1530, loss 0.00154201, acc 1, learning_rate 0.000484765
2018-04-02T15:42:19.922267: step 1531, loss 0.00357989, acc 1, learning_rate 0.000484125
2018-04-02T15:42:21.066319: step 1532, loss 0.00177257, acc 1, learning_rate 0.000483487
2018-04-02T15:42:22.224050: step 1533, loss 0.00290028, acc 1, learning_rate 0.000482849
2018-04-02T15:42:23.381320: step 1534, loss 0.000528512, acc 1, learning_rate 0.000482212
2018-04-02T15:42:24.529300: step 1535, loss 0.000948713, acc 1, learning_rate 0.000481577
2018-04-02T15:42:25.655801: step 1536, loss 0.00106841, acc 1, learning_rate 0.000480942
2018-04-02T15:42:26.771270: step 1537, loss 0.00428248, acc 1, learning_rate 0.000480309
2018-04-02T15:42:27.928520: step 1538, loss 0.00156111, acc 1, learning_rate 0.000479677
2018-04-02T15:42:29.078666: step 1539, loss 0.00137969, acc 1, learning_rate 0.000479045
2018-04-02T15:42:30.233060: step 1540, loss 0.00164749, acc 1, learning_rate 0.000478415
2018-04-02T15:42:31.351850: step 1541, loss 0.00164855, acc 1, learning_rate 0.000477786
2018-04-02T15:42:32.470720: step 1542, loss 0.00264999, acc 1, learning_rate 0.000477158
2018-04-02T15:42:33.592092: step 1543, loss 0.00175353, acc 1, learning_rate 0.000476531
2018-04-02T15:42:34.716897: step 1544, loss 0.00259611, acc 1, learning_rate 0.000475905
2018-04-02T15:42:35.868836: step 1545, loss 0.00657975, acc 1, learning_rate 0.00047528
2018-04-02T15:42:37.028104: step 1546, loss 0.000670736, acc 1, learning_rate 0.000474656
2018-04-02T15:42:38.184608: step 1547, loss 0.0806906, acc 0.984375, learning_rate 0.000474033
2018-04-02T15:42:39.343838: step 1548, loss 0.00339836, acc 1, learning_rate 0.000473411
2018-04-02T15:42:40.497737: step 1549, loss 0.00739012, acc 1, learning_rate 0.00047279
2018-04-02T15:42:41.654548: step 1550, loss 0.00243407, acc 1, learning_rate 0.00047217
2018-04-02T15:42:42.807109: step 1551, loss 0.00163315, acc 1, learning_rate 0.000471551
2018-04-02T15:42:43.963522: step 1552, loss 0.00341073, acc 1, learning_rate 0.000470934
2018-04-02T15:42:45.118453: step 1553, loss 0.00225782, acc 1, learning_rate 0.000470317
2018-04-02T15:42:46.272625: step 1554, loss 0.0199026, acc 0.984375, learning_rate 0.000469701
2018-04-02T15:42:47.429443: step 1555, loss 0.000457965, acc 1, learning_rate 0.000469087
2018-04-02T15:42:48.592253: step 1556, loss 0.00175309, acc 1, learning_rate 0.000468473
2018-04-02T15:42:49.750525: step 1557, loss 0.00171612, acc 1, learning_rate 0.00046786
2018-04-02T15:42:50.911201: step 1558, loss 0.00111871, acc 1, learning_rate 0.000467249
2018-04-02T15:42:52.072992: step 1559, loss 0.0018638, acc 1, learning_rate 0.000466638
2018-04-02T15:42:53.231317: step 1560, loss 0.00140657, acc 1, learning_rate 0.000466028
2018-04-02T15:42:54.390224: step 1561, loss 0.00221195, acc 1, learning_rate 0.00046542
2018-04-02T15:42:55.550044: step 1562, loss 0.00113141, acc 1, learning_rate 0.000464812
2018-04-02T15:42:56.710277: step 1563, loss 0.00383468, acc 1, learning_rate 0.000464206
2018-04-02T15:42:57.869259: step 1564, loss 0.00171101, acc 1, learning_rate 0.0004636
2018-04-02T15:42:59.031302: step 1565, loss 0.00157599, acc 1, learning_rate 0.000462995
2018-04-02T15:43:00.191271: step 1566, loss 0.00307154, acc 1, learning_rate 0.000462392
2018-04-02T15:43:01.350070: step 1567, loss 0.00157785, acc 1, learning_rate 0.000461789
2018-04-02T15:43:02.510814: step 1568, loss 0.00238487, acc 1, learning_rate 0.000461188
2018-04-02T15:43:03.677783: step 1569, loss 0.0012304, acc 1, learning_rate 0.000460587
2018-04-02T15:43:04.835326: step 1570, loss 0.00262125, acc 1, learning_rate 0.000459988
2018-04-02T15:43:05.997374: step 1571, loss 0.00146005, acc 1, learning_rate 0.000459389
2018-04-02T15:43:07.163959: step 1572, loss 0.0473608, acc 0.984375, learning_rate 0.000458792
2018-04-02T15:43:08.329479: step 1573, loss 0.00184223, acc 1, learning_rate 0.000458195
2018-04-02T15:43:09.487545: step 1574, loss 0.00307534, acc 1, learning_rate 0.0004576
2018-04-02T15:43:10.648347: step 1575, loss 0.00421746, acc 1, learning_rate 0.000457005
2018-04-02T15:43:11.805176: step 1576, loss 0.0013813, acc 1, learning_rate 0.000456411
2018-04-02T15:43:12.966124: step 1577, loss 0.000895393, acc 1, learning_rate 0.000455819
2018-04-02T15:43:14.125731: step 1578, loss 0.00327516, acc 1, learning_rate 0.000455227
2018-04-02T15:43:15.283030: step 1579, loss 0.00133648, acc 1, learning_rate 0.000454637
2018-04-02T15:43:16.441692: step 1580, loss 0.0036582, acc 1, learning_rate 0.000454047
2018-04-02T15:43:17.602036: step 1581, loss 0.00126041, acc 1, learning_rate 0.000453458
2018-04-02T15:43:18.757200: step 1582, loss 0.00219272, acc 1, learning_rate 0.000452871
2018-04-02T15:43:19.916392: step 1583, loss 0.00209724, acc 1, learning_rate 0.000452284
2018-04-02T15:43:21.078810: step 1584, loss 0.00143636, acc 1, learning_rate 0.000451698
2018-04-02T15:43:22.246512: step 1585, loss 0.000523338, acc 1, learning_rate 0.000451113
2018-04-02T15:43:23.413105: step 1586, loss 0.00104103, acc 1, learning_rate 0.00045053
2018-04-02T15:43:24.569025: step 1587, loss 0.00264713, acc 1, learning_rate 0.000449947
2018-04-02T15:43:25.738332: step 1588, loss 0.00234572, acc 1, learning_rate 0.000449365
2018-04-02T15:43:26.897513: step 1589, loss 0.00179563, acc 1, learning_rate 0.000448784
2018-04-02T15:43:28.053765: step 1590, loss 0.00424388, acc 1, learning_rate 0.000448204
2018-04-02T15:43:29.213265: step 1591, loss 0.00211222, acc 1, learning_rate 0.000447625
2018-04-02T15:43:30.375560: step 1592, loss 0.000821356, acc 1, learning_rate 0.000447047
2018-04-02T15:43:31.532779: step 1593, loss 0.0142512, acc 1, learning_rate 0.00044647
2018-04-02T15:43:32.693706: step 1594, loss 0.00122414, acc 1, learning_rate 0.000445894
2018-04-02T15:43:33.851971: step 1595, loss 0.00295191, acc 1, learning_rate 0.000445319
2018-04-02T15:43:35.007673: step 1596, loss 0.00134054, acc 1, learning_rate 0.000444745
2018-04-02T15:43:36.168780: step 1597, loss 0.00187589, acc 1, learning_rate 0.000444172
2018-04-02T15:43:37.327821: step 1598, loss 0.000930743, acc 1, learning_rate 0.000443599
2018-04-02T15:43:38.486057: step 1599, loss 0.00150088, acc 1, learning_rate 0.000443028
2018-04-02T15:43:39.646669: step 1600, loss 0.00492875, acc 1, learning_rate 0.000442458

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:43:39.860329: step 1600, loss 0.0441901, acc 0.980222

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1600

2018-04-02T15:43:44.454835: step 1601, loss 0.00147251, acc 1, learning_rate 0.000441888
2018-04-02T15:43:45.611801: step 1602, loss 0.00111345, acc 1, learning_rate 0.00044132
2018-04-02T15:43:46.769435: step 1603, loss 0.00147371, acc 1, learning_rate 0.000440752
2018-04-02T15:43:47.925839: step 1604, loss 0.00123202, acc 1, learning_rate 0.000440186
2018-04-02T15:43:49.078536: step 1605, loss 0.00360163, acc 1, learning_rate 0.00043962
2018-04-02T15:43:50.239405: step 1606, loss 0.00128361, acc 1, learning_rate 0.000439055
2018-04-02T15:43:51.397173: step 1607, loss 0.00120967, acc 1, learning_rate 0.000438492
2018-04-02T15:43:52.555117: step 1608, loss 0.000625496, acc 1, learning_rate 0.000437929
2018-04-02T15:43:53.712311: step 1609, loss 0.00137399, acc 1, learning_rate 0.000437367
2018-04-02T15:43:54.874142: step 1610, loss 0.00149728, acc 1, learning_rate 0.000436806
2018-04-02T15:43:56.033300: step 1611, loss 0.00798378, acc 1, learning_rate 0.000436246
2018-04-02T15:43:57.192460: step 1612, loss 0.00120041, acc 1, learning_rate 0.000435687
2018-04-02T15:43:58.352142: step 1613, loss 0.068343, acc 0.984375, learning_rate 0.000435129
2018-04-02T15:43:59.517038: step 1614, loss 0.00172429, acc 1, learning_rate 0.000434572
2018-04-02T15:44:00.675225: step 1615, loss 0.00220667, acc 1, learning_rate 0.000434015
2018-04-02T15:44:01.836657: step 1616, loss 0.00107609, acc 1, learning_rate 0.00043346
2018-04-02T15:44:02.995641: step 1617, loss 0.000689588, acc 1, learning_rate 0.000432906
2018-04-02T15:44:04.157916: step 1618, loss 0.00224073, acc 1, learning_rate 0.000432352
2018-04-02T15:44:05.318028: step 1619, loss 0.00117883, acc 1, learning_rate 0.000431799
2018-04-02T15:44:06.478987: step 1620, loss 0.00149239, acc 1, learning_rate 0.000431248
2018-04-02T15:44:07.638567: step 1621, loss 0.001169, acc 1, learning_rate 0.000430697
2018-04-02T15:44:08.799164: step 1622, loss 0.000440576, acc 1, learning_rate 0.000430147
2018-04-02T15:44:09.958990: step 1623, loss 0.00214073, acc 1, learning_rate 0.000429598
2018-04-02T15:44:11.122748: step 1624, loss 0.0252833, acc 0.984375, learning_rate 0.00042905
2018-04-02T15:44:12.286268: step 1625, loss 0.00133181, acc 1, learning_rate 0.000428503
2018-04-02T15:44:13.436305: step 1626, loss 0.000794516, acc 1, learning_rate 0.000427957
2018-04-02T15:44:14.593790: step 1627, loss 0.00192621, acc 1, learning_rate 0.000427412
2018-04-02T15:44:15.755413: step 1628, loss 0.00198103, acc 1, learning_rate 0.000426867
2018-04-02T15:44:16.916276: step 1629, loss 0.00271104, acc 1, learning_rate 0.000426324
2018-04-02T15:44:18.077764: step 1630, loss 0.000633967, acc 1, learning_rate 0.000425781
2018-04-02T15:44:19.237916: step 1631, loss 0.00192616, acc 1, learning_rate 0.00042524
2018-04-02T15:44:20.394548: step 1632, loss 0.0763119, acc 0.984375, learning_rate 0.000424699
2018-04-02T15:44:21.554502: step 1633, loss 0.00172874, acc 1, learning_rate 0.000424159
2018-04-02T15:44:22.710008: step 1634, loss 0.0123269, acc 1, learning_rate 0.00042362
2018-04-02T15:44:23.870343: step 1635, loss 0.00383824, acc 1, learning_rate 0.000423082
2018-04-02T15:44:25.027732: step 1636, loss 0.0026112, acc 1, learning_rate 0.000422545
2018-04-02T15:44:26.186937: step 1637, loss 0.00267107, acc 1, learning_rate 0.000422008
2018-04-02T15:44:27.348437: step 1638, loss 0.00106575, acc 1, learning_rate 0.000421473
2018-04-02T15:44:28.508644: step 1639, loss 0.00221957, acc 1, learning_rate 0.000420938
2018-04-02T15:44:29.664925: step 1640, loss 0.00135762, acc 1, learning_rate 0.000420405
2018-04-02T15:44:30.818516: step 1641, loss 0.00301469, acc 1, learning_rate 0.000419872
2018-04-02T15:44:31.979224: step 1642, loss 0.00156652, acc 1, learning_rate 0.00041934
2018-04-02T15:44:33.137313: step 1643, loss 0.00234178, acc 1, learning_rate 0.000418809
2018-04-02T15:44:34.290676: step 1644, loss 0.00286479, acc 1, learning_rate 0.000418279
2018-04-02T15:44:35.450173: step 1645, loss 0.00124943, acc 1, learning_rate 0.00041775
2018-04-02T15:44:36.609342: step 1646, loss 0.00504557, acc 1, learning_rate 0.000417222
2018-04-02T15:44:37.768728: step 1647, loss 0.0119088, acc 1, learning_rate 0.000416694
2018-04-02T15:44:38.927912: step 1648, loss 0.00221129, acc 1, learning_rate 0.000416168
2018-04-02T15:44:40.087577: step 1649, loss 0.00360399, acc 1, learning_rate 0.000415642
2018-04-02T15:44:41.245127: step 1650, loss 0.0011971, acc 1, learning_rate 0.000415117
2018-04-02T15:44:42.401502: step 1651, loss 0.00172009, acc 1, learning_rate 0.000414593
2018-04-02T15:44:43.556802: step 1652, loss 0.00145944, acc 1, learning_rate 0.00041407
2018-04-02T15:44:44.719464: step 1653, loss 0.0115579, acc 1, learning_rate 0.000413548
2018-04-02T15:44:45.882243: step 1654, loss 0.0028496, acc 1, learning_rate 0.000413027
2018-04-02T15:44:47.041099: step 1655, loss 0.00164311, acc 1, learning_rate 0.000412506
2018-04-02T15:44:48.202342: step 1656, loss 0.00212469, acc 1, learning_rate 0.000411987
2018-04-02T15:44:49.362608: step 1657, loss 0.00181296, acc 1, learning_rate 0.000411468
2018-04-02T15:44:50.524778: step 1658, loss 0.00163217, acc 1, learning_rate 0.00041095
2018-04-02T15:44:51.684500: step 1659, loss 0.000482026, acc 1, learning_rate 0.000410433
2018-04-02T15:44:52.845417: step 1660, loss 0.00451151, acc 1, learning_rate 0.000409917
2018-04-02T15:44:54.006203: step 1661, loss 0.00583726, acc 1, learning_rate 0.000409402
2018-04-02T15:44:55.161120: step 1662, loss 0.0013383, acc 1, learning_rate 0.000408887
2018-04-02T15:44:56.320635: step 1663, loss 0.00420458, acc 1, learning_rate 0.000408374
2018-04-02T15:44:57.483226: step 1664, loss 0.000684153, acc 1, learning_rate 0.000407861
2018-04-02T15:44:58.643852: step 1665, loss 0.00837134, acc 1, learning_rate 0.000407349
2018-04-02T15:44:59.805195: step 1666, loss 0.00244289, acc 1, learning_rate 0.000406838
2018-04-02T15:45:00.966196: step 1667, loss 0.00140311, acc 1, learning_rate 0.000406328
2018-04-02T15:45:02.114080: step 1668, loss 0.00137929, acc 1, learning_rate 0.000405818
2018-04-02T15:45:03.271897: step 1669, loss 0.00375169, acc 1, learning_rate 0.00040531
2018-04-02T15:45:04.437602: step 1670, loss 0.000710111, acc 1, learning_rate 0.000404802
2018-04-02T15:45:05.600320: step 1671, loss 0.00151816, acc 1, learning_rate 0.000404296
2018-04-02T15:45:06.758603: step 1672, loss 0.00100534, acc 1, learning_rate 0.00040379
2018-04-02T15:45:07.918190: step 1673, loss 0.00455749, acc 1, learning_rate 0.000403284
2018-04-02T15:45:09.078576: step 1674, loss 0.00215234, acc 1, learning_rate 0.00040278
2018-04-02T15:45:10.229170: step 1675, loss 0.00196566, acc 1, learning_rate 0.000402277
2018-04-02T15:45:11.388178: step 1676, loss 0.000901244, acc 1, learning_rate 0.000401774
2018-04-02T15:45:12.543377: step 1677, loss 0.00228352, acc 1, learning_rate 0.000401272
2018-04-02T15:45:13.706427: step 1678, loss 0.0017663, acc 1, learning_rate 0.000400772
2018-04-02T15:45:14.866718: step 1679, loss 0.00116526, acc 1, learning_rate 0.000400271
2018-04-02T15:45:16.024949: step 1680, loss 0.00221334, acc 1, learning_rate 0.000399772
2018-04-02T15:45:17.184310: step 1681, loss 0.00180227, acc 1, learning_rate 0.000399274
2018-04-02T15:45:18.343056: step 1682, loss 0.00196672, acc 1, learning_rate 0.000398776
2018-04-02T15:45:19.501957: step 1683, loss 0.00555806, acc 1, learning_rate 0.000398279
2018-04-02T15:45:20.664011: step 1684, loss 0.00191593, acc 1, learning_rate 0.000397783
2018-04-02T15:45:21.823147: step 1685, loss 0.00164825, acc 1, learning_rate 0.000397288
2018-04-02T15:45:22.984913: step 1686, loss 0.0116971, acc 1, learning_rate 0.000396794
2018-04-02T15:45:24.099549: step 1687, loss 0.00772469, acc 1, learning_rate 0.000396301
2018-04-02T15:45:25.259355: step 1688, loss 0.00429444, acc 1, learning_rate 0.000395808
2018-04-02T15:45:26.418557: step 1689, loss 0.000466626, acc 1, learning_rate 0.000395316
2018-04-02T15:45:27.578738: step 1690, loss 0.00164961, acc 1, learning_rate 0.000394825
2018-04-02T15:45:28.736954: step 1691, loss 0.00134129, acc 1, learning_rate 0.000394335
2018-04-02T15:45:29.893711: step 1692, loss 0.00240071, acc 1, learning_rate 0.000393845
2018-04-02T15:45:31.050935: step 1693, loss 0.00161651, acc 1, learning_rate 0.000393357
2018-04-02T15:45:32.212758: step 1694, loss 0.00186681, acc 1, learning_rate 0.000392869
2018-04-02T15:45:33.365665: step 1695, loss 0.00115696, acc 1, learning_rate 0.000392382
2018-04-02T15:45:34.525505: step 1696, loss 0.00187118, acc 1, learning_rate 0.000391896
2018-04-02T15:45:35.683071: step 1697, loss 0.00214446, acc 1, learning_rate 0.000391411
2018-04-02T15:45:36.842001: step 1698, loss 0.0160077, acc 0.984375, learning_rate 0.000390926
2018-04-02T15:45:38.000421: step 1699, loss 0.00575398, acc 1, learning_rate 0.000390442
2018-04-02T15:45:39.158613: step 1700, loss 0.00238117, acc 1, learning_rate 0.00038996

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:45:39.372925: step 1700, loss 0.042571, acc 0.980223

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1700

2018-04-02T15:45:43.686741: step 1701, loss 0.000564901, acc 1, learning_rate 0.000389477
2018-04-02T15:45:44.847096: step 1702, loss 0.000707272, acc 1, learning_rate 0.000388996
2018-04-02T15:45:46.002107: step 1703, loss 0.00149542, acc 1, learning_rate 0.000388516
2018-04-02T15:45:47.162345: step 1704, loss 0.000804643, acc 1, learning_rate 0.000388036
2018-04-02T15:45:48.331002: step 1705, loss 0.00220282, acc 1, learning_rate 0.000387557
2018-04-02T15:45:49.490118: step 1706, loss 0.0369138, acc 0.984375, learning_rate 0.000387079
2018-04-02T15:45:50.650168: step 1707, loss 0.000725913, acc 1, learning_rate 0.000386602
2018-04-02T15:45:51.811583: step 1708, loss 0.00100849, acc 1, learning_rate 0.000386125
2018-04-02T15:45:52.973342: step 1709, loss 0.00166652, acc 1, learning_rate 0.000385649
2018-04-02T15:45:54.135536: step 1710, loss 0.00267145, acc 1, learning_rate 0.000385174
2018-04-02T15:45:55.290971: step 1711, loss 0.00225549, acc 1, learning_rate 0.0003847
2018-04-02T15:45:56.452895: step 1712, loss 0.000632733, acc 1, learning_rate 0.000384227
2018-04-02T15:45:57.612499: step 1713, loss 0.000817718, acc 1, learning_rate 0.000383754
2018-04-02T15:45:58.773251: step 1714, loss 0.00177661, acc 1, learning_rate 0.000383282
2018-04-02T15:45:59.932736: step 1715, loss 0.00160435, acc 1, learning_rate 0.000382811
2018-04-02T15:46:01.091863: step 1716, loss 0.00321397, acc 1, learning_rate 0.000382341
2018-04-02T15:46:02.252664: step 1717, loss 0.00116475, acc 1, learning_rate 0.000381872
2018-04-02T15:46:03.411561: step 1718, loss 0.00467229, acc 1, learning_rate 0.000381403
2018-04-02T15:46:04.571749: step 1719, loss 0.00118703, acc 1, learning_rate 0.000380935
2018-04-02T15:46:05.734227: step 1720, loss 0.00230018, acc 1, learning_rate 0.000380468
2018-04-02T15:46:06.892477: step 1721, loss 0.00207528, acc 1, learning_rate 0.000380002
2018-04-02T15:46:08.055025: step 1722, loss 0.00132948, acc 1, learning_rate 0.000379536
2018-04-02T15:46:09.215991: step 1723, loss 0.00171912, acc 1, learning_rate 0.000379071
2018-04-02T15:46:10.371773: step 1724, loss 0.00193273, acc 1, learning_rate 0.000378607
2018-04-02T15:46:11.531527: step 1725, loss 0.00622951, acc 1, learning_rate 0.000378144
2018-04-02T15:46:12.694438: step 1726, loss 0.00140402, acc 1, learning_rate 0.000377682
2018-04-02T15:46:13.851112: step 1727, loss 0.000460829, acc 1, learning_rate 0.00037722
2018-04-02T15:46:15.012586: step 1728, loss 0.00287109, acc 1, learning_rate 0.000376759
2018-04-02T15:46:16.170213: step 1729, loss 0.00219503, acc 1, learning_rate 0.000376299
2018-04-02T15:46:17.322372: step 1730, loss 0.00190491, acc 1, learning_rate 0.00037584
2018-04-02T15:46:18.478655: step 1731, loss 0.00159909, acc 1, learning_rate 0.000375381
2018-04-02T15:46:19.638677: step 1732, loss 0.000400563, acc 1, learning_rate 0.000374923
2018-04-02T15:46:20.804864: step 1733, loss 0.00151636, acc 1, learning_rate 0.000374466
2018-04-02T15:46:21.962207: step 1734, loss 0.00170974, acc 1, learning_rate 0.00037401
2018-04-02T15:46:23.121898: step 1735, loss 0.000956444, acc 1, learning_rate 0.000373554
2018-04-02T15:46:24.282345: step 1736, loss 0.000837966, acc 1, learning_rate 0.000373099
2018-04-02T15:46:25.436831: step 1737, loss 0.00162818, acc 1, learning_rate 0.000372645
2018-04-02T15:46:26.592666: step 1738, loss 0.00313189, acc 1, learning_rate 0.000372192
2018-04-02T15:46:27.752818: step 1739, loss 0.00182691, acc 1, learning_rate 0.000371739
2018-04-02T15:46:28.912789: step 1740, loss 0.0013272, acc 1, learning_rate 0.000371287
2018-04-02T15:46:30.072212: step 1741, loss 0.000319717, acc 1, learning_rate 0.000370836
2018-04-02T15:46:31.230122: step 1742, loss 0.00109211, acc 1, learning_rate 0.000370386
2018-04-02T15:46:32.386274: step 1743, loss 0.000820836, acc 1, learning_rate 0.000369936
2018-04-02T15:46:33.546546: step 1744, loss 0.00110631, acc 1, learning_rate 0.000369488
2018-04-02T15:46:34.708237: step 1745, loss 0.00121387, acc 1, learning_rate 0.00036904
2018-04-02T15:46:35.866092: step 1746, loss 0.00217197, acc 1, learning_rate 0.000368592
2018-04-02T15:46:37.021911: step 1747, loss 0.00272328, acc 1, learning_rate 0.000368146
2018-04-02T15:46:38.179368: step 1748, loss 0.00102057, acc 1, learning_rate 0.0003677
2018-04-02T15:46:39.338023: step 1749, loss 0.000742664, acc 1, learning_rate 0.000367255
2018-04-02T15:46:40.500748: step 1750, loss 0.00168058, acc 1, learning_rate 0.00036681
2018-04-02T15:46:41.656236: step 1751, loss 0.001459, acc 1, learning_rate 0.000366367
2018-04-02T15:46:42.812278: step 1752, loss 0.00202185, acc 1, learning_rate 0.000365924
2018-04-02T15:46:43.970454: step 1753, loss 0.00443353, acc 1, learning_rate 0.000365482
2018-04-02T15:46:45.129130: step 1754, loss 0.00234613, acc 1, learning_rate 0.00036504
2018-04-02T15:46:46.291137: step 1755, loss 0.00488221, acc 1, learning_rate 0.0003646
2018-04-02T15:46:47.450328: step 1756, loss 0.000859787, acc 1, learning_rate 0.00036416
2018-04-02T15:46:48.607889: step 1757, loss 0.000882863, acc 1, learning_rate 0.00036372
2018-04-02T15:46:49.767199: step 1758, loss 0.001515, acc 1, learning_rate 0.000363282
2018-04-02T15:46:50.924952: step 1759, loss 0.00177794, acc 1, learning_rate 0.000362844
2018-04-02T15:46:52.083352: step 1760, loss 0.00125981, acc 1, learning_rate 0.000362407
2018-04-02T15:46:53.240487: step 1761, loss 0.00314937, acc 1, learning_rate 0.000361971
2018-04-02T15:46:54.399622: step 1762, loss 0.00303753, acc 1, learning_rate 0.000361535
2018-04-02T15:46:55.564133: step 1763, loss 0.021071, acc 0.984375, learning_rate 0.0003611
2018-04-02T15:46:56.723595: step 1764, loss 0.0129812, acc 1, learning_rate 0.000360666
2018-04-02T15:46:57.882534: step 1765, loss 0.0726858, acc 0.984375, learning_rate 0.000360233
2018-04-02T15:46:59.046277: step 1766, loss 0.00156783, acc 1, learning_rate 0.0003598
2018-04-02T15:47:00.208804: step 1767, loss 0.00222104, acc 1, learning_rate 0.000359368
2018-04-02T15:47:01.370386: step 1768, loss 0.00105048, acc 1, learning_rate 0.000358937
2018-04-02T15:47:02.530145: step 1769, loss 0.0224531, acc 0.984375, learning_rate 0.000358506
2018-04-02T15:47:03.688434: step 1770, loss 0.00149098, acc 1, learning_rate 0.000358077
2018-04-02T15:47:04.839942: step 1771, loss 0.00173337, acc 1, learning_rate 0.000357648
2018-04-02T15:47:06.000464: step 1772, loss 0.00100851, acc 1, learning_rate 0.000357219
2018-04-02T15:47:07.158960: step 1773, loss 0.00137213, acc 1, learning_rate 0.000356792
2018-04-02T15:47:08.318698: step 1774, loss 0.00275177, acc 1, learning_rate 0.000356365
2018-04-02T15:47:09.477909: step 1775, loss 0.00406523, acc 1, learning_rate 0.000355938
2018-04-02T15:47:10.631760: step 1776, loss 0.000685232, acc 1, learning_rate 0.000355513
2018-04-02T15:47:11.791761: step 1777, loss 0.0725594, acc 0.984375, learning_rate 0.000355088
2018-04-02T15:47:12.943942: step 1778, loss 0.00109477, acc 1, learning_rate 0.000354664
2018-04-02T15:47:14.100518: step 1779, loss 0.00525619, acc 1, learning_rate 0.00035424
2018-04-02T15:47:15.248367: step 1780, loss 0.00167975, acc 1, learning_rate 0.000353818
2018-04-02T15:47:16.400225: step 1781, loss 0.00307821, acc 1, learning_rate 0.000353396
2018-04-02T15:47:17.553764: step 1782, loss 0.00117873, acc 1, learning_rate 0.000352974
2018-04-02T15:47:18.709174: step 1783, loss 0.0658164, acc 0.984375, learning_rate 0.000352554
2018-04-02T15:47:19.864715: step 1784, loss 0.00273422, acc 1, learning_rate 0.000352134
2018-04-02T15:47:21.005125: step 1785, loss 0.00351145, acc 1, learning_rate 0.000351715
2018-04-02T15:47:22.116948: step 1786, loss 0.00186531, acc 1, learning_rate 0.000351296
2018-04-02T15:47:23.236984: step 1787, loss 0.00172501, acc 1, learning_rate 0.000350878
2018-04-02T15:47:24.356385: step 1788, loss 0.00171519, acc 1, learning_rate 0.000350461
2018-04-02T15:47:25.507844: step 1789, loss 0.00560333, acc 1, learning_rate 0.000350045
2018-04-02T15:47:26.665850: step 1790, loss 0.00307275, acc 1, learning_rate 0.000349629
2018-04-02T15:47:27.790605: step 1791, loss 0.00380257, acc 1, learning_rate 0.000349214
2018-04-02T15:47:28.914857: step 1792, loss 0.00173166, acc 1, learning_rate 0.000348799
2018-04-02T15:47:30.036060: step 1793, loss 0.00234324, acc 1, learning_rate 0.000348386
2018-04-02T15:47:31.156058: step 1794, loss 0.00258473, acc 1, learning_rate 0.000347973
2018-04-02T15:47:32.275357: step 1795, loss 0.000920409, acc 1, learning_rate 0.000347561
2018-04-02T15:47:33.431004: step 1796, loss 0.00114035, acc 1, learning_rate 0.000347149
2018-04-02T15:47:34.588188: step 1797, loss 0.00089255, acc 1, learning_rate 0.000346738
2018-04-02T15:47:35.736045: step 1798, loss 0.00133618, acc 1, learning_rate 0.000346328
2018-04-02T15:47:36.893664: step 1799, loss 0.00214004, acc 1, learning_rate 0.000345918
2018-04-02T15:47:38.049542: step 1800, loss 0.00132937, acc 1, learning_rate 0.000345509

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:47:38.261577: step 1800, loss 0.0449916, acc 0.980222

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1800

2018-04-02T15:47:42.383060: step 1801, loss 0.00104522, acc 1, learning_rate 0.000345101
2018-04-02T15:47:43.532667: step 1802, loss 0.00151516, acc 1, learning_rate 0.000344694
2018-04-02T15:47:44.673280: step 1803, loss 0.00176224, acc 1, learning_rate 0.000344287
2018-04-02T15:47:45.825358: step 1804, loss 0.000976556, acc 1, learning_rate 0.000343881
2018-04-02T15:47:46.977587: step 1805, loss 0.00144689, acc 1, learning_rate 0.000343475
2018-04-02T15:47:48.129332: step 1806, loss 0.00135779, acc 1, learning_rate 0.00034307
2018-04-02T15:47:49.287546: step 1807, loss 0.00140176, acc 1, learning_rate 0.000342666
2018-04-02T15:47:50.448593: step 1808, loss 0.00194631, acc 1, learning_rate 0.000342263
2018-04-02T15:47:51.606687: step 1809, loss 0.00367116, acc 1, learning_rate 0.00034186
2018-04-02T15:47:52.771078: step 1810, loss 0.00198559, acc 1, learning_rate 0.000341458
2018-04-02T15:47:53.934888: step 1811, loss 0.00475248, acc 1, learning_rate 0.000341056
2018-04-02T15:47:55.093681: step 1812, loss 0.000401964, acc 1, learning_rate 0.000340655
2018-04-02T15:47:56.246202: step 1813, loss 0.00183025, acc 1, learning_rate 0.000340255
2018-04-02T15:47:57.406759: step 1814, loss 0.000676489, acc 1, learning_rate 0.000339856
2018-04-02T15:47:58.568507: step 1815, loss 0.00190886, acc 1, learning_rate 0.000339457
2018-04-02T15:47:59.725228: step 1816, loss 0.00119237, acc 1, learning_rate 0.000339059
2018-04-02T15:48:00.886791: step 1817, loss 0.0016475, acc 1, learning_rate 0.000338661
2018-04-02T15:48:02.048031: step 1818, loss 0.0221621, acc 0.984375, learning_rate 0.000338265
2018-04-02T15:48:03.207664: step 1819, loss 0.000315381, acc 1, learning_rate 0.000337868
2018-04-02T15:48:04.368565: step 1820, loss 0.00152444, acc 1, learning_rate 0.000337473
2018-04-02T15:48:05.527911: step 1821, loss 0.0101945, acc 1, learning_rate 0.000337078
2018-04-02T15:48:06.685493: step 1822, loss 0.00115061, acc 1, learning_rate 0.000336684
2018-04-02T15:48:07.842745: step 1823, loss 0.000373265, acc 1, learning_rate 0.00033629
2018-04-02T15:48:08.998199: step 1824, loss 0.00110803, acc 1, learning_rate 0.000335897
2018-04-02T15:48:10.156644: step 1825, loss 0.000996273, acc 1, learning_rate 0.000335505
2018-04-02T15:48:11.318645: step 1826, loss 0.00153925, acc 1, learning_rate 0.000335114
2018-04-02T15:48:12.479256: step 1827, loss 0.00319912, acc 1, learning_rate 0.000334723
2018-04-02T15:48:13.635342: step 1828, loss 0.00440465, acc 1, learning_rate 0.000334332
2018-04-02T15:48:14.793088: step 1829, loss 0.00136729, acc 1, learning_rate 0.000333943
2018-04-02T15:48:15.955433: step 1830, loss 0.00059198, acc 1, learning_rate 0.000333554
2018-04-02T15:48:17.113532: step 1831, loss 0.00131063, acc 1, learning_rate 0.000333166
2018-04-02T15:48:18.267293: step 1832, loss 4.41831e-05, acc 1, learning_rate 0.000332778
2018-04-02T15:48:19.423265: step 1833, loss 0.00405766, acc 1, learning_rate 0.000332391
2018-04-02T15:48:20.581609: step 1834, loss 0.00139824, acc 1, learning_rate 0.000332004
2018-04-02T15:48:21.740102: step 1835, loss 0.00285648, acc 1, learning_rate 0.000331619
2018-04-02T15:48:22.901534: step 1836, loss 0.000555202, acc 1, learning_rate 0.000331234
2018-04-02T15:48:24.059582: step 1837, loss 0.000926076, acc 1, learning_rate 0.000330849
2018-04-02T15:48:25.215803: step 1838, loss 0.0444857, acc 0.984375, learning_rate 0.000330465
2018-04-02T15:48:26.373023: step 1839, loss 0.00132085, acc 1, learning_rate 0.000330082
2018-04-02T15:48:27.530433: step 1840, loss 0.000693213, acc 1, learning_rate 0.0003297
2018-04-02T15:48:28.690372: step 1841, loss 0.00121234, acc 1, learning_rate 0.000329318
2018-04-02T15:48:29.849367: step 1842, loss 0.00217352, acc 1, learning_rate 0.000328936
2018-04-02T15:48:31.008113: step 1843, loss 0.00185856, acc 1, learning_rate 0.000328556
2018-04-02T15:48:32.164400: step 1844, loss 0.00282251, acc 1, learning_rate 0.000328176
2018-04-02T15:48:33.323988: step 1845, loss 0.000917573, acc 1, learning_rate 0.000327796
2018-04-02T15:48:34.476952: step 1846, loss 0.00164639, acc 1, learning_rate 0.000327418
2018-04-02T15:48:35.630189: step 1847, loss 0.00129813, acc 1, learning_rate 0.000327039
2018-04-02T15:48:36.785802: step 1848, loss 0.00360155, acc 1, learning_rate 0.000326662
2018-04-02T15:48:37.939281: step 1849, loss 0.000742586, acc 1, learning_rate 0.000326285
2018-04-02T15:48:39.091704: step 1850, loss 0.00187235, acc 1, learning_rate 0.000325909
2018-04-02T15:48:40.249431: step 1851, loss 0.00271616, acc 1, learning_rate 0.000325533
2018-04-02T15:48:41.401592: step 1852, loss 0.00147461, acc 1, learning_rate 0.000325158
2018-04-02T15:48:42.557529: step 1853, loss 0.000898828, acc 1, learning_rate 0.000324784
2018-04-02T15:48:43.716270: step 1854, loss 0.0773323, acc 0.984375, learning_rate 0.00032441
2018-04-02T15:48:44.874974: step 1855, loss 0.000260991, acc 1, learning_rate 0.000324037
2018-04-02T15:48:46.026792: step 1856, loss 0.0105248, acc 1, learning_rate 0.000323664
2018-04-02T15:48:47.185335: step 1857, loss 0.00142303, acc 1, learning_rate 0.000323293
2018-04-02T15:48:48.347897: step 1858, loss 0.00226251, acc 1, learning_rate 0.000322921
2018-04-02T15:48:49.504528: step 1859, loss 0.000927252, acc 1, learning_rate 0.000322551
2018-04-02T15:48:50.661413: step 1860, loss 0.00197719, acc 1, learning_rate 0.000322181
2018-04-02T15:48:51.818061: step 1861, loss 0.00255175, acc 1, learning_rate 0.000321811
2018-04-02T15:48:52.981763: step 1862, loss 0.00118631, acc 1, learning_rate 0.000321442
2018-04-02T15:48:54.140633: step 1863, loss 0.00559051, acc 1, learning_rate 0.000321074
2018-04-02T15:48:55.295764: step 1864, loss 0.00140688, acc 1, learning_rate 0.000320707
2018-04-02T15:48:56.457903: step 1865, loss 0.000802955, acc 1, learning_rate 0.00032034
2018-04-02T15:48:57.616727: step 1866, loss 0.00474742, acc 1, learning_rate 0.000319973
2018-04-02T15:48:58.775424: step 1867, loss 0.000966497, acc 1, learning_rate 0.000319608
2018-04-02T15:48:59.924535: step 1868, loss 0.00160088, acc 1, learning_rate 0.000319242
2018-04-02T15:49:01.077499: step 1869, loss 0.000439875, acc 1, learning_rate 0.000318878
2018-04-02T15:49:02.232479: step 1870, loss 0.00439127, acc 1, learning_rate 0.000318514
2018-04-02T15:49:03.390768: step 1871, loss 0.00138151, acc 1, learning_rate 0.000318151
2018-04-02T15:49:04.543356: step 1872, loss 0.00226179, acc 1, learning_rate 0.000317788
2018-04-02T15:49:05.698254: step 1873, loss 0.00111136, acc 1, learning_rate 0.000317426
2018-04-02T15:49:06.859660: step 1874, loss 0.00102464, acc 1, learning_rate 0.000317064
2018-04-02T15:49:08.019653: step 1875, loss 0.00115477, acc 1, learning_rate 0.000316703
2018-04-02T15:49:09.178449: step 1876, loss 0.00472027, acc 1, learning_rate 0.000316343
2018-04-02T15:49:10.336809: step 1877, loss 0.00131199, acc 1, learning_rate 0.000315983
2018-04-02T15:49:11.494067: step 1878, loss 0.00274882, acc 1, learning_rate 0.000315624
2018-04-02T15:49:12.657062: step 1879, loss 0.00283166, acc 1, learning_rate 0.000315266
2018-04-02T15:49:13.816273: step 1880, loss 0.00340108, acc 1, learning_rate 0.000314908
2018-04-02T15:49:14.982406: step 1881, loss 0.00806388, acc 1, learning_rate 0.000314551
2018-04-02T15:49:16.144485: step 1882, loss 0.00180119, acc 1, learning_rate 0.000314194
2018-04-02T15:49:17.304258: step 1883, loss 0.00362235, acc 1, learning_rate 0.000313838
2018-04-02T15:49:18.465514: step 1884, loss 0.0929143, acc 0.984375, learning_rate 0.000313482
2018-04-02T15:49:19.621044: step 1885, loss 0.00178069, acc 1, learning_rate 0.000313127
2018-04-02T15:49:20.780507: step 1886, loss 0.00257903, acc 1, learning_rate 0.000312773
2018-04-02T15:49:21.938402: step 1887, loss 0.00173449, acc 1, learning_rate 0.000312419
2018-04-02T15:49:23.099548: step 1888, loss 0.000660539, acc 1, learning_rate 0.000312066
2018-04-02T15:49:24.255084: step 1889, loss 0.0123307, acc 0.984375, learning_rate 0.000311713
2018-04-02T15:49:25.413851: step 1890, loss 0.00277201, acc 1, learning_rate 0.000311361
2018-04-02T15:49:26.572894: step 1891, loss 0.00274302, acc 1, learning_rate 0.00031101
2018-04-02T15:49:27.729518: step 1892, loss 0.00121717, acc 1, learning_rate 0.000310659
2018-04-02T15:49:28.886537: step 1893, loss 0.0022874, acc 1, learning_rate 0.000310309
2018-04-02T15:49:30.045278: step 1894, loss 0.00209089, acc 1, learning_rate 0.000309959
2018-04-02T15:49:31.200889: step 1895, loss 0.00118193, acc 1, learning_rate 0.00030961
2018-04-02T15:49:32.361854: step 1896, loss 0.00211991, acc 1, learning_rate 0.000309261
2018-04-02T15:49:33.520850: step 1897, loss 0.0393123, acc 0.984375, learning_rate 0.000308913
2018-04-02T15:49:34.684227: step 1898, loss 0.000546832, acc 1, learning_rate 0.000308566
2018-04-02T15:49:35.841904: step 1899, loss 0.00228365, acc 1, learning_rate 0.000308219
2018-04-02T15:49:37.001159: step 1900, loss 0.00136081, acc 1, learning_rate 0.000307873

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:49:37.214312: step 1900, loss 0.04712, acc 0.97775

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-1900

2018-04-02T15:49:42.472819: step 1901, loss 0.00330765, acc 1, learning_rate 0.000307528
2018-04-02T15:49:43.625768: step 1902, loss 0.0721678, acc 0.984375, learning_rate 0.000307182
2018-04-02T15:49:44.782443: step 1903, loss 0.00293879, acc 1, learning_rate 0.000306838
2018-04-02T15:49:45.940697: step 1904, loss 0.00252434, acc 1, learning_rate 0.000306494
2018-04-02T15:49:47.098830: step 1905, loss 0.00302793, acc 1, learning_rate 0.000306151
2018-04-02T15:49:48.255114: step 1906, loss 0.000461332, acc 1, learning_rate 0.000305808
2018-04-02T15:49:49.412746: step 1907, loss 0.00180524, acc 1, learning_rate 0.000305466
2018-04-02T15:49:50.571072: step 1908, loss 0.00215634, acc 1, learning_rate 0.000305124
2018-04-02T15:49:51.734065: step 1909, loss 0.00194276, acc 1, learning_rate 0.000304783
2018-04-02T15:49:52.894352: step 1910, loss 0.00228077, acc 1, learning_rate 0.000304443
2018-04-02T15:49:54.051351: step 1911, loss 0.00146991, acc 1, learning_rate 0.000304103
2018-04-02T15:49:55.209015: step 1912, loss 0.000224944, acc 1, learning_rate 0.000303763
2018-04-02T15:49:56.367340: step 1913, loss 0.00306871, acc 1, learning_rate 0.000303425
2018-04-02T15:49:57.525839: step 1914, loss 0.00195431, acc 1, learning_rate 0.000303086
2018-04-02T15:49:58.685362: step 1915, loss 0.00104647, acc 1, learning_rate 0.000302749
2018-04-02T15:49:59.849376: step 1916, loss 0.00259249, acc 1, learning_rate 0.000302412
2018-04-02T15:50:01.011909: step 1917, loss 0.00169126, acc 1, learning_rate 0.000302075
2018-04-02T15:50:02.170423: step 1918, loss 0.000538963, acc 1, learning_rate 0.000301739
2018-04-02T15:50:03.329956: step 1919, loss 0.000696694, acc 1, learning_rate 0.000301404
2018-04-02T15:50:04.489455: step 1920, loss 0.00585202, acc 1, learning_rate 0.000301069
2018-04-02T15:50:05.646974: step 1921, loss 0.00072673, acc 1, learning_rate 0.000300734
2018-04-02T15:50:06.805745: step 1922, loss 0.00219221, acc 1, learning_rate 0.000300401
2018-04-02T15:50:07.964382: step 1923, loss 0.000857513, acc 1, learning_rate 0.000300067
2018-04-02T15:50:09.123448: step 1924, loss 0.000495373, acc 1, learning_rate 0.000299735
2018-04-02T15:50:10.280929: step 1925, loss 0.00273097, acc 1, learning_rate 0.000299403
2018-04-02T15:50:11.441106: step 1926, loss 0.00401551, acc 1, learning_rate 0.000299071
2018-04-02T15:50:12.599581: step 1927, loss 0.0373911, acc 0.984375, learning_rate 0.00029874
2018-04-02T15:50:13.710197: step 1928, loss 0.000412486, acc 1, learning_rate 0.00029841
2018-04-02T15:50:14.872894: step 1929, loss 0.00448869, acc 1, learning_rate 0.00029808
2018-04-02T15:50:16.033672: step 1930, loss 0.00135593, acc 1, learning_rate 0.00029775
2018-04-02T15:50:17.189926: step 1931, loss 0.000734664, acc 1, learning_rate 0.000297422
2018-04-02T15:50:18.347415: step 1932, loss 0.00135483, acc 1, learning_rate 0.000297093
2018-04-02T15:50:19.509260: step 1933, loss 0.00142172, acc 1, learning_rate 0.000296766
2018-04-02T15:50:20.669609: step 1934, loss 0.00181825, acc 1, learning_rate 0.000296439
2018-04-02T15:50:21.827704: step 1935, loss 0.00256162, acc 1, learning_rate 0.000296112
2018-04-02T15:50:22.988314: step 1936, loss 0.00171639, acc 1, learning_rate 0.000295786
2018-04-02T15:50:24.151020: step 1937, loss 0.0043962, acc 1, learning_rate 0.00029546
2018-04-02T15:50:25.311700: step 1938, loss 0.00369821, acc 1, learning_rate 0.000295135
2018-04-02T15:50:26.476489: step 1939, loss 0.00100646, acc 1, learning_rate 0.000294811
2018-04-02T15:50:27.636505: step 1940, loss 0.00199423, acc 1, learning_rate 0.000294487
2018-04-02T15:50:28.798510: step 1941, loss 0.000645475, acc 1, learning_rate 0.000294164
2018-04-02T15:50:29.956008: step 1942, loss 0.00100623, acc 1, learning_rate 0.000293841
2018-04-02T15:50:31.118265: step 1943, loss 0.0019503, acc 1, learning_rate 0.000293519
2018-04-02T15:50:32.276195: step 1944, loss 0.00110121, acc 1, learning_rate 0.000293197
2018-04-02T15:50:33.438690: step 1945, loss 0.0068409, acc 1, learning_rate 0.000292876
2018-04-02T15:50:34.600691: step 1946, loss 0.00206892, acc 1, learning_rate 0.000292555
2018-04-02T15:50:35.763477: step 1947, loss 0.00182036, acc 1, learning_rate 0.000292235
2018-04-02T15:50:36.918030: step 1948, loss 0.00196561, acc 1, learning_rate 0.000291915
2018-04-02T15:50:38.076605: step 1949, loss 0.00274138, acc 1, learning_rate 0.000291596
2018-04-02T15:50:39.235274: step 1950, loss 0.00852817, acc 1, learning_rate 0.000291277
2018-04-02T15:50:40.393599: step 1951, loss 0.00114638, acc 1, learning_rate 0.000290959
2018-04-02T15:50:41.552432: step 1952, loss 0.0851766, acc 0.984375, learning_rate 0.000290642
2018-04-02T15:50:42.711524: step 1953, loss 0.00132204, acc 1, learning_rate 0.000290325
2018-04-02T15:50:43.872244: step 1954, loss 0.00049451, acc 1, learning_rate 0.000290008
2018-04-02T15:50:45.032050: step 1955, loss 0.00300683, acc 1, learning_rate 0.000289693
2018-04-02T15:50:46.195886: step 1956, loss 0.00123579, acc 1, learning_rate 0.000289377
2018-04-02T15:50:47.358404: step 1957, loss 0.00184968, acc 1, learning_rate 0.000289062
2018-04-02T15:50:48.517999: step 1958, loss 0.00176495, acc 1, learning_rate 0.000288748
2018-04-02T15:50:49.680339: step 1959, loss 0.00453111, acc 1, learning_rate 0.000288434
2018-04-02T15:50:50.841440: step 1960, loss 0.00133248, acc 1, learning_rate 0.000288121
2018-04-02T15:50:52.001414: step 1961, loss 0.00157824, acc 1, learning_rate 0.000287808
2018-04-02T15:50:53.163759: step 1962, loss 0.00302342, acc 1, learning_rate 0.000287496
2018-04-02T15:50:54.323473: step 1963, loss 0.0018586, acc 1, learning_rate 0.000287184
2018-04-02T15:50:55.486825: step 1964, loss 0.000770678, acc 1, learning_rate 0.000286873
2018-04-02T15:50:56.646023: step 1965, loss 0.00174565, acc 1, learning_rate 0.000286562
2018-04-02T15:50:57.803375: step 1966, loss 0.000605436, acc 1, learning_rate 0.000286252
2018-04-02T15:50:58.962215: step 1967, loss 0.00188567, acc 1, learning_rate 0.000285942
2018-04-02T15:51:00.120216: step 1968, loss 0.00147341, acc 1, learning_rate 0.000285633
2018-04-02T15:51:01.276177: step 1969, loss 0.00242566, acc 1, learning_rate 0.000285324
2018-04-02T15:51:02.438436: step 1970, loss 0.00158535, acc 1, learning_rate 0.000285016
2018-04-02T15:51:03.593442: step 1971, loss 0.00134349, acc 1, learning_rate 0.000284709
2018-04-02T15:51:04.750073: step 1972, loss 0.00187058, acc 1, learning_rate 0.000284401
2018-04-02T15:51:05.906263: step 1973, loss 0.00191683, acc 1, learning_rate 0.000284095
2018-04-02T15:51:07.067994: step 1974, loss 0.00521126, acc 1, learning_rate 0.000283789
2018-04-02T15:51:08.228836: step 1975, loss 0.000325565, acc 1, learning_rate 0.000283483
2018-04-02T15:51:09.386977: step 1976, loss 0.000378266, acc 1, learning_rate 0.000283178
2018-04-02T15:51:10.545868: step 1977, loss 0.00184826, acc 1, learning_rate 0.000282874
2018-04-02T15:51:11.704849: step 1978, loss 0.00138603, acc 1, learning_rate 0.000282569
2018-04-02T15:51:12.863354: step 1979, loss 0.0034238, acc 1, learning_rate 0.000282266
2018-04-02T15:51:14.019861: step 1980, loss 0.000633923, acc 1, learning_rate 0.000281963
2018-04-02T15:51:15.177798: step 1981, loss 0.00121444, acc 1, learning_rate 0.00028166
2018-04-02T15:51:16.338238: step 1982, loss 0.00150613, acc 1, learning_rate 0.000281358
2018-04-02T15:51:17.497412: step 1983, loss 0.000583995, acc 1, learning_rate 0.000281057
2018-04-02T15:51:18.653927: step 1984, loss 0.00477162, acc 1, learning_rate 0.000280756
2018-04-02T15:51:19.810377: step 1985, loss 0.000827181, acc 1, learning_rate 0.000280455
2018-04-02T15:51:20.969955: step 1986, loss 0.00261842, acc 1, learning_rate 0.000280155
2018-04-02T15:51:22.128893: step 1987, loss 0.00843012, acc 1, learning_rate 0.000279856
2018-04-02T15:51:23.291244: step 1988, loss 0.000531147, acc 1, learning_rate 0.000279557
2018-04-02T15:51:24.447691: step 1989, loss 0.00267356, acc 1, learning_rate 0.000279258
2018-04-02T15:51:25.604313: step 1990, loss 0.000963788, acc 1, learning_rate 0.00027896
2018-04-02T15:51:26.764516: step 1991, loss 0.00102919, acc 1, learning_rate 0.000278662
2018-04-02T15:51:27.925352: step 1992, loss 0.00368933, acc 1, learning_rate 0.000278365
2018-04-02T15:51:29.084856: step 1993, loss 0.00204325, acc 1, learning_rate 0.000278069
2018-04-02T15:51:30.243711: step 1994, loss 0.00432881, acc 1, learning_rate 0.000277773
2018-04-02T15:51:31.405001: step 1995, loss 0.000894294, acc 1, learning_rate 0.000277477
2018-04-02T15:51:32.566267: step 1996, loss 0.0100669, acc 1, learning_rate 0.000277182
2018-04-02T15:51:33.722914: step 1997, loss 0.00127217, acc 1, learning_rate 0.000276887
2018-04-02T15:51:34.881620: step 1998, loss 0.00371339, acc 1, learning_rate 0.000276593
2018-04-02T15:51:36.039988: step 1999, loss 0.00103002, acc 1, learning_rate 0.0002763
2018-04-02T15:51:37.197747: step 2000, loss 0.00304865, acc 1, learning_rate 0.000276007

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:51:37.411326: step 2000, loss 0.0423992, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-2000

2018-04-02T15:51:42.157543: step 2001, loss 0.000662749, acc 1, learning_rate 0.000275714
2018-04-02T15:51:43.315941: step 2002, loss 0.00586518, acc 1, learning_rate 0.000275422
2018-04-02T15:51:44.469238: step 2003, loss 0.00658152, acc 1, learning_rate 0.00027513
2018-04-02T15:51:45.625762: step 2004, loss 0.00268761, acc 1, learning_rate 0.000274839
2018-04-02T15:51:46.787130: step 2005, loss 0.00132084, acc 1, learning_rate 0.000274548
2018-04-02T15:51:47.947161: step 2006, loss 0.00528345, acc 1, learning_rate 0.000274258
2018-04-02T15:51:49.105706: step 2007, loss 0.00147376, acc 1, learning_rate 0.000273968
2018-04-02T15:51:50.268163: step 2008, loss 0.00019442, acc 1, learning_rate 0.000273679
2018-04-02T15:51:51.425545: step 2009, loss 0.00115946, acc 1, learning_rate 0.00027339
2018-04-02T15:51:52.586657: step 2010, loss 0.00166564, acc 1, learning_rate 0.000273102
2018-04-02T15:51:53.740502: step 2011, loss 0.00280568, acc 1, learning_rate 0.000272814
2018-04-02T15:51:54.897311: step 2012, loss 0.00155455, acc 1, learning_rate 0.000272527
2018-04-02T15:51:56.056893: step 2013, loss 0.097683, acc 0.984375, learning_rate 0.00027224
2018-04-02T15:51:57.211153: step 2014, loss 0.00486423, acc 1, learning_rate 0.000271954
2018-04-02T15:51:58.369749: step 2015, loss 0.00108917, acc 1, learning_rate 0.000271668
2018-04-02T15:51:59.529123: step 2016, loss 0.000448841, acc 1, learning_rate 0.000271382
2018-04-02T15:52:00.683338: step 2017, loss 0.00197427, acc 1, learning_rate 0.000271097
2018-04-02T15:52:01.846388: step 2018, loss 0.00058135, acc 1, learning_rate 0.000270813
2018-04-02T15:52:03.003801: step 2019, loss 0.00385634, acc 1, learning_rate 0.000270529
2018-04-02T15:52:04.164109: step 2020, loss 0.000228921, acc 1, learning_rate 0.000270245
2018-04-02T15:52:05.324467: step 2021, loss 0.000912223, acc 1, learning_rate 0.000269962
2018-04-02T15:52:06.479956: step 2022, loss 0.00072905, acc 1, learning_rate 0.00026968
2018-04-02T15:52:07.645486: step 2023, loss 0.00151077, acc 1, learning_rate 0.000269397
2018-04-02T15:52:08.809264: step 2024, loss 0.00277602, acc 1, learning_rate 0.000269116
2018-04-02T15:52:09.973024: step 2025, loss 0.0125573, acc 1, learning_rate 0.000268835
2018-04-02T15:52:11.128376: step 2026, loss 0.000586435, acc 1, learning_rate 0.000268554
2018-04-02T15:52:12.280006: step 2027, loss 0.000648919, acc 1, learning_rate 0.000268274
2018-04-02T15:52:13.435113: step 2028, loss 0.00195466, acc 1, learning_rate 0.000267994
2018-04-02T15:52:14.588105: step 2029, loss 0.0035121, acc 1, learning_rate 0.000267715
2018-04-02T15:52:15.738246: step 2030, loss 0.00977566, acc 1, learning_rate 0.000267436
2018-04-02T15:52:16.890129: step 2031, loss 0.00404582, acc 1, learning_rate 0.000267157
2018-04-02T15:52:18.046905: step 2032, loss 0.00142476, acc 1, learning_rate 0.000266879
2018-04-02T15:52:19.198432: step 2033, loss 0.0551407, acc 0.984375, learning_rate 0.000266602
2018-04-02T15:52:20.346562: step 2034, loss 0.0106525, acc 1, learning_rate 0.000266325
2018-04-02T15:52:21.471760: step 2035, loss 0.00199949, acc 1, learning_rate 0.000266048
2018-04-02T15:52:22.588731: step 2036, loss 0.0116205, acc 1, learning_rate 0.000265772
2018-04-02T15:52:23.706389: step 2037, loss 0.000942521, acc 1, learning_rate 0.000265497
2018-04-02T15:52:24.852383: step 2038, loss 0.000211105, acc 1, learning_rate 0.000265221
2018-04-02T15:52:26.005667: step 2039, loss 0.00232196, acc 1, learning_rate 0.000264947
2018-04-02T15:52:27.131170: step 2040, loss 0.00259116, acc 1, learning_rate 0.000264673
2018-04-02T15:52:28.253266: step 2041, loss 0.00316259, acc 1, learning_rate 0.000264399
2018-04-02T15:52:29.439554: step 2042, loss 0.000721747, acc 1, learning_rate 0.000264125
2018-04-02T15:52:30.558951: step 2043, loss 0.00199276, acc 1, learning_rate 0.000263852
2018-04-02T15:52:31.687564: step 2044, loss 0.000900917, acc 1, learning_rate 0.00026358
2018-04-02T15:52:32.842799: step 2045, loss 0.00313323, acc 1, learning_rate 0.000263308
2018-04-02T15:52:33.993492: step 2046, loss 0.0754332, acc 0.984375, learning_rate 0.000263037
2018-04-02T15:52:35.150090: step 2047, loss 0.00246693, acc 1, learning_rate 0.000262765
2018-04-02T15:52:36.299278: step 2048, loss 0.00256272, acc 1, learning_rate 0.000262495
2018-04-02T15:52:37.452769: step 2049, loss 0.00241249, acc 1, learning_rate 0.000262225
2018-04-02T15:52:38.607351: step 2050, loss 0.000560512, acc 1, learning_rate 0.000261955
2018-04-02T15:52:39.764491: step 2051, loss 0.00197994, acc 1, learning_rate 0.000261686
2018-04-02T15:52:40.924559: step 2052, loss 0.00141973, acc 1, learning_rate 0.000261417
2018-04-02T15:52:42.073102: step 2053, loss 0.00515816, acc 1, learning_rate 0.000261148
2018-04-02T15:52:43.224319: step 2054, loss 0.000991884, acc 1, learning_rate 0.00026088
2018-04-02T15:52:44.377061: step 2055, loss 0.00022928, acc 1, learning_rate 0.000260613
2018-04-02T15:52:45.534016: step 2056, loss 0.00400856, acc 1, learning_rate 0.000260346
2018-04-02T15:52:46.692189: step 2057, loss 0.00223172, acc 1, learning_rate 0.000260079
2018-04-02T15:52:47.853110: step 2058, loss 0.00176114, acc 1, learning_rate 0.000259813
2018-04-02T15:52:49.017218: step 2059, loss 0.00489109, acc 1, learning_rate 0.000259547
2018-04-02T15:52:50.179232: step 2060, loss 0.000614199, acc 1, learning_rate 0.000259282
2018-04-02T15:52:51.338515: step 2061, loss 0.00160154, acc 1, learning_rate 0.000259017
2018-04-02T15:52:52.493982: step 2062, loss 0.000559648, acc 1, learning_rate 0.000258753
2018-04-02T15:52:53.652329: step 2063, loss 0.000793327, acc 1, learning_rate 0.000258489
2018-04-02T15:52:54.806601: step 2064, loss 0.0026031, acc 1, learning_rate 0.000258225
2018-04-02T15:52:55.964139: step 2065, loss 0.0015919, acc 1, learning_rate 0.000257962
2018-04-02T15:52:57.123210: step 2066, loss 0.00224818, acc 1, learning_rate 0.0002577
2018-04-02T15:52:58.282439: step 2067, loss 0.00137638, acc 1, learning_rate 0.000257438
2018-04-02T15:52:59.442762: step 2068, loss 0.00250316, acc 1, learning_rate 0.000257176
2018-04-02T15:53:00.598890: step 2069, loss 0.000962854, acc 1, learning_rate 0.000256914
2018-04-02T15:53:01.756366: step 2070, loss 0.00248067, acc 1, learning_rate 0.000256654
2018-04-02T15:53:02.918731: step 2071, loss 0.00200985, acc 1, learning_rate 0.000256393
2018-04-02T15:53:04.079167: step 2072, loss 0.00235984, acc 1, learning_rate 0.000256133
2018-04-02T15:53:05.238330: step 2073, loss 0.00203663, acc 1, learning_rate 0.000255873
2018-04-02T15:53:06.394575: step 2074, loss 0.00174007, acc 1, learning_rate 0.000255614
2018-04-02T15:53:07.558969: step 2075, loss 0.000404155, acc 1, learning_rate 0.000255356
2018-04-02T15:53:08.712759: step 2076, loss 0.00243967, acc 1, learning_rate 0.000255097
2018-04-02T15:53:09.870358: step 2077, loss 0.00197019, acc 1, learning_rate 0.000254839
2018-04-02T15:53:11.028799: step 2078, loss 0.000752546, acc 1, learning_rate 0.000254582
2018-04-02T15:53:12.187747: step 2079, loss 0.000934166, acc 1, learning_rate 0.000254325
2018-04-02T15:53:13.346589: step 2080, loss 0.000888052, acc 1, learning_rate 0.000254068
2018-04-02T15:53:14.505411: step 2081, loss 0.00125428, acc 1, learning_rate 0.000253812
2018-04-02T15:53:15.665689: step 2082, loss 0.00187705, acc 1, learning_rate 0.000253556
2018-04-02T15:53:16.822287: step 2083, loss 0.000963462, acc 1, learning_rate 0.000253301
2018-04-02T15:53:17.980704: step 2084, loss 0.00175511, acc 1, learning_rate 0.000253046
2018-04-02T15:53:19.140003: step 2085, loss 0.00375517, acc 1, learning_rate 0.000252792
2018-04-02T15:53:20.300810: step 2086, loss 0.00124846, acc 1, learning_rate 0.000252538
2018-04-02T15:53:21.461861: step 2087, loss 0.00148393, acc 1, learning_rate 0.000252284
2018-04-02T15:53:22.622809: step 2088, loss 0.00215029, acc 1, learning_rate 0.000252031
2018-04-02T15:53:23.784336: step 2089, loss 0.00353425, acc 1, learning_rate 0.000251778
2018-04-02T15:53:24.943680: step 2090, loss 0.000976311, acc 1, learning_rate 0.000251526
2018-04-02T15:53:26.104801: step 2091, loss 0.00160566, acc 1, learning_rate 0.000251274
2018-04-02T15:53:27.263335: step 2092, loss 0.00179178, acc 1, learning_rate 0.000251022
2018-04-02T15:53:28.421116: step 2093, loss 0.00152176, acc 1, learning_rate 0.000250771
2018-04-02T15:53:29.571433: step 2094, loss 0.00073335, acc 1, learning_rate 0.00025052
2018-04-02T15:53:30.727007: step 2095, loss 0.00208312, acc 1, learning_rate 0.00025027
2018-04-02T15:53:31.888565: step 2096, loss 0.00054938, acc 1, learning_rate 0.00025002
2018-04-02T15:53:33.044378: step 2097, loss 0.000874796, acc 1, learning_rate 0.000249771
2018-04-02T15:53:34.204583: step 2098, loss 0.00199794, acc 1, learning_rate 0.000249522
2018-04-02T15:53:35.358210: step 2099, loss 0.00122993, acc 1, learning_rate 0.000249273
2018-04-02T15:53:36.514274: step 2100, loss 0.0045973, acc 1, learning_rate 0.000249025

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:53:36.727322: step 2100, loss 0.0423505, acc 0.982695

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-2100

2018-04-02T15:53:40.479878: step 2101, loss 0.0296115, acc 0.984375, learning_rate 0.000248777
2018-04-02T15:53:41.634598: step 2102, loss 0.00111964, acc 1, learning_rate 0.00024853
2018-04-02T15:53:42.798795: step 2103, loss 0.00155216, acc 1, learning_rate 0.000248283
2018-04-02T15:53:43.954289: step 2104, loss 0.00424575, acc 1, learning_rate 0.000248036
2018-04-02T15:53:45.104754: step 2105, loss 0.0011759, acc 1, learning_rate 0.00024779
2018-04-02T15:53:46.260864: step 2106, loss 0.000782088, acc 1, learning_rate 0.000247545
2018-04-02T15:53:47.409734: step 2107, loss 0.000661306, acc 1, learning_rate 0.000247299
2018-04-02T15:53:48.568117: step 2108, loss 0.0013185, acc 1, learning_rate 0.000247054
2018-04-02T15:53:49.726551: step 2109, loss 0.00112057, acc 1, learning_rate 0.00024681
2018-04-02T15:53:50.884646: step 2110, loss 0.00109714, acc 1, learning_rate 0.000246566
2018-04-02T15:53:52.042196: step 2111, loss 0.00112429, acc 1, learning_rate 0.000246322
2018-04-02T15:53:53.205164: step 2112, loss 0.00142338, acc 1, learning_rate 0.000246079
2018-04-02T15:53:54.363496: step 2113, loss 0.000929093, acc 1, learning_rate 0.000245836
2018-04-02T15:53:55.519892: step 2114, loss 0.0635739, acc 0.984375, learning_rate 0.000245593
2018-04-02T15:53:56.679654: step 2115, loss 0.000979175, acc 1, learning_rate 0.000245351
2018-04-02T15:53:57.837376: step 2116, loss 0.0016153, acc 1, learning_rate 0.00024511
2018-04-02T15:53:58.995472: step 2117, loss 0.00172307, acc 1, learning_rate 0.000244868
2018-04-02T15:54:00.151895: step 2118, loss 0.00559598, acc 1, learning_rate 0.000244628
2018-04-02T15:54:01.310048: step 2119, loss 0.00279127, acc 1, learning_rate 0.000244387
2018-04-02T15:54:02.473147: step 2120, loss 0.000947667, acc 1, learning_rate 0.000244147
2018-04-02T15:54:03.631715: step 2121, loss 0.000241196, acc 1, learning_rate 0.000243907
2018-04-02T15:54:04.792568: step 2122, loss 0.00159139, acc 1, learning_rate 0.000243668
2018-04-02T15:54:05.952602: step 2123, loss 0.00326861, acc 1, learning_rate 0.000243429
2018-04-02T15:54:07.108408: step 2124, loss 0.00732572, acc 1, learning_rate 0.000243191
2018-04-02T15:54:08.266402: step 2125, loss 0.00191149, acc 1, learning_rate 0.000242953
2018-04-02T15:54:09.423739: step 2126, loss 0.012046, acc 1, learning_rate 0.000242715
2018-04-02T15:54:10.581946: step 2127, loss 0.000549599, acc 1, learning_rate 0.000242478
2018-04-02T15:54:11.747326: step 2128, loss 0.000987718, acc 1, learning_rate 0.000242241
2018-04-02T15:54:12.911308: step 2129, loss 0.00219063, acc 1, learning_rate 0.000242004
2018-04-02T15:54:14.071514: step 2130, loss 0.0107172, acc 1, learning_rate 0.000241768
2018-04-02T15:54:15.234317: step 2131, loss 0.00180698, acc 1, learning_rate 0.000241532
2018-04-02T15:54:16.390996: step 2132, loss 0.00129208, acc 1, learning_rate 0.000241297
2018-04-02T15:54:17.547876: step 2133, loss 0.0017548, acc 1, learning_rate 0.000241062
2018-04-02T15:54:18.707351: step 2134, loss 0.0215583, acc 0.984375, learning_rate 0.000240828
2018-04-02T15:54:19.870553: step 2135, loss 0.00407268, acc 1, learning_rate 0.000240593
2018-04-02T15:54:21.029096: step 2136, loss 0.00140005, acc 1, learning_rate 0.00024036
2018-04-02T15:54:22.191734: step 2137, loss 0.000388292, acc 1, learning_rate 0.000240126
2018-04-02T15:54:23.352734: step 2138, loss 0.00432778, acc 1, learning_rate 0.000239893
2018-04-02T15:54:24.512244: step 2139, loss 0.000780817, acc 1, learning_rate 0.000239661
2018-04-02T15:54:25.671463: step 2140, loss 0.00153812, acc 1, learning_rate 0.000239429
2018-04-02T15:54:26.827445: step 2141, loss 0.00184398, acc 1, learning_rate 0.000239197
2018-04-02T15:54:27.981422: step 2142, loss 0.00170605, acc 1, learning_rate 0.000238965
2018-04-02T15:54:29.140355: step 2143, loss 0.00127822, acc 1, learning_rate 0.000238734
2018-04-02T15:54:30.298307: step 2144, loss 0.00764814, acc 1, learning_rate 0.000238504
2018-04-02T15:54:31.459439: step 2145, loss 0.00111622, acc 1, learning_rate 0.000238273
2018-04-02T15:54:32.620984: step 2146, loss 0.00183001, acc 1, learning_rate 0.000238043
2018-04-02T15:54:33.778715: step 2147, loss 0.000963466, acc 1, learning_rate 0.000237814
2018-04-02T15:54:34.930044: step 2148, loss 0.00112061, acc 1, learning_rate 0.000237585
2018-04-02T15:54:36.089001: step 2149, loss 0.00162825, acc 1, learning_rate 0.000237356
2018-04-02T15:54:37.248713: step 2150, loss 0.00198981, acc 1, learning_rate 0.000237128
2018-04-02T15:54:38.403654: step 2151, loss 0.00224616, acc 1, learning_rate 0.0002369
2018-04-02T15:54:39.559418: step 2152, loss 0.0590125, acc 0.984375, learning_rate 0.000236672
2018-04-02T15:54:40.720381: step 2153, loss 0.00116561, acc 1, learning_rate 0.000236445
2018-04-02T15:54:41.878798: step 2154, loss 0.000587706, acc 1, learning_rate 0.000236218
2018-04-02T15:54:43.041132: step 2155, loss 0.00223576, acc 1, learning_rate 0.000235991
2018-04-02T15:54:44.199260: step 2156, loss 0.00209343, acc 1, learning_rate 0.000235765
2018-04-02T15:54:45.361981: step 2157, loss 0.000784758, acc 1, learning_rate 0.000235539
2018-04-02T15:54:46.522932: step 2158, loss 0.00275407, acc 1, learning_rate 0.000235314
2018-04-02T15:54:47.684903: step 2159, loss 0.00261247, acc 1, learning_rate 0.000235089
2018-04-02T15:54:48.849465: step 2160, loss 0.00254301, acc 1, learning_rate 0.000234865
2018-04-02T15:54:50.010472: step 2161, loss 0.000552343, acc 1, learning_rate 0.00023464
2018-04-02T15:54:51.173063: step 2162, loss 0.00214063, acc 1, learning_rate 0.000234416
2018-04-02T15:54:52.330246: step 2163, loss 0.00191246, acc 1, learning_rate 0.000234193
2018-04-02T15:54:53.488153: step 2164, loss 0.00239811, acc 1, learning_rate 0.00023397
2018-04-02T15:54:54.650167: step 2165, loss 0.00136882, acc 1, learning_rate 0.000233747
2018-04-02T15:54:55.809465: step 2166, loss 0.000748412, acc 1, learning_rate 0.000233525
2018-04-02T15:54:56.964379: step 2167, loss 0.0213711, acc 0.984375, learning_rate 0.000233303
2018-04-02T15:54:58.123748: step 2168, loss 0.00193987, acc 1, learning_rate 0.000233081
2018-04-02T15:54:59.237606: step 2169, loss 0.000567611, acc 1, learning_rate 0.00023286
2018-04-02T15:55:00.399575: step 2170, loss 0.000795606, acc 1, learning_rate 0.000232639
2018-04-02T15:55:01.560055: step 2171, loss 0.00243159, acc 1, learning_rate 0.000232418
2018-04-02T15:55:02.722215: step 2172, loss 0.000981387, acc 1, learning_rate 0.000232198
2018-04-02T15:55:03.886269: step 2173, loss 0.00391832, acc 1, learning_rate 0.000231978
2018-04-02T15:55:05.051833: step 2174, loss 0.000421016, acc 1, learning_rate 0.000231759
2018-04-02T15:55:06.209609: step 2175, loss 0.000641751, acc 1, learning_rate 0.00023154
2018-04-02T15:55:07.367646: step 2176, loss 0.000918264, acc 1, learning_rate 0.000231321
2018-04-02T15:55:08.523738: step 2177, loss 0.00112997, acc 1, learning_rate 0.000231103
2018-04-02T15:55:09.680290: step 2178, loss 0.00198938, acc 1, learning_rate 0.000230885
2018-04-02T15:55:10.831827: step 2179, loss 0.00164198, acc 1, learning_rate 0.000230667
2018-04-02T15:55:11.995656: step 2180, loss 0.001733, acc 1, learning_rate 0.00023045
2018-04-02T15:55:13.157141: step 2181, loss 0.00108155, acc 1, learning_rate 0.000230233
2018-04-02T15:55:14.315767: step 2182, loss 0.00543714, acc 1, learning_rate 0.000230016
2018-04-02T15:55:15.477354: step 2183, loss 0.00289173, acc 1, learning_rate 0.0002298
2018-04-02T15:55:16.635275: step 2184, loss 0.00157952, acc 1, learning_rate 0.000229584
2018-04-02T15:55:17.793104: step 2185, loss 0.000618237, acc 1, learning_rate 0.000229369
2018-04-02T15:55:18.950316: step 2186, loss 9.91852e-05, acc 1, learning_rate 0.000229154
2018-04-02T15:55:20.106716: step 2187, loss 0.000297216, acc 1, learning_rate 0.000228939
2018-04-02T15:55:21.264806: step 2188, loss 0.00108853, acc 1, learning_rate 0.000228725
2018-04-02T15:55:22.425422: step 2189, loss 0.00133808, acc 1, learning_rate 0.000228511
2018-04-02T15:55:23.585369: step 2190, loss 0.00152848, acc 1, learning_rate 0.000228297
2018-04-02T15:55:24.742833: step 2191, loss 0.00243758, acc 1, learning_rate 0.000228084
2018-04-02T15:55:25.902571: step 2192, loss 0.00105654, acc 1, learning_rate 0.000227871
2018-04-02T15:55:27.055856: step 2193, loss 0.000395871, acc 1, learning_rate 0.000227658
2018-04-02T15:55:28.209554: step 2194, loss 0.000905184, acc 1, learning_rate 0.000227446
2018-04-02T15:55:29.370794: step 2195, loss 0.000520709, acc 1, learning_rate 0.000227234
2018-04-02T15:55:30.529199: step 2196, loss 0.0878259, acc 0.984375, learning_rate 0.000227023
2018-04-02T15:55:31.688941: step 2197, loss 0.00177569, acc 1, learning_rate 0.000226811
2018-04-02T15:55:32.847403: step 2198, loss 0.000928767, acc 1, learning_rate 0.0002266
2018-04-02T15:55:33.999067: step 2199, loss 0.00396381, acc 1, learning_rate 0.00022639
2018-04-02T15:55:35.156820: step 2200, loss 0.00127502, acc 1, learning_rate 0.00022618

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:55:35.369671: step 2200, loss 0.0423695, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-2200

2018-04-02T15:55:38.996035: step 2201, loss 0.000812886, acc 1, learning_rate 0.00022597
2018-04-02T15:55:40.146517: step 2202, loss 0.000213537, acc 1, learning_rate 0.000225761
2018-04-02T15:55:41.300735: step 2203, loss 0.00216095, acc 1, learning_rate 0.000225551
2018-04-02T15:55:42.461301: step 2204, loss 0.000621577, acc 1, learning_rate 0.000225343
2018-04-02T15:55:43.619507: step 2205, loss 0.000548584, acc 1, learning_rate 0.000225134
2018-04-02T15:55:44.776168: step 2206, loss 0.00399927, acc 1, learning_rate 0.000224926
2018-04-02T15:55:45.936910: step 2207, loss 0.000541738, acc 1, learning_rate 0.000224719
2018-04-02T15:55:47.093343: step 2208, loss 0.000835167, acc 1, learning_rate 0.000224511
2018-04-02T15:55:48.251665: step 2209, loss 0.00209095, acc 1, learning_rate 0.000224304
2018-04-02T15:55:49.414178: step 2210, loss 0.00118028, acc 1, learning_rate 0.000224097
2018-04-02T15:55:50.574330: step 2211, loss 0.000420224, acc 1, learning_rate 0.000223891
2018-04-02T15:55:51.732010: step 2212, loss 0.00715443, acc 1, learning_rate 0.000223685
2018-04-02T15:55:52.891823: step 2213, loss 0.000582836, acc 1, learning_rate 0.00022348
2018-04-02T15:55:54.052837: step 2214, loss 0.00174764, acc 1, learning_rate 0.000223274
2018-04-02T15:55:55.208895: step 2215, loss 0.00316822, acc 1, learning_rate 0.000223069
2018-04-02T15:55:56.368137: step 2216, loss 0.00188378, acc 1, learning_rate 0.000222865
2018-04-02T15:55:57.521805: step 2217, loss 0.00348123, acc 1, learning_rate 0.00022266
2018-04-02T15:55:58.680023: step 2218, loss 0.00223231, acc 1, learning_rate 0.000222456
2018-04-02T15:55:59.833298: step 2219, loss 0.000722321, acc 1, learning_rate 0.000222253
2018-04-02T15:56:00.987735: step 2220, loss 0.00323493, acc 1, learning_rate 0.00022205
2018-04-02T15:56:02.151863: step 2221, loss 0.00227613, acc 1, learning_rate 0.000221847
2018-04-02T15:56:03.310810: step 2222, loss 0.00165409, acc 1, learning_rate 0.000221644
2018-04-02T15:56:04.471637: step 2223, loss 0.00055046, acc 1, learning_rate 0.000221442
2018-04-02T15:56:05.633598: step 2224, loss 0.000900959, acc 1, learning_rate 0.00022124
2018-04-02T15:56:06.791030: step 2225, loss 0.000598126, acc 1, learning_rate 0.000221038
2018-04-02T15:56:07.946580: step 2226, loss 0.00349691, acc 1, learning_rate 0.000220837
2018-04-02T15:56:09.104426: step 2227, loss 0.000343806, acc 1, learning_rate 0.000220636
2018-04-02T15:56:10.263355: step 2228, loss 0.00123573, acc 1, learning_rate 0.000220435
2018-04-02T15:56:11.423725: step 2229, loss 0.000836525, acc 1, learning_rate 0.000220235
2018-04-02T15:56:12.580945: step 2230, loss 0.000610927, acc 1, learning_rate 0.000220035
2018-04-02T15:56:13.736526: step 2231, loss 0.00062484, acc 1, learning_rate 0.000219836
2018-04-02T15:56:14.895527: step 2232, loss 0.00180212, acc 1, learning_rate 0.000219637
2018-04-02T15:56:16.054194: step 2233, loss 0.00221631, acc 1, learning_rate 0.000219438
2018-04-02T15:56:17.211295: step 2234, loss 0.00151534, acc 1, learning_rate 0.000219239
2018-04-02T15:56:18.369232: step 2235, loss 0.00132445, acc 1, learning_rate 0.000219041
2018-04-02T15:56:19.529202: step 2236, loss 0.0296913, acc 0.984375, learning_rate 0.000218843
2018-04-02T15:56:20.685241: step 2237, loss 0.00144388, acc 1, learning_rate 0.000218645
2018-04-02T15:56:21.845843: step 2238, loss 0.00096826, acc 1, learning_rate 0.000218448
2018-04-02T15:56:23.007321: step 2239, loss 0.00308783, acc 1, learning_rate 0.000218251
2018-04-02T15:56:24.163202: step 2240, loss 0.00156055, acc 1, learning_rate 0.000218054
2018-04-02T15:56:25.324987: step 2241, loss 0.0101822, acc 1, learning_rate 0.000217858
2018-04-02T15:56:26.487259: step 2242, loss 0.00169677, acc 1, learning_rate 0.000217662
2018-04-02T15:56:27.649138: step 2243, loss 0.00131564, acc 1, learning_rate 0.000217467
2018-04-02T15:56:28.809937: step 2244, loss 0.00165831, acc 1, learning_rate 0.000217271
2018-04-02T15:56:29.966403: step 2245, loss 0.0871846, acc 0.984375, learning_rate 0.000217076
2018-04-02T15:56:31.127723: step 2246, loss 0.00212953, acc 1, learning_rate 0.000216882
2018-04-02T15:56:32.283931: step 2247, loss 0.00234928, acc 1, learning_rate 0.000216687
2018-04-02T15:56:33.440005: step 2248, loss 0.00324873, acc 1, learning_rate 0.000216493
2018-04-02T15:56:34.601096: step 2249, loss 0.00136746, acc 1, learning_rate 0.0002163
2018-04-02T15:56:35.761760: step 2250, loss 0.00295763, acc 1, learning_rate 0.000216106
2018-04-02T15:56:36.915748: step 2251, loss 0.00233151, acc 1, learning_rate 0.000215913
2018-04-02T15:56:38.071548: step 2252, loss 0.00115709, acc 1, learning_rate 0.00021572
2018-04-02T15:56:39.228705: step 2253, loss 0.0147722, acc 0.984375, learning_rate 0.000215528
2018-04-02T15:56:40.392037: step 2254, loss 0.00273103, acc 1, learning_rate 0.000215336
2018-04-02T15:56:41.555150: step 2255, loss 0.00229116, acc 1, learning_rate 0.000215144
2018-04-02T15:56:42.717374: step 2256, loss 0.00211548, acc 1, learning_rate 0.000214953
2018-04-02T15:56:43.878092: step 2257, loss 0.000548033, acc 1, learning_rate 0.000214762
2018-04-02T15:56:45.037131: step 2258, loss 0.00133532, acc 1, learning_rate 0.000214571
2018-04-02T15:56:46.191125: step 2259, loss 0.00136221, acc 1, learning_rate 0.00021438
2018-04-02T15:56:47.348552: step 2260, loss 0.000528972, acc 1, learning_rate 0.00021419
2018-04-02T15:56:48.507888: step 2261, loss 0.00329664, acc 1, learning_rate 0.000214
2018-04-02T15:56:49.668662: step 2262, loss 0.00109102, acc 1, learning_rate 0.000213811
2018-04-02T15:56:50.824790: step 2263, loss 0.00134769, acc 1, learning_rate 0.000213621
2018-04-02T15:56:51.978363: step 2264, loss 0.000555746, acc 1, learning_rate 0.000213432
2018-04-02T15:56:53.135939: step 2265, loss 0.000680585, acc 1, learning_rate 0.000213244
2018-04-02T15:56:54.293550: step 2266, loss 0.00229076, acc 1, learning_rate 0.000213056
2018-04-02T15:56:55.448120: step 2267, loss 0.00160735, acc 1, learning_rate 0.000212868
2018-04-02T15:56:56.607103: step 2268, loss 0.00145079, acc 1, learning_rate 0.00021268
2018-04-02T15:56:57.767508: step 2269, loss 0.00197167, acc 1, learning_rate 0.000212493
2018-04-02T15:56:58.925434: step 2270, loss 0.00871869, acc 1, learning_rate 0.000212306
2018-04-02T15:57:00.086317: step 2271, loss 0.000521104, acc 1, learning_rate 0.000212119
2018-04-02T15:57:01.242509: step 2272, loss 0.00241452, acc 1, learning_rate 0.000211932
2018-04-02T15:57:02.395393: step 2273, loss 0.00441235, acc 1, learning_rate 0.000211746
2018-04-02T15:57:03.549962: step 2274, loss 0.0024852, acc 1, learning_rate 0.000211561
2018-04-02T15:57:04.706152: step 2275, loss 0.000803574, acc 1, learning_rate 0.000211375
2018-04-02T15:57:05.860270: step 2276, loss 0.00798194, acc 1, learning_rate 0.00021119
2018-04-02T15:57:07.013641: step 2277, loss 0.00139806, acc 1, learning_rate 0.000211005
2018-04-02T15:57:08.154496: step 2278, loss 0.00412328, acc 1, learning_rate 0.00021082
2018-04-02T15:57:09.306485: step 2279, loss 0.00178481, acc 1, learning_rate 0.000210636
2018-04-02T15:57:10.462004: step 2280, loss 0.00151096, acc 1, learning_rate 0.000210452
2018-04-02T15:57:11.617155: step 2281, loss 0.00191518, acc 1, learning_rate 0.000210269
2018-04-02T15:57:12.776512: step 2282, loss 0.00599513, acc 1, learning_rate 0.000210085
2018-04-02T15:57:13.933668: step 2283, loss 0.000928637, acc 1, learning_rate 0.000209902
2018-04-02T15:57:15.054083: step 2284, loss 0.000402363, acc 1, learning_rate 0.000209719
2018-04-02T15:57:16.201185: step 2285, loss 0.00200945, acc 1, learning_rate 0.000209537
2018-04-02T15:57:17.343476: step 2286, loss 0.00199286, acc 1, learning_rate 0.000209355
2018-04-02T15:57:18.497035: step 2287, loss 0.00116585, acc 1, learning_rate 0.000209173
2018-04-02T15:57:19.623587: step 2288, loss 0.00116993, acc 1, learning_rate 0.000208992
2018-04-02T15:57:20.745664: step 2289, loss 0.0121766, acc 1, learning_rate 0.00020881
2018-04-02T15:57:21.867182: step 2290, loss 0.000889898, acc 1, learning_rate 0.000208629
2018-04-02T15:57:22.990307: step 2291, loss 0.00117807, acc 1, learning_rate 0.000208449
2018-04-02T15:57:24.129529: step 2292, loss 0.00321295, acc 1, learning_rate 0.000208268
2018-04-02T15:57:25.283737: step 2293, loss 0.00160523, acc 1, learning_rate 0.000208088
2018-04-02T15:57:26.439494: step 2294, loss 0.000818009, acc 1, learning_rate 0.000207909
2018-04-02T15:57:27.585936: step 2295, loss 0.00132883, acc 1, learning_rate 0.000207729
2018-04-02T15:57:28.740841: step 2296, loss 0.00255683, acc 1, learning_rate 0.00020755
2018-04-02T15:57:29.897489: step 2297, loss 0.000787888, acc 1, learning_rate 0.000207371
2018-04-02T15:57:31.049065: step 2298, loss 0.00150377, acc 1, learning_rate 0.000207193
2018-04-02T15:57:32.197240: step 2299, loss 0.0696369, acc 0.984375, learning_rate 0.000207015
2018-04-02T15:57:33.352580: step 2300, loss 0.000824477, acc 1, learning_rate 0.000206837

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:57:33.565836: step 2300, loss 0.0444959, acc 0.981459

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-2300

2018-04-02T15:57:37.448893: step 2301, loss 0.00123095, acc 1, learning_rate 0.000206659
2018-04-02T15:57:38.606288: step 2302, loss 0.000905702, acc 1, learning_rate 0.000206482
2018-04-02T15:57:39.763925: step 2303, loss 0.00131054, acc 1, learning_rate 0.000206305
2018-04-02T15:57:40.918894: step 2304, loss 0.00196034, acc 1, learning_rate 0.000206128
2018-04-02T15:57:42.080723: step 2305, loss 0.000793336, acc 1, learning_rate 0.000205951
2018-04-02T15:57:43.239247: step 2306, loss 0.00203823, acc 1, learning_rate 0.000205775
2018-04-02T15:57:44.401733: step 2307, loss 0.00110828, acc 1, learning_rate 0.000205599
2018-04-02T15:57:45.561332: step 2308, loss 0.00198594, acc 1, learning_rate 0.000205424
2018-04-02T15:57:46.722198: step 2309, loss 0.000393336, acc 1, learning_rate 0.000205249
2018-04-02T15:57:47.879166: step 2310, loss 0.0028234, acc 1, learning_rate 0.000205074
2018-04-02T15:57:49.036484: step 2311, loss 0.00313233, acc 1, learning_rate 0.000204899
2018-04-02T15:57:50.195593: step 2312, loss 0.00071434, acc 1, learning_rate 0.000204724
2018-04-02T15:57:51.358051: step 2313, loss 0.00134701, acc 1, learning_rate 0.00020455
2018-04-02T15:57:52.518341: step 2314, loss 0.000840743, acc 1, learning_rate 0.000204376
2018-04-02T15:57:53.674658: step 2315, loss 0.00465263, acc 1, learning_rate 0.000204203
2018-04-02T15:57:54.830446: step 2316, loss 0.00728175, acc 1, learning_rate 0.00020403
2018-04-02T15:57:55.993367: step 2317, loss 0.000489812, acc 1, learning_rate 0.000203857
2018-04-02T15:57:57.158455: step 2318, loss 0.000645568, acc 1, learning_rate 0.000203684
2018-04-02T15:57:58.323076: step 2319, loss 0.000703211, acc 1, learning_rate 0.000203512
2018-04-02T15:57:59.486488: step 2320, loss 0.000952938, acc 1, learning_rate 0.00020334
2018-04-02T15:58:00.650054: step 2321, loss 0.0059259, acc 1, learning_rate 0.000203168
2018-04-02T15:58:01.795068: step 2322, loss 0.00200505, acc 1, learning_rate 0.000202996
2018-04-02T15:58:02.898852: step 2323, loss 0.00167939, acc 1, learning_rate 0.000202825
2018-04-02T15:58:04.006250: step 2324, loss 0.000414142, acc 1, learning_rate 0.000202654
2018-04-02T15:58:05.166342: step 2325, loss 0.00433651, acc 1, learning_rate 0.000202483
2018-04-02T15:58:06.332188: step 2326, loss 0.0018361, acc 1, learning_rate 0.000202313
2018-04-02T15:58:07.491067: step 2327, loss 0.000503683, acc 1, learning_rate 0.000202143
2018-04-02T15:58:08.658805: step 2328, loss 0.00154465, acc 1, learning_rate 0.000201973
2018-04-02T15:58:09.830471: step 2329, loss 0.00122076, acc 1, learning_rate 0.000201803
2018-04-02T15:58:10.991478: step 2330, loss 0.0132522, acc 0.984375, learning_rate 0.000201634
2018-04-02T15:58:12.162933: step 2331, loss 0.00127725, acc 1, learning_rate 0.000201465
2018-04-02T15:58:13.326173: step 2332, loss 0.000865608, acc 1, learning_rate 0.000201296
2018-04-02T15:58:14.483075: step 2333, loss 0.00190133, acc 1, learning_rate 0.000201128
2018-04-02T15:58:15.643336: step 2334, loss 0.000573059, acc 1, learning_rate 0.00020096
2018-04-02T15:58:16.797410: step 2335, loss 0.00125826, acc 1, learning_rate 0.000200792
2018-04-02T15:58:17.958363: step 2336, loss 0.0906035, acc 0.984375, learning_rate 0.000200624
2018-04-02T15:58:19.117966: step 2337, loss 0.00155088, acc 1, learning_rate 0.000200457
2018-04-02T15:58:20.284503: step 2338, loss 0.000998502, acc 1, learning_rate 0.00020029
2018-04-02T15:58:21.445186: step 2339, loss 0.00135595, acc 1, learning_rate 0.000200123
2018-04-02T15:58:22.598077: step 2340, loss 0.0041748, acc 1, learning_rate 0.000199957
2018-04-02T15:58:23.758487: step 2341, loss 0.00402389, acc 1, learning_rate 0.000199791
2018-04-02T15:58:24.918713: step 2342, loss 0.00125931, acc 1, learning_rate 0.000199625
2018-04-02T15:58:26.077551: step 2343, loss 0.000654954, acc 1, learning_rate 0.000199459
2018-04-02T15:58:27.231170: step 2344, loss 0.00239268, acc 1, learning_rate 0.000199294
2018-04-02T15:58:28.382889: step 2345, loss 0.00177301, acc 1, learning_rate 0.000199129
2018-04-02T15:58:29.539023: step 2346, loss 0.00156945, acc 1, learning_rate 0.000198964
2018-04-02T15:58:30.694050: step 2347, loss 0.0651961, acc 0.984375, learning_rate 0.000198799
2018-04-02T15:58:31.851819: step 2348, loss 0.00197268, acc 1, learning_rate 0.000198635
2018-04-02T15:58:33.008385: step 2349, loss 0.000254082, acc 1, learning_rate 0.000198471
2018-04-02T15:58:34.165831: step 2350, loss 0.00151324, acc 1, learning_rate 0.000198307
2018-04-02T15:58:35.325642: step 2351, loss 0.00185879, acc 1, learning_rate 0.000198144
2018-04-02T15:58:36.484943: step 2352, loss 0.000704639, acc 1, learning_rate 0.000197981
2018-04-02T15:58:37.644523: step 2353, loss 0.00117254, acc 1, learning_rate 0.000197818
2018-04-02T15:58:38.799572: step 2354, loss 0.00189309, acc 1, learning_rate 0.000197655
2018-04-02T15:58:39.962261: step 2355, loss 0.00221283, acc 1, learning_rate 0.000197493
2018-04-02T15:58:41.110261: step 2356, loss 0.00844415, acc 1, learning_rate 0.000197331
2018-04-02T15:58:42.275480: step 2357, loss 0.00312289, acc 1, learning_rate 0.000197169
2018-04-02T15:58:43.432106: step 2358, loss 0.0021432, acc 1, learning_rate 0.000197007
2018-04-02T15:58:44.594119: step 2359, loss 0.00054611, acc 1, learning_rate 0.000196846
2018-04-02T15:58:45.751281: step 2360, loss 0.00182483, acc 1, learning_rate 0.000196685
2018-04-02T15:58:46.908224: step 2361, loss 0.00389975, acc 1, learning_rate 0.000196524
2018-04-02T15:58:48.067948: step 2362, loss 0.000853455, acc 1, learning_rate 0.000196364
2018-04-02T15:58:49.228387: step 2363, loss 0.000756778, acc 1, learning_rate 0.000196203
2018-04-02T15:58:50.380086: step 2364, loss 0.00120064, acc 1, learning_rate 0.000196043
2018-04-02T15:58:51.542966: step 2365, loss 0.00130278, acc 1, learning_rate 0.000195884
2018-04-02T15:58:52.700958: step 2366, loss 0.000488941, acc 1, learning_rate 0.000195724
2018-04-02T15:58:53.859512: step 2367, loss 0.00149705, acc 1, learning_rate 0.000195565
2018-04-02T15:58:55.019907: step 2368, loss 0.00145017, acc 1, learning_rate 0.000195406
2018-04-02T15:58:56.180192: step 2369, loss 0.069792, acc 0.984375, learning_rate 0.000195248
2018-04-02T15:58:57.340419: step 2370, loss 0.00161527, acc 1, learning_rate 0.000195089
2018-04-02T15:58:58.498636: step 2371, loss 0.00214815, acc 1, learning_rate 0.000194931
2018-04-02T15:58:59.659506: step 2372, loss 0.000653541, acc 1, learning_rate 0.000194773
2018-04-02T15:59:00.820250: step 2373, loss 0.00248649, acc 1, learning_rate 0.000194616
2018-04-02T15:59:01.979910: step 2374, loss 0.00249078, acc 1, learning_rate 0.000194458
2018-04-02T15:59:03.139279: step 2375, loss 0.00152195, acc 1, learning_rate 0.000194301
2018-04-02T15:59:04.302720: step 2376, loss 0.000644146, acc 1, learning_rate 0.000194145
2018-04-02T15:59:05.463481: step 2377, loss 0.00062046, acc 1, learning_rate 0.000193988
2018-04-02T15:59:06.623447: step 2378, loss 0.00225408, acc 1, learning_rate 0.000193832
2018-04-02T15:59:07.783080: step 2379, loss 0.00272586, acc 1, learning_rate 0.000193676
2018-04-02T15:59:08.943642: step 2380, loss 0.00112555, acc 1, learning_rate 0.00019352
2018-04-02T15:59:10.103470: step 2381, loss 0.000718015, acc 1, learning_rate 0.000193365
2018-04-02T15:59:11.263709: step 2382, loss 0.0017301, acc 1, learning_rate 0.000193209
2018-04-02T15:59:12.420346: step 2383, loss 0.00565759, acc 1, learning_rate 0.000193054
2018-04-02T15:59:13.583447: step 2384, loss 0.0100006, acc 1, learning_rate 0.0001929
2018-04-02T15:59:14.741224: step 2385, loss 0.00131574, acc 1, learning_rate 0.000192745
2018-04-02T15:59:15.900175: step 2386, loss 0.00134358, acc 1, learning_rate 0.000192591
2018-04-02T15:59:17.061046: step 2387, loss 0.00601189, acc 1, learning_rate 0.000192437
2018-04-02T15:59:18.219254: step 2388, loss 0.00252728, acc 1, learning_rate 0.000192283
2018-04-02T15:59:19.378472: step 2389, loss 0.000593887, acc 1, learning_rate 0.00019213
2018-04-02T15:59:20.547949: step 2390, loss 0.00654303, acc 1, learning_rate 0.000191977
2018-04-02T15:59:21.713296: step 2391, loss 0.000956996, acc 1, learning_rate 0.000191824
2018-04-02T15:59:22.867015: step 2392, loss 0.00171707, acc 1, learning_rate 0.000191671
2018-04-02T15:59:24.021770: step 2393, loss 0.00114108, acc 1, learning_rate 0.000191519
2018-04-02T15:59:25.185065: step 2394, loss 0.00129988, acc 1, learning_rate 0.000191367
2018-04-02T15:59:26.343050: step 2395, loss 0.00183497, acc 1, learning_rate 0.000191215
2018-04-02T15:59:27.497662: step 2396, loss 0.00123088, acc 1, learning_rate 0.000191063
2018-04-02T15:59:28.658426: step 2397, loss 0.0144772, acc 0.984375, learning_rate 0.000190912
2018-04-02T15:59:29.819965: step 2398, loss 0.000621951, acc 1, learning_rate 0.00019076
2018-04-02T15:59:30.979180: step 2399, loss 0.00173476, acc 1, learning_rate 0.000190609
2018-04-02T15:59:32.136010: step 2400, loss 0.00131922, acc 1, learning_rate 0.000190459

Evaluation:
got here 0
got here 1
got here 2
2018-04-02T15:59:32.348659: step 2400, loss 0.0461589, acc 0.978986

Saved model checkpoint to /om/user/njobrien/FakeNews/cnn-text-classification-tf (body)/log/runs/1522681744/checkpoints/model-2400

2018-04-02T15:59:36.997583: step 2401, loss 0.00102831, acc 1, learning_rate 0.000190308
2018-04-02T15:59:38.156841: step 2402, loss 0.00133234, acc 1, learning_rate 0.000190158
2018-04-02T15:59:39.311189: step 2403, loss 0.0130748, acc 0.984375, learning_rate 0.000190008
2018-04-02T15:59:40.464520: step 2404, loss 0.0345635, acc 0.984375, learning_rate 0.000189859
2018-04-02T15:59:41.630740: step 2405, loss 0.00074082, acc 1, learning_rate 0.000189709
2018-04-02T15:59:42.792346: step 2406, loss 0.00127493, acc 1, learning_rate 0.00018956
2018-04-02T15:59:43.954567: step 2407, loss 0.00106148, acc 1, learning_rate 0.000189411
2018-04-02T15:59:45.111192: step 2408, loss 0.000979921, acc 1, learning_rate 0.000189263
2018-04-02T15:59:46.269461: step 2409, loss 0.00156167, acc 1, learning_rate 0.000189114
2018-04-02T15:59:47.381070: step 2410, loss 0.00304124, acc 1, learning_rate 0.000188966
